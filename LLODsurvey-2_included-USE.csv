Authors,Title,Pasquale,Mary,Overall,Motivation,Year,Link,Abstract,Publication Stage,Open Access,Manually added
Chen Y.-H.; Lu E.J.-L.; Lin S.-C.,Ontology-based Dynamic Semantic Annotation for Social Image Retrieval,0,0,0,#use #annotation automatic semantic image annotation model for social image retrieval,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090381498&doi=10.1109%2fMDM48529.2020.00074&partnerID=40&md5=0f913291ac5297a8ef08dcb0c1851b52,"To mitigate the semantic gap between a visual feature and linguistic representation in social image retrieval, researchers have proposed an automatic image annotation approach, which employs multiple visual features and text matching to label images. This paper used a linked open data approach and ontology to construct a model, an automatic semantic image annotation model for social image retrieval. These models enable users to label images through automatic semantic annotation and to identify the underlying intents of semantics, thereby fulfilling user needs and enhancing the retrieval accuracy. © 2020 IEEE.",Final,,
Bourgonje P.; Moreno-Schneider J.; Nehring J.; Rehm G.; Sasaki F.; Srivastava A.,Towards a platform for curation technologies: Enriching text collections with a semantic-web layer,1,1,1,#use,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994527341&doi=10.1007%2f978-3-319-47602-5_14&partnerID=40&md5=bd87b516c5ba0f26a86c73d8b6c06189,"In an attempt to put a Semantic Web-layer that provides linguistic analysis and discourse information on top of digital content, we develop a platform for digital curation technologies. The platform offers language-, knowledge-and data-aware services as a flexible set of workflows and pipelines for the efficient processing of various types of digital content. The platform is intended to enable human experts (knowledge workers) to get a grasp and understand the contents of large document collections in an efficient way so that they can curate, process and further analyse the collection according to their sector-specific needs. © Springer International Publishing AG 2016.",Final,,
Xing C.; Liu X.; Du D.; Hu W.; Zhang M.,Relation extraction using language model based on knowledge graph,-1,-1,-1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096414931&doi=10.1088%2f1742-6596%2f1624%2f2%2f022037&partnerID=40&md5=8dabd909fb79c05a8f685bdafcfba007,"Relation extraction is an important task in natural language processing (NLP). The existing methods generally pay more attention on extracting textual semantic information from text, but ignore the relation contextual information from existed relations in datasets, which is very important for the performance of relation extraction task. In this paper, we represent each individual entity as a embedding based on entities and relations knowledge graph, which encodes the relation contextual information between the given entity pairs and relations. Besides, inspired by the impressive performance of language models recently, we used the language model to leverage word semantic information, in which word semantic information can be better captured than word embedding. The experimental results on SemEval2010 Task 8 dataset showed that the F1-score of our proposed method improved nearly 3% compared with the previous methods. © 2020 Institute of Physics Publishing. All rights reserved.",Final,All Open Access; Bronze Open Access,
Kumar S.; Ramaneswaran S.; Akhtar M.S.; Chakraborty T.,From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues,1,1,1,#use #commonsensekg,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184822341&partnerID=40&md5=14584b69691f8ab92b34cc16279846d8,"Understanding emotions during conversation is a fundamental aspect of human communication, driving NLP research for Emotion Recognition in Conversation (ERC). While considerable research has focused on discerning emotions of individual speakers in monolingual dialogues, understanding the emotional dynamics in code-mixed conversations has received relatively less attention. This motivates our undertaking of ERC for code-mixed conversations in this study. Recognizing that emotional intelligence encompasses a comprehension of worldly knowledge, we propose an innovative approach that integrates commonsense information with dialogue context to facilitate a deeper understanding of emotions. To achieve this, we devise an efficient pipeline that extracts relevant commonsense from existing knowledge graphs based on the code-mixed input. Subsequently, we develop an advanced fusion technique that seamlessly combines the acquired commonsense information with the dialogue representation obtained from a dedicated dialogue understanding module. Our comprehensive experimentation showcases the substantial performance improvement obtained through the systematic incorporation of commonsense in ERC. Both quantitative assessments and qualitative analyses further corroborate the validity of our hypothesis, reaffirming the pivotal role of commonsense integration in enhancing ERC. © 2023 Association for Computational Linguistics.",Final,,
Zhao Y.; Zhang J.; Zhou Y.; Zong C.,Knowledge graphs enhanced neural machine translation,1,1,1,#use #translation,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097342403&partnerID=40&md5=0ae6d87f08dd49ee81d8742071255ca0,"Knowledge graphs (KGs) store much structured information on various entities, many of which are not covered by the parallel sentence pairs of neural machine translation (NMT). To improve the translation quality of these entities, in this paper we propose a novel KGs enhanced NMT method. Specifically, we first induce the new translation results of these entities by transforming the source and target KGs into a unified semantic space. We then generate adequate pseudo parallel sentence pairs that contain these induced entity pairs. Finally, NMT model is jointly trained by the original and pseudo sentence pairs. The extensive experiments on Chinese-to-English and English-to-Japanese translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling the induced entities. © 2020 Inst. Sci. inf., Univ. Defence in Belgrade. All rights reserved.",Final,,
Chen X.; Chen M.; Fan C.; Uppunda A.; Sun Y.; Zaniolo C.,Multilingual knowledge graph completion via ensemble knowledge transfer,1,1,1,#use #kgcompletion,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098414071&partnerID=40&md5=16c05b1bd715d103286c9b4c277dc12a,"Predicting missing facts in a knowledge graph (KG) is a crucial task in knowledge base construction and reasoning; and it has been the subject of much research in recent works using KG embeddings. While existing KG embedding approaches mainly learn and predict facts within a single KG, a more plausible solution would benefit from the knowledge in multiple language-specific KGs, considering that different KGs have their own strengths and limitations on data quality and coverage. This is quite challenging, since the transfer of knowledge among multiple independently maintained KGs is often hindered by the insufficiency of alignment information and the inconsistency of described facts. In this paper, we propose KEnS, a novel framework for embedding learning and ensemble knowledge transfer across a number of language-specific KGs. KEnS embeds all KGs in a shared embedding space, where the association of entities is captured based on self-learning. Then, KEnS performs ensemble inference to combine prediction results from embeddings of multiple language-specific KGs, for which multiple ensemble techniques are investigated. Experiments on five real-world language-specific KGs show that KEnS consistently improves state-of-the-art methods on KG completion, via effectively identifying and leveraging complementary knowledge. © 2020 Association for Computational Linguistics",Final,,
Tan Y.; Zhang X.; Chen Y.; Ali Z.; Hua Y.; Qi G.,CLRN: A reasoning network for multi-relation question answering over Cross-lingual Knowledge Graphs,1,1,1,#use #cross-lingualkgs #questionanswering,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162144389&doi=10.1016%2fj.eswa.2023.120721&partnerID=40&md5=725cd1d23f8e203ad0f1238fbe3f76a0,"Cross-lingual Knowledge Graphs-based Question Answering (CLKGQA) requires the question answering (QA) system to combine the knowledge graphs (KGs) in different languages to obtain answers to input questions. In previous works, the common idea is to merge Cross-lingual Knowledge Graphs (CLKGs) into a single KG through aligned entity pairs and then treat it as a traditional KG-based QA. However, as demonstrated by Tan et al. (2023), existing Entity Alignment (EA) models cannot generate highly accurate aligned entity pairs for CLKGs. Therefore, two issues need to be addressed in the CLKGQA task: (1) Remove the dependency of the QA model on the fused KG; (2) Improve the performance of the EA model in obtaining aligned entity pairs from locally isomorphic CLKGs. To solve the above two issues, this paper presents Cross-lingual Reasoning Network (CLRN), a novel multi-hop QA model that allows switching knowledge graphs at any stage of the multi-hop reasoning. Furthermore, we establish an iterative framework that combines CLRN and EA model, in which CLRN is used for extracting potential alignment triple pairs from CLKGs during the QA process. The extracted triple pairs provide pseudo-aligned entities, and the additional aligned entity pairs are used to mine missing relations between entities in CLKGs. These pseudo-aligned entity pairs and relations improve the performance of the EA model, resulting in higher accuracy in QA. Extensive experiments demonstrate the effectiveness of the proposed model, which outperforms the baseline approaches. Through iterative enhancement, the performance of the EA model has also been improved by > 1.0 % in Hit@1 and Hit@10, and the improvement is statistically significant in the confidence interval of p<0.01. Moreover, our work discusses the correlation between QA and EA from the side of QA, which has reference value for the follow-up exploration of related communities. We have open-sourced our dataset and code, which is available at the URL https://github.com/tan92hl/Cross-lingual-Reasoning-Network-for-CLKGQA. © 2023 Elsevier Ltd",Final,,
Goel S.; Gracia J.; Forcada M.L.,Bilingual dictionary generation and enrichment via graph exploration,1,1,1,#kg apertium #use #synonyms,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140834359&doi=10.3233%2fSW-222899&partnerID=40&md5=dacaa9f99e46d8a0814518971cac32c2,"In recent years, we have witnessed a steady growth of linguistic information represented and exposed as linked data on the Web. Such linguistic linked data have stimulated the development and use of openly available linguistic knowledge graphs, as is the case with the Apertium RDF, a collection of interconnected bilingual dictionaries represented and accessible through Semantic Web standards. In this work, we explore techniques that exploit the graph nature of bilingual dictionaries to automatically infer new links (translations). We build upon a cycle density based method: partitioning the graph into biconnected components for a speed-up, and simplifying the pipeline through a careful structural analysis that reduces hyperparameter tuning requirements. We also analyse the shortcomings of traditional evaluation metrics used for translation inference and propose to complement them with new ones, both-word precision (BWP) and both-word recall (BWR), aimed at being more informative of algorithmic improvements. Over twenty-seven language pairs, our algorithm produces dictionaries about 70% the size of existing Apertium RDF dictionaries at a high BWP of 85% from scratch within a minute. Human evaluation shows that 78% of the additional translations generated for dictionary enrichment are correct as well. We further describe an interesting use-case: inferring synonyms within a single language, on which our initial human-based evaluation shows an average accuracy of 84%. We release our tool as free/open-source software which can not only be applied to RDF data and Apertium dictionaries, but is also easily usable for other formats and communities. © 2022 - The authors. Published by IOS Press.",Final,All Open Access; Bronze Open Access; Green Open Access,
Liu L.; Li X.; He R.; Bing L.; Joty S.; Si L.,Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples,1,1,1,#use,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143087240&partnerID=40&md5=23c56bd34f15c1847bdc19f5bba9c742,"Knowledge-enhanced language representation learning has shown promising results across various knowledge-intensive NLP tasks. However, prior methods are limited in efficient utilization of multilingual knowledge graph (KG) data for language model (LM) pretraining. They often train LMs with KGs in indirect ways, relying on extra entity/relation embeddings to facilitate knowledge injection. In this work, we explore methods to make better use of the multilingual annotation and language agnostic property of KG triples, and present novel knowledge based multilingual language models (KMLMs) trained directly on the knowledge triples. We first generate a large amount of multilingual synthetic sentences using the Wikidata KG triples. Then based on the intra- and inter-sentence structures of the generated data, we design pretraining tasks to enable the LMs to not only memorize the factual knowledge but also learn useful logical patterns. Our pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive crosslingual tasks, including named entity recognition (NER), factual knowledge retrieval, relation classification, and a newly designed logical reasoning task. © 2022 Association for Computational Linguistics.",Final,,
Zhou Y.; Geng X.; Shen T.; Zhang W.; Jiang D.,Improving Zero-Shot Cross-lingual Transfer for Multilingual Question Answering over Knowledge Graph,1,1,1,#use #question-anserwing,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113823264&partnerID=40&md5=927041cb034315cb207fa63a6422b3d5,"Multilingual question answering over knowledge graph (KGQA) aims to derive answers from a knowledge graph (KG) for questions in multiple languages. To be widely applicable, we focus on its zero-shot transfer setting. That is, we can only access training data in a high-resource language, while need to answer multilingual questions without any labeled data in target languages. A straightforward approach is resorting to pre-trained multilingual models (e.g., mBERT) for cross-lingual transfer, but there is a still significant gap of KGQA performance between source and target languages. In this paper, we exploit unsupervised bilingual lexicon induction (BLI) to map training questions in source language into those in target language as augmented training data, which circumvents language inconsistency between training and inference. Furthermore, we propose an adversarial learning strategy to alleviate syntax-disorder of the augmented data, making the model incline to both language- and syntax-independence. Consequently, our model narrows the gap in zero-shot cross-lingual transfer. Experiments on two multilingual KGQA datasets with 11 zero-resource languages verify its effectiveness. © 2021 Association for Computational Linguistics.",Final,,
Recupero D.R.; Consoli S.; Gangemi A.; Nuzzolese A.G.; Presutti V.; Spampinato D.,Semantic web-based sentiment analysis,1,1,1,#use sentiment analysis,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926391179&partnerID=40&md5=2348848a14158c7f8c3ec2bdd1146d5a,"The introduction of semantics in Sentiment Analysis research has proved to bring several benefits for what performances are concerned and has allowed to identify new challenging tasks to be accomplished. Semantics helps structuring the plain natural language text with formal representation. The current system we are developing performs sentiment analysis by hybridizing natural language processing techniques with Semantic Web technologies. Our system, called Sentilo, is able to recognize the holder of an opinion, to detect the topics and sub-topics in its scope, and to measure the sentiment expressed by them. This information is formally represented by means of RDF graphs according to an OWL opinion ontology, while holders and topics identity is resolved on the Linked Open Data cloud.",Final,,
Calì A.; Capuzzi S.; Dimartino M.M.; Frosini R.,Recommendation of text tags in social applications using linked data,-1,1,1,#use #sematic-similarity,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893742773&doi=10.1007%2f978-3-319-04244-2_17&partnerID=40&md5=45ad37b0010cf876305de854d418aa6f,We present a recommender system that suggests geo-located text tags by using linguistic information extracted from Linked Data sets available on the Web. The recommender system performs tag matching by measuring the semantic similarity of natural language texts. Our approach evaluates similarity using a technique that compares sentences taking into account their grammatical structure. © Springer International Publishing 2013.,Final,All Open Access; Bronze Open Access,
Guo M.; Chen Y.; Xu J.; Zhang Y.,Dynamic Knowledge Integration for Natural Language Inference,-1,1,1,#use,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139414930&doi=10.1109%2fICNLP55136.2022.00066&partnerID=40&md5=0bedbd55d6413eaf9aaa331f6cc6d90c,"Natural language inference (NLI) aims to determine the entailment relationship between the premise and the hypothesis. It is a fundamental but difficult problem, since there may exists serious semantic and logistic gap between the premise and the hypothesis. Despite using strong pre-trained language model (PLM), previous work performs poorly on complicated reasoning for knowledge-sensitive cases ignoring the integration of external knowledge. We propose a dynamic knowledge integration strategy for NLI, where knowledge from multiple knowledge graphs (KGs) can be dynamically integrated. For each KG, it transforms input tokens into a graph according to the connectivity of the related entities. All the graphs are encoded by a group of parallel graph neural networks (GNNs), and after each layer the intermediate results are integrated dynamically by being conditioned on the input text. This strategy also facilitates the incorporation of PLM, simply by treating the input tokens as a fully connected graph and adapting the PLM outputs as the node embeddings. Experiments on SNLI, MNLI and SciTail show that, the dynamic integration of knowledge from WordNet and ConceptNet achieves significant improvements over the strongest baseline built upon RoBERTa. © 2022 IEEE.",Final,,
Pan Y.; Chen Q.; Peng W.; Wang X.; Hu B.; Liu X.; Chen J.; Zhou W.,MedWriter: Knowledge-Aware Medical Text Generation,-1,1,1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139892424&partnerID=40&md5=e3ce78079138dbef5b8f502fc13e0dea,"To exploit the domain knowledge to guarantee the correctness of generated text has been a hot topic in recent years, especially for high professional domains such as medical. However, most of recent works only consider the information of unstructured text rather than structured information of the knowledge graph. In this paper, we focus on the medical topic-to-text generation task and adapt a knowledge-aware text generation model to the medical domain, named MedWriter, which not only introduces the specific knowledge from the external MKG but also is capable of learning graph-level representation. We conduct experiments on a medical literature dataset collected from medical journals, each of which has a set of topic words, an abstract of medical literature and a corresponding knowledge graph from CMeKG. Experimental results demonstrate incorporating knowledge graph into generation model can improve the quality of the generated text and has robust superiority over the competitor methods. © 2020 COLING 2020 - 28th International Conference on Computational Linguistics, Proceedings of the Conference. All rights reserved.",Final,,
Kumar S.; Jat S.; Saxena K.; Talukdar P.,Zero-shot word sense disambiguation using sense definition embeddings,-1,1,1,#use #word-sense_disambiguation WordNet,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075859601&partnerID=40&md5=df7fa442077c5b74e0de03860cb4219b,"Word Sense Disambiguation (WSD) is a longstanding but open problem in Natural Language Processing (NLP). WSD corpora are typically small in size, owing to an expensive annotation process. Current supervised WSD methods treat senses as discrete labels and also resort to predicting the Most-Frequent-Sense (MFS) for words unseen during training. This leads to poor performance on rare and unseen senses. To overcome this challenge, we propose Extended WSD Incorporating Sense Embeddings (EWISE), a supervised model to perform WSD by predicting over a continuous sense embedding space as opposed to a discrete label space. This allows EWISE to generalize over both seen and unseen senses, thus achieving generalized zero-shot learning. To obtain target sense embeddings, EWISE utilizes sense definitions. EWISE learns a novel sentence encoder for sense definitions by using WordNet relations and also ConvE, a recently proposed knowledge graph embedding method. We also compare EWISE against other sentence encoders pretrained on large corpora to generate definition embeddings. EWISE achieves new state-of-the-art WSD performance. © 2019 Association for Computational Linguistics",Final,,
Li H.; Xu F.,Question answering with DBpedia based on the dependency parser and entity-centric index,,1,1,#use #question-answering WordNet and conceptnet to expand synonyms,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994850866&doi=10.1109%2fICCIA.2016.10&partnerID=40&md5=d27f5e4648b945d672480870bf24972f,"The emerging Linked Open Data provides an opportunity to answer the natural language question based on knowledge bases (KB). This study proposes an approach to question answering (QA) on the DBpedia dataset. After parsing the question by a dependency parser, we locate the entity mention and property mention with predefined templates. We propose an entity-centric indexing model to help search referent entities in KB. After obtaining the referent entities, we expand the property mention with WordNet and ConceptNet to find the referent properties of the returned entities. The values of the referent property are then considered the answer to the question. Evaluations are performed on DBpedia version 2015. Results show that our approach reaches 46% precision when the top-10 entities are returned in the final QA stage. The evaluation tests show that our approach is promising in dealing with QA in Linked Data. © 2016 IEEE.",Final,,
Van Aggelen A.; Hollink L.; Van Ossenbruggen J.,Combining distributional semantics and structured data to study lexical change,1,1,1,#use,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016291410&partnerID=40&md5=c553b8279d1c93ff2cff33aba3370daa,"Statistical Natural Language Processing (NLP) techniques allow to quantify lexical semantic change using large text corpora. Word-level results of these methods can be hard to analyse in the context of sets of semantically or linguistically related words. On the other hand, structured knowledge sources represent such relationships explicitly, but ignore the problem of semantic change. We aim to address these limitations by combining the statistical and symbolic approach: we enrich WordNet, a structured lexical database, with quantitative lexical change scores provided by HistWords, a dataset produced by distributional NLP methods. We publish the result as Linked Open Data and demonstrate how queries on the combined dataset can provide new insights. © 2016, CEUR-WS. All rights reserved.",Final,,
Srivastava N.; Perevalov A.; Kuchelev D.; Moussallem D.; Ngonga Ngomo A.-C.; Both A.,Lingua Franca - Entity-Aware Machine Translation Approach for Question Answering over Knowledge Graphs,1,1,1,#use,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180369135&doi=10.1145%2f3587259.3627567&partnerID=40&md5=65a582eb42da55e0bc56ce9e187cbe41,"This research paper proposes an approach called Lingua Franca that improves machine translation quality by utilizing information from a knowledge graph to translate named entities accurately. The accurate entity translation is crucial when applied to entity-oriented search including Knowledge Graph Question Answering systems. In a nutshell, the approach preserves recognized named entities with an entity-replacement technique during the translation process. It replaces the entities back with their labels found in a knowledge graph for the target language to ensure that questions are translated correctly before answering them using a Knowledge Graph Question Answering system. The paper also introduces an open-source modular framework that enables researchers to design their own named entity-aware machine translation pipelines. The presented experimental results demonstrate the effectiveness of the Lingua Franca approach in comparison to baseline Machine Translation models. The approach shows a statistically significant improvement in the quality provided by several Knowledge Graph Question Answering systems using Lingua Franca on different datasets. © 2023 ACM.",Final,,
AlMousa M.; Benlamri R.; Khoury R.,A novel word sense disambiguation approach using WordNet knowledge graph,1,1,1,#use #wordsensedisambiguation,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122705398&doi=10.1016%2fj.csl.2021.101337&partnerID=40&md5=ac2d90607397c902c5eaef4ae5251b0c,"Various applications in computational linguistics and artificial intelligence rely on high-performing word sense disambiguation techniques to solve challenging tasks such as information retrieval, machine translation, question answering, and document clustering. While text comprehension is intuitive for humans, machines face tremendous challenges in processing and interpreting a human's natural language. This paper presents a novel knowledge-based word sense disambiguation algorithm, namely Sequential Contextual Similarity Matrix Multiplication (SCSMM). The SCSMM algorithm combines semantic similarity, heuristic knowledge, and document context to respectively exploit the merits of local sense-based context between consecutive terms, human knowledge about terms, and a document's main topic in disambiguating terms. Unlike other algorithms, the SCSMM algorithm guarantees the capture of the maximum sentence context while maintaining the terms’ order within the sentence. The proposed algorithm outperformed all other algorithms when disambiguating nouns on the combined gold standard datasets, while demonstrating comparable results to current state-of-the-art word sense disambiguation systems when dealing with each dataset separately. Furthermore, the paper discusses the impact of granularity level, ambiguity rate, sentence size, and part of speech distribution on the performance of the proposed algorithm. © 2022 Elsevier Ltd",Final,All Open Access; Green Open Access,
Pedretti I.; Del Grosso A.; Giovannetti E.; Mancini L.; Piccini S.; Abrate M.; Lo Duca A.; Marchetti A.,"The clavius on the web project: Digitization, annotation and visualization of early modern manuscripts",1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958691991&doi=10.1145%2f2802612.2802636&partnerID=40&md5=62dee99529fbb39587a182d59688ec41,"This paper describes the full procedure adopted in the context of the Clavius on the Web project, which aims to help Web users to appraise the importance of specific manuscripts by going beyond their digital reproduction. The proposed approach is based on the multilayered explication of linguistic, lexical and semantic data representing the innermost nature of the analyzed manuscripts. The final purpose of the project is to gather and display the results of the three layers of analysis through interactive visualization techniques and export them as Linked Data. All the analyses rely on the XML/TEI encoding of the text, followed by a CTS-based tokenization. As a working example for this paper, the analysis of a portion of a manuscript provided by Historical Archives of the Pontifical Gregorian University will be illustrated. The text is a letter written in Latin and sent by Botvitus Nericius to Christophorus Clavius in 1598 from Madrid. © 2014 ACM.",Final,,
Li Y.; Clark P.,Answering elementary science questions by constructing coherent scenes using background knowledge,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959867303&doi=10.18653%2fv1%2fd15-1236&partnerID=40&md5=ca35d2968fd72770c7eabd9c013c4b46,"Much of what we understand from text is not explicitly stated. Rather, the reader uses his/her knowledge to fill in gaps and create a coherent, mental picture or ""scene"" depicting what text appears to convey. The scene constitutes an understanding of the text, and can be used to answer questions that go beyond the text. Our goal is to answer elementary science questions, where this requirement is pervasive; A question will often give a partial description of a scene and ask the student about implicit information. We show that by using a simple ""knowledge graph"" representation of the question, we can leverage several large-scale linguistic resources to provide missing background knowledge, somewhat alleviating the knowledge bottleneck in previous approaches. The coherence of the best resulting scene, built from a question/answer-candidate pair, reflects the confidence that the answer candidate is correct, and thus can be used to answer multiple choice questions. Our experiments show that this approach outperforms competitive algorithms on several datasets tested. The significance of this work is thus to show that a simple ""knowledge graph"" representation allows a version of ""interpretation as scene construction"" to be made viable. © 2015 Association for Computational Linguistics.",Final,All Open Access; Green Open Access; Hybrid Gold Open Access,
Wendt M.; Gerlach M.; Düwiger H.,Linguistic modeling of linked open data for question answering,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942694222&doi=10.1007%2f978-3-662-46641-4_8&partnerID=40&md5=5d225bae902853fe2896daa11926a55a,"With the evolution of linked open data sources, question answering regains importance as a way to make data accessible and explorable to the public. The triple structure of RDF-data at the same time seems to predetermine question answering for being devised in its native subject-verb-object form. The devices of natural language, however, often exceed this trFiple-centered model. But RDF does not preclude this point of view. Rather, it depends on the modeling. As part of a government funded research project named Alexandria, we implemented an approach to question answering that enables the user to ask questions in ways that may involve more than binary relations. © Springer-Verlag Berlin Heidelberg 2015.",Final,All Open Access; Green Open Access,
Franco-Salvador M.; Gupta P.; Rosso P.,Knowledge graphs as context models: Improving the detection of cross-language plagiarism with paraphrasing,1,1,1,#use #plagiarism-detection,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901341490&doi=10.1007%2f978-3-642-54798-0_12&partnerID=40&md5=38f36f4e100941b64bcdc632b5e709fd,"Cross-language plagiarism detection attempts to identify and extract automatically plagiarism among documents in different languages. Plagiarized fragments can be translated verbatim copies or may alter their structure to hide the copying, which is known as paraphrasing and is more difficult to detect. In order to improve the paraphrasing detection, we use a knowledge graph-based approach to obtain and compare context models of document fragments in different languages. Experimental results in German-English and Spanish-English cross-language plagiarism detection indicate that our knowledge graph-based approach offers a better performance compared to other state-of-the-art models. © 2014 Springer-Verlag.",Final,All Open Access; Green Open Access,
Li X.; Tur G.; Hakkani-Tur D.; Li Q.,Personal knowledge graph population from user utterances in conversational understanding,1,1,1,#use,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983195590&doi=10.1109%2fSLT.2014.7078578&partnerID=40&md5=1cdb2929b884ab85366b7f988ad9290f,"Knowledge graphs provide a powerful representation of entities and the relationships between them, but automatically constructing such graphs from spoken language utterances presents the novelty and numerous challenges. In this paper, we introduce a statistical language understanding approach to automatically construct personal (user-centric) knowledge graphs in conversational dialogs. Such information has the potential to better understand the users' requests, fulfilling them, and enabling other technologies such as developing better inferences or proactive interactions. Knowledge encoded in semantic graphs such as Freebase has been shown to benefit semantic parsing and interpretation of natural language utterances. Hence, as a first step, we exploit the personal factual relation triples from Freebase to mine natural language snippets with a search engine, and the resulting snippets containing pairs of related entities to create the training data. This data is then used to build three key language understanding components: (1) Personal Assertion Classification identifies the user utterances that are relevant with personal facts, e.g., 'my mother's name is Rosa'; (2) Relation Detection classifies the personal assertion utterance into one of the predefined relation classes, e.g., 'parents'; and (3) Slot Filling labels the attributes or arguments of relations, e.g., 'name(parents): Rosa'. Our experiments using the Microsoft conversational understanding system demonstrate the performance of this proposed approach on the population of personal knowledge graphs. © 2014 IEEE.",Final,,
Zugarini A.; Röthenbacher T.; Klede K.; Ernandes M.; Eskofier B.M.; Zanca D.,Die Rätselrevolution: Automated German Crossword Solving,1,1,1,#use,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181170035&partnerID=40&md5=e1a37ea17994eb346966d1a80adfd77c,"Crossword puzzles are popular word games played in various languages around the world, with diverse styles across different countries. For this reason, automated crossword solvers designed for a language, may not work well on others. In this paper, we extend Webcrow, an automatic crossword solver, to German, making it the first program for crossword solving in the German language. To address the lack of large clue-answer crossword pairs data, Webcrow combines multiple modules, known as experts, which retrieve potential answers from various resources, including the web, knowledge graphs, and linguistic rules. The system is evaluated on a collection of crosswords from variegate sources, where it is able to solve perfectly 67% of them. Additional analysis reveals that while our solver achieved commendable results, puzzles with poorly constrained schemas and original clues still presented significant hurdles. These findings shed light on the complexity of the crossword-solving problem and emphasize the need for future research to address and overcome these particular challenges effectively. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",Final,,
Pavithra C.P.; Mandal S.,An Overview of Relevant Literature on Different Approaches to Word Sense Disambiguation,1,1,1,#use #wordsensedisambiguation,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123448034&doi=10.1109%2fICECCT52121.2021.9616677&partnerID=40&md5=362966991d677e643d37bcc41ddb99cc,"WSD (Word Sense Disambiguation) is a common issue in Natural Language Processing (NLP) and Machine Learning technology. In NLP, word sense disambiguation is described as the capacity to detect which meaning of a word is activated by its use in a specific context. WSD is a solution to the uncertainty that occurs when words have different meanings in different contexts. Contextual word meaning plays an important role in various applications such as sentiment analysis, search engine, information extraction, machine translation etc. It is a challenge for these systems to detect and overcome the uncertainty that emerges from the lexical ambiguity. Many studies have been conducted over the decades to propose various approaches to the WSD problem. In this manuscript, a comparative study of three approaches namely LESK algorithm, embedding techniques, and Neural Network techniques based on the text collected from children's story books is performed. We explored an approach that combines Bi-LSTM neural network with Knowledge Graph to predict contextual word meaning. Our study shows that the combined approach accuracy is 80.34approaches © 2021 IEEE.",Final,,
Becker M.; Korfhage K.; Frank A.,COCO-EX: A tool for linking concepts from texts to conceptnet,1,1,1,#use,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107282443&partnerID=40&md5=bf9c125f24319e7264ffc336123d4791,"In this paper we present COCO-EX, a tool for Extracting Concepts from texts and linking them to the ConceptNet knowledge graph. COCO-EX extracts meaningful concepts from natural language texts and maps them to conjunct concept nodes in ConceptNet, utilizing the maximum of relational information stored in the ConceptNet knowledge graph. COCO-EX takes into account the challenging characteristics of ConceptNet, namely that - unlike conventional knowledge graphs - nodes are represented as non-canonicalized, free-form text. This means that i) concepts are not normalized; ii) they often consist of several different, nested phrase types; and iii) many of them are uninformative, over-specific, or misspelled. A commonly used shortcut to circumvent these problems is to apply string matching. We compare COCO-EX to this method and show that COCO-EX enables the extraction of meaningful, important rather than overspecific or uninformative concepts, and allows to assess more relational information stored in the knowledge graph. © 2021 Association for Computational Linguistics",Final,,
Chen M.; Shi W.; Zhou B.; Roth D.,Cross-lingual entity alignment with incidental supervision,1,1,1,#use #embeddings,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106367685&partnerID=40&md5=6ce87ce9d8dab67a1d80ecba9685f668,"Much research effort has been put to multilingual knowledge graph (KG) embedding methods to address the entity alignment task, which seeks to match entities in different language-specific KGs that refer to the same real-world object. Such methods are often hindered by the insufficiency of seed alignment provided between KGs. Therefore, we propose an incidentally supervised model, JEANS, which jointly represents multilingual KGs and text corpora in a shared embedding scheme, and seeks to improve entity alignment with incidental supervision signals from text. JEANS first deploys an entity grounding process to combine each KG with the monolingual text corpus. Then, two learning processes are conducted: (i) an embedding learning process to encode the KG and text of each language in one embedding space, and (ii) a self-learning based alignment learning process to iteratively induce the matching of entities and that of lexemes between embeddings. Experiments on benchmark datasets show that JEANS leads to promising improvement on entity alignment with incidental supervision, and significantly outperforms state-of-the-art methods that solely rely on internal information of KGs. © 2021 Association for Computational Linguistics",Final,,
Zhou P.; Gopalakrishnan K.; Hedayatnia B.; Kim S.; Pujara J.; Ren X.; Liu Y.; Hakkani-Tu D.,Commonsense-Focused Dialogues for Response Generation: An Empirical Study,1,1,1,#use #commonsense,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117308108&partnerID=40&md5=303ec9ab3fd4f438ba19544878e2e34c,"Smooth and effective communication requires the ability to perform latent or explicit commonsense inference. Prior commonsense reasoning benchmarks (such as SocialIQA and CommonsenseQA) mainly focus on the discriminative task of choosing the right answer from a set of candidates, and do not involve interactive language generation as in dialogue. Moreover, existing dialogue datasets do not explicitly focus on exhibiting commonsense as a facet. In this paper, we present an empirical study of commonsense in dialogue response generation. We first auto-extract commonsensical dialogues from existing dialogue datasets by leveraging ConceptNet, a commonsense knowledge graph. Furthermore, building on social contexts/situations in SocialIQA, we collect a new dialogue dataset with 25K dialogues aimed at exhibiting social commonsense in an interactive setting. We evaluate response generation models trained using these datasets and find that models trained on both extracted and our collected data produce responses that consistently exhibit more commonsense than baselines. Finally we propose an approach for automatic evaluation of commonsense that relies on features derived from ConceptNet and pretrained language and dialog models, and show reasonable correlation with human evaluation of responses' commonsense quality. ©2021 Association for Computational Linguistics.",Final,,
Bauer L.; Bansal M.,"Identify, align, and integrate: Matching knowledge graphs to commonsense reasoning tasks",1,1,1,#use #commonsense,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107268857&partnerID=40&md5=1a3d30a6d333c1d466373512665e1c17,"Integrating external knowledge into commonsense reasoning tasks has shown progress in resolving some, but not all, knowledge gaps in these tasks. For knowledge integration to yield peak performance, it is critical to select a knowledge graph (KG) that is well-aligned with the given task's objective. We present an approach to assess how well a candidate KG can correctly identify and accurately fill in gaps of reasoning for a task, which we call KG-to-task match. We show this KG-to-task match in 3 phases: knowledge-task identification, knowledge-task alignment, and knowledge-task integration. We also analyze our transformer-based KG-to-task models via commonsense probes to measure how much knowledge is captured in these models before and after KG integration. Empirically, we investigate KG matches for the SocialIQA (SIQA) (Sap et al., 2019b), Physical IQA (PIQA) (Bisk et al., 2020), and MCScript2.0 (Ostermann et al., 2019) datasets with 3 diverse KGs: ATOMIC (Sap et al., 2019a), ConceptNet (Speer et al., 2017), and an automatically constructed instructional KG based on WikiHow (Koupaee and Wang, 2018). With our methods we are able to demonstrate that ATOMIC, an event-inference focused KG, is the best match for SIQA and MCScript2.0, and that the taxonomic ConceptNet and WikiHow-based KGs are the best matches for PIQA across all 3 analysis phases. We verify our methods and findings with human evaluation. © 2021 Association for Computational Linguistics",Final,,
Bouziane A.; Bouchiha D.; Doumi N.,Annotating Arabic Texts with Linked Data,1,1,1,#use #arabic,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106857950&doi=10.1109%2fISIA51297.2020.9416543&partnerID=40&md5=ffe754a542a65be0b0f0ada6d5e53302,"The evolution of the traditional Web towards the semantic Web allows the machine to be a first-order citizen on the Web and increases discoverability of and accessibility to the unstructured data on the Web. This evolution enables the Linked Data technology to be used as background knowledge bases for unstructured data, notably the texts, available nowadays on the Web. For the Arabic language, the current situation is less brightness; the content of the Arabic language on the Web doesn't reflect the importance of this language. Given the fact that Arabic is one of the most important languages in the Web, and unfortunately it is under-resourced, so creating linguistic resources for it now is a necessity. Thus, we developed a linguistic approach for annotating Arabic textual corpus with Linked Data, especially DBpedia, which is Linked Open Data (LOD) extracted from Wikipedia. This approach uses natural language techniques to shedding light on Arabic text with Linked Open Data. The evaluation results of this approach are encouraging, despite the high complexity of our independent-domain knowledge base and the reduced resources in Arabic natural language processing. © 2020 IEEE.",Final,,
Kancheva Z.; Radev I.,Linguistic vs Encyclopaedic Knowledge. Classification of MWEs from Wikipedia Articles,1,1,1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098103687&doi=10.2478%2fcait-2020-0051&partnerID=40&md5=a78ab248628da4eb3d98a025ca7a4b6d,"This paper reports on the first steps in the creation of linked data through the mapping between the synsets of BTB-WordNet and the articles in Bulgarian Wikipedia. The task of expanding the BTB-WordNet with encyclopaedic knowledge is done by mapping its synsets to Wikipedia articles with many MWEs found in the articles and subjected to further analysis. We look for a way to filter the Wikipedia MWEs in the effort of selecting the ones most beneficial to the enrichment of BTB-WN. © 2020 Z. Kancheva et al., published by Sciendo 2020.",Final,All Open Access; Gold Open Access,
Winiwarter W.,JAMRED - A Japanese abstract meaning representation EDitor,1,1,1,#use WordNet,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967225539&doi=10.1145%2f2837185.2837246&partnerID=40&md5=eae858b0144bbc20b9a769a5da793aa3,"Meaning Representation (AMR) has emerged recently as one of the most influential formalisms for the semantic representation of natural language. In this paper, we present a Web-based environment for the annotation of Japanese Web pages using augmented browsing and logic programming as the two key enabling technologies. We have developed a visual AMR editor strongly integrated into our Web-based Japanese language learning environment, which hides most of the complexities of AMR syntax from the user and extends it by grounding concepts through links to WordNet synsets and DBpedia resources. The resulting tool can be used meaningfully for several language engineering tasks, in particular, language learning, corpus annotation, and translation. © 2015 ACM.",Final,,
Winiwarter W.,JILL - Japanese incidental language learning,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967145274&doi=10.1145%2f2837185.2837191&partnerID=40&md5=57f44e22da2b1e839b9e93bfae6dced2,"We present aWeb-based environment for incidental learning of Japanese. As the two key enabling technologies we use augmented browsing at the client and logic programming at the server. This way we can create a learning experience which blends foreign language acquisition with the user's everyday browsing activities. Main features presented in this paper are an intuitive visual presentation of sentence structures, comprehensive support at the word level by integrating several lexical resources including DBpedia, and multiple customization options. © 2015 ACM.",Final,,
Presutti V.; Nuzzolese A.G.; Consoli S.; Gangemi A.; Recupero D.R.,From hyperlinks to Semantic Web properties using Open Knowledge Extraction,1,1,1,#use,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971554079&doi=10.3233%2fSW-160221&partnerID=40&md5=ac1d14b2b6bd082d9df1734161891fb7,"Open information extraction approaches are useful but insufficient alone for populating the Web with machine readable information as their results are not directly linkable to, and immediately reusable from, other Linked Data sources. This work proposes a novel paradigm, named Open Knowledge Extraction, and its implementation (Legalo) that performs unsupervised, open domain, and abstractive knowledge extraction from text for producing machine readable information. The implemented method is based on the hypothesis that hyperlinks (either created by humans or knowledge extraction tools) provide a pragmatic trace of semantic relations between two entities, and that such semantic relations, their subjects and objects, can be revealed by processing their linguistic traces (i.e. the sentences that embed the hyperlinks) and formalised as Semantic Web triples and ontology axioms. Experimental evaluations conducted on validated text extracted from Wikipedia pages, with the help of crowdsourcing, confirm this hypothesis showing high performances. © 2016 - IOS Press and the authors.",Final,All Open Access; Green Open Access,
McCrae J.P.; Cimiano P.; Matteis L.; Navigli R.; Doncel V.R.; Vila-Suero D.; Gracia J.; Abele A.; Vulcu G.; Buitelaar P.,Reconciling Heterogeneous Descriptions of Language Resources,1,1,1,#use #reconciliation,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037062941&partnerID=40&md5=ed7fb332868691d899ab1410be3ce03d,"Language resources are a cornerstone of linguistic research and for the development of natural language processing tools, but the discovery of relevant resources remains a challenging task. This is due to the fact that relevant metadata records are spread among different repositories and it is currently impossible to query all these repositories in an integrated fashion, as they use different data models and vocabularies. In this paper we present a first attempt to collect and harmonize the metadata of different repositories, thus making them queriable and browsable in an integrated way. We make use of RDF and linked data technologies for this and provide a first level of harmonization of the vocabularies used in the different resources by mapping them to standard RDF vocabularies including Dublin Core and DCAT. Further, we present an approach that relies on NLP and in particular word sense disambiguation techniques to harmonize resources by mapping values of attributes - such as the type, license or intended use of a resource - into normalized values. Finally, as there are duplicate entries within the same repository as well as across different repositories, we also report results of detection of these duplicates. © 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing.",Final,,
Cheniki N.; Belkhir A.; Atif Y.,Supporting multilingual semantic Web services discovery by consuming data from DBpedia knowledge base,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959933874&doi=10.1145%2f2816839.2816862&partnerID=40&md5=a79701a17e143e5939ac809da64d1d98,"The Web is becoming a truly multilingual hub in which speakers from different languages and cultures are producing, interacting and consuming information. Web services are an essential part of the Web which nowadays attract more and more people around the world. So, supporting multilingual Web services discovery is a crucial goal to achieve, so that providers and users publish or consume Web services independently from their culture and native language. In this paper, we propose to overcome language barrier by supporting multilingual Web services discovery using DB-pedia, which is a cross-domain multilingual knowledge base. DBpedia is used to annotate services with semantic entities called resources as well as their categories and types. We take advantage of semantic and multilingual information provided by DBpedia to enable cross-language semantic Web services discovery. Implementation shows that DBpedia offers a valuable information source to achieve our goal. © 2015 ACM.",Final,,
Mohamed R.; El-Makky N.M.; Nagi K.,ArabRelat: Arabic relation extraction using distant supervision,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961176131&doi=10.5220%2f0005636604100417&partnerID=40&md5=2dd482bce31d0abcd1362f835dce4eb3,"Relation Extraction is an important preprocessing task for a number of text mining applications, including: Information Retrieval, Question Answering, Ontology building, among others. In this paper, we propose a novel Arabic relation extraction method that leverages linguistic features of the Arabic language in Web data to infer relations between entities. Due to the lack of labeled Arabic corpora, we adopt the idea of distant supervision, where DBpedia, a large database of semantic relations extracted from Wikipedia, is used along with a large unlabeled text corpus to build the training data. We extract the sentences from the unlabeled text corpus, and tag them using the corresponding DBpedia relations. Finally, we build a relation classifier using this data which predicts the relation type of new instances. Our experimental results show that the system reaches 70% for the F-measure in detecting relations. Copyright © 2015 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",Final,,
Dragoni M.; Tettamanzi A.G.B.; da Costa Pereira C.,Propagating and Aggregating Fuzzy Polarities for Concept-Level Sentiment Analysis,1,1,1,#use #sentimentanalysis WordNet and senticnet,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926322258&doi=10.1007%2fs12559-014-9308-6&partnerID=40&md5=76b58e9064aa0df3998923e453fa0fa6,"An emerging field within sentiment analysis concerns the investigation about how sentiment polarities associated with concepts have to be adapted with respect to the different domains in which they are used. In this paper, we explore the use of fuzzy logic for modeling concept polarities, and the uncertainty associated with them, with respect to different domains. The approach is based on the use of a knowledge graph built by combining two linguistic resources, namely WordNet and SenticNet. Such a knowledge graph is then exploited by a graph-propagation algorithm that propagates sentiment information learned from labeled datasets. The system implementing the proposed approach has been evaluated on the Blitzer dataset. The results demonstrate its viability in real-world cases. © 2014, Springer Science+Business Media New York.",Final,All Open Access; Green Open Access,
Álvarez A.A.,"Enriching Digitized Medieval Manuscripts: Linking Image, Text and Lexical Knowledge",1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122503695&partnerID=40&md5=a7452c56d9465f76100490ec9185d749,"This paper describes an on-going project of transcribing and annotating digitized manuscripts of medieval Spanish with paleographic and lexical information. We link lexical units from the manuscripts with the Multilingual Central Repository (MCR), making terms retrievable by any of the languages that integrate MCR. The goal of the project is twofold: creating a paleographic knowledge base from digitized medieval facsimiles, that will allow paleographers, philologist, historical linguist, and humanities scholars in general, to analyze and retrieve graphemic, lexical and textual information from historical documents; and on the other hand, developing machine readable documents that will link image representations of graphemic and lexical units in a facsimile with Linked Open Data resources. This paper concentrates on the encoding and cross-linking procedures. c 2015 Association for Computational Linguistics and The Asian Federation of Natural Language Processing. © 2015 Proceedings of the Annual Meeting of the Association for Computational Linguistics.",Final,,
Siemoneit B.; McCrae J.P.; Cimiano P.,Linking four heterogeneous language resources as linked data,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009269613&partnerID=40&md5=dfbe34f7414893e5c07f0c81c0ec3ccb,"The interest in publishing language resources as linked data is increasing, as clearly corroborated by the recent growth of the Linguistic Linked Data cloud. However, the actual value of data published as linked data is the fact that it is linked across datasets, supporting integration and discovery of data. As the manual creation of links between datasets is costly and therefore does not scale well, automatic linking approaches are of great importance to increase the quality and degree of linking of the Linguistic Linked Data cloud. In this paper we examine an automatic approach to link four different datasets to each other: Two terminologies, the European Migration Network (EMN) glossary as well as the Interactive Terminology for Europe (IATE), BabelNet, and the Manually Annotated Subcorpus (MASC) of the American National Corpus. We describe our methodology, present some results on the quality of the links and summarize our experiences with this small linking exercise We will make sure that the resources are added to the linguistic linked data cloud. © 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing.",Final,,
Chen Y.-N.; Wang W.Y.; Rudnicky A.I.,Jointly modeling inter-slot relations by random walk on knowledge graphs for unsupervised spoken language understanding,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959178594&doi=10.3115%2fv1%2fn15-1064&partnerID=40&md5=063c7e0df5060b318024b813d9264f66,"A key challenge of designing coherent semantic ontology for spoken language understanding is to consider inter-slot relations. In practice, however, it is difficult for domain experts and professional annotators to define a coherent slot set, while considering various lexical, syntactic, and semantic dependencies. In this paper, we exploit the typed syntactic dependency theory for unsupervised induction and filling of semantics slots in spoken dialogue systems. More specifically, we build two knowledge graphs: a slot-based semantic graph, and a word-based lexical graph. To jointly consider word-to-word, word-toslot, and slot-to-slot relations, we use a random walk inference algorithm to combine the two knowledge graphs, guided by dependency grammars. The experiments show that considering inter-slot relations is crucial for generating a more coherent and compete slot set, resulting in a better spoken language understanding model, while enhancing the interpretability of semantic slots. © 2015 Association for Computational Linguistics.",Final,All Open Access; Green Open Access; Hybrid Gold Open Access,
Arcan M.; Turchi M.; Buitelaar P.,Knowledge portability with semantic expansion of ontology labels,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943746735&doi=10.3115%2fv1%2fp15-1069&partnerID=40&md5=cb59323a8b2b76877cf4445da028a55e,"Our research focuses on the multilingual enhancement of ontologies that, often represented only in English, need to be translated in different languages to enable knowledge access across languages. Ontology translation is a rather different task then the classic document translation, because ontologies contain highly specific vocabulary and they lack contextual information. For these reasons, to improve automatic ontology translations, we first focus on identifying relevant unambiguous and domain-specific sentences from a large set of generic parallel corpora. Then, we leverage Linked Open Data resources, such as DBPedia, to isolate ontologyspecific bilingual lexical knowledge. In both cases, we take advantage of the semantic information of the labels to select relevant bilingual data with the aim of building an ontology-specific statistical machine translation system. We evaluate our approach on the translation of a medical ontology, translating from English into German. Our experiment shows a significant improvement of around 3 BLEU points compared to a generic as well as a domain-specific translation approach. © 2015 Association for Computationl Linguisticss.",Final,All Open Access; Green Open Access; Hybrid Gold Open Access,
Perera R.; Nand P.,A multi-strategy approach for lexicalizing linked open data,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942510700&doi=10.1007%2f978-3-319-18117-2_26&partnerID=40&md5=377178b92a49433985cfee371f267073,"This paper aims at exploiting Linked Data for generating natural text, often referred to as lexicalization. We propose a framework that can generate patterns which can be used to lexicalize Linked Data triples. Linked Data is structured knowledge organized in the form of triples consisting of a subject, a predicate and an object. We use DBpedia as the Linked Data source which is not only free but is currently the fastest growing data source organized as Linked Data. The proposed framework utilizes the Open Information Extraction (OpenIE) to extract relations from natural text and these relations are then aligned with triples to identify lexicalization patterns. We also exploit lexical semantic resources which encode knowledge on lexical, semantic and syntactic information about entities. Our framework uses VerbNet and WordNet as semantic resources. The extracted patterns are ranked and categorized based on the DBpedia ontology class hierarchy. The pattern collection is then sorted based on the score assigned and stored in an index embedded database for use in the framework as well as for future lexical resource. The framework was evaluated for syntactic accuracy and validity by measuring the Mean Reciprocal Rank (MRR) of the first correct pattern. The results indicated that framework can achieve 70.36% accuracy and a MRR value of 0.72 for five DBpedia ontology classes generating 101 accurate lexicalization patterns. © Springer International Publishing Switzerland 2015.",Final,,
Li Q.; Liu S.; Lin R.; Li M.; Zhou M.,Entity translation with collective inference in knowledge graph,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951275676&doi=10.1007%2f978-3-319-25207-0_5&partnerID=40&md5=5981a3d872977177e02b88848fe0a77c,"Nowadays knowledge base (KB) has been viewed as one of the important infrastructures for many web search applications and NLP tasks. However, in practice the availability of KB data varies from language to language, which greatly limits potential usage of knowledge base. In this paper, we propose a novel method to construct or enrich a knowledge base by entity translation with help of another KB but compiled in a different language. In our work, we concentrate on two key tasks: 1) collecting translation candidates with as good coverage as possible from various sources such as web or lexicon; 2) building an effective disambiguation algorithm based on collective inference approach over knowledge graph to find correct translation for entities in the source knowledge base. We conduct experiments on movie domain of our inhouse knowledge base from English to Chinese, and the results show the proposed method can achieve very high translation precision compared with classical translation methods, and significantly increase the volume of Chinese knowledge base in this domain. © Springer International Publishing Switzerland 2015.",Final,,
Chen Y.; Skiena S.,Building sentiment lexicons for all major languages,1,1,1,#use,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906927782&doi=10.3115%2fv1%2fp14-2063&partnerID=40&md5=21feca95c076bbfb9e61213e4383cbf7,"Sentiment analysis in a multilingual world remains a challenging problem, because developing language-specific sentiment lexicons is an extremely resourceintensive process. Such lexicons remain a scarce resource for most languages. In this paper, we address this lexicon gap by building high-quality sentiment lexicons for 136 major languages. We integrate a variety of linguistic resources to produce an immense knowledge graph. By appropriately propagating from seed words, we construct sentiment lexicons for each component language of our graph. Our lexicons have a polarity agreement of 95.7% with published lexicons, while achieving an overall coverage of 45.2%. We demonstrate the performance of our lexicons in an extrinsic analysis of 2,000 distinct historical figures' Wikipedia articles on 30 languages. Despite cultural difference and the intended neutrality of Wikipedia articles, our lexicons show an average sentiment correlation of 0.28 across all language pairs. © 2014 Association for Computational Linguistics.",Final,All Open Access; Bronze Open Access; Green Open Access,
Di Buono M.P.; Monteleone M.; Elia A.,Terminology and Knowledge Representation Italian Linguistic Resources for the Archaeological Domain,1,1,1,#use #domainspecific,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978822095&partnerID=40&md5=ef7bef62c8826d6dc79abd7bef319b80,"Knowledge representation is heavily based on using terminology, due to the fact that many terms have precise meanings in a specific domain but not in others. As a consequence, terms becomes unambiguous and clear, and at last, being useful for conceptualizations, are used as a starting point for formalizations. Starting from an analysis of problems in existing dictionaries, in this paper we present formalized Italian Linguistic Resources (LRs) for the Archaeological domain, in which we integrate/couple formal ontology classes and properties into/to electronic dictionary entries, using a standardized conceptual reference model. We also add Linguistic Linked Open Data (LLOD) references in order to guarantee the interoperability between linguistic and language resources, and therefore to represent knowledge. © COLING 2014. All rights reserved.",Final,,
Walter S.; Unger C.; Cimiano P.,A corpus-based approach for the induction of ontology lexica,1,1,1,#use,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884933747&doi=10.1007%2f978-3-642-38824-8_9&partnerID=40&md5=6d5dbc4fbc39c798ec928d0de77de5d1,"While there are many large knowledge bases (e.g. Freebase, Yago, DBpedia) as well as linked data sets available on the web, they typically lack lexical information stating how the properties and classes are realized lexically. If at all, typically only one label is attached to these properties, thus lacking any deeper syntactic information, e.g. about syntactic arguments and how these map to the semantic arguments of the property as well as about possible lexical variants or paraphrases. While there are lexicon models such as lemon allowing to define a lexicon for a given ontology, the cost involved in creating and maintaining such lexica is substantial, requiring a high manual effort. Towards lowering this effort, in this paper we present a semi-automatic approach that exploits a corpus to find occurrences in which a given property is expressed, and generalizing over these occurrences by extracting dependency paths that can be used as a basis to create lemon lexicon entries. We evaluate the resulting automatically generated lexica with respect to DBpedia as dataset and Wikipedia as corresponding corpus, both in an automatic mode, by comparing to a manually created lexicon, and in a semi-automatic mode in which a lexicon engineer inspected the results of the corpus-based approach, adding them to the existing lexicon if appropriate. © 2013 Springer-Verlag Berlin Heidelberg.",Final,All Open Access; Green Open Access,
Schneidermann N.S.; Pedersen B.S.,"Evaluating a New Danish Sentiment Resource: the Danish Sentiment Lexicon, DSL",1,1,1,#use,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146225959&partnerID=40&md5=f56055c8d4bfb8efa5e69f27e218c159,"In this paper, we evaluate a new sentiment lexicon for Danish, the Danish Sentiment Lexicon (DSL), to gain input regarding how to carry out the final adjustments of the lexicon. A feature of the lexicon that differentiates it from other sentiment resources for Danish is that it is linked to a large number of other Danish lexical resources via the DDO lemma and sense inventory and the LLOD via the Danish WordNet, DanNet. We perform our evaluation on four datasets labeled with sentiments. In addition, we compare the lexicon against two existing benchmarks for Danish: the Afinn and the Sentida resources. We observe that DSL performs mostly comparably to the existing resources, but that more fine-grained explorations need to be done in order to fully exploit its possibilities given its linking properties. © European Language Resources Association (ELRA), licensed under CC-BY-NC-4.0.",Final,,
Sun Y.; Shi Q.; Qi L.; Zhang Y.,JointLK: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering,1,1,1,#use,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138372259&partnerID=40&md5=5e07a3b2313a072bb3f369edb054dac8,"Existing KG-augmented models for commonsense question answering primarily focus on designing elaborate Graph Neural Networks (GNNs) to model knowledge graphs (KGs). However, they ignore (i) the effectively fusing and reasoning over question context representations and the KG representations, and (ii) automatically selecting relevant nodes from the noisy KGs during reasoning. In this paper, we propose a novel model, JointLK, which solves the above limitations through the joint reasoning of LM and GNN and the dynamic KGs pruning mechanism. Specifically, JointLK performs joint reasoning between LM and GNN through a novel dense bidirectional attention module, in which each question token attends on KG nodes and each KG node attends on question tokens, and the two modal representations fuse and update mutually by multi-step interactions. Then, the dynamic pruning module uses the attention weights generated by joint reasoning to prune irrelevant KG nodes recursively. We evaluate JointLK on the CommonsenseQA and OpenBookQA datasets, and demonstrate its improvements to the existing LM and LM+KG models, as well as its capability to perform interpretable reasoning. © 2022 Association for Computational Linguistics.",Final,,
Kirillovich A.; Nikolaev K.,Adapting the LodView RDF Browser for Navigation over the Multilingual Linguistic Linked Open Data Cloud,1,1,1,#use,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138990992&doi=10.1109%2fSETIT54465.2022.9875628&partnerID=40&md5=6ad70bd7390e781408048147b270f432,"This paper is dedicated to using of LodView RDF browser for navigation over the multilingual Linguistic Linked Open Data cloud. We reveal several limitations of LodView that impede its use for this purpose, and propose improvements to be made for fixing these limitations. These improvements are: 1) resolution of Cyrillic URIs; 2) decoding Cyrillic URIs in Turtle representations of resources; 3) support of Cyrillic literals; 4) user-friendly URLs for RDF representations of resources; 5) support of hash URIs; 6) expanding nested resources; 7) support of RDF collections; 8) support of LATEX math notation; and 9) pagination of resource property values. We implement several of the proposed improvements. © 2022 IEEE.",Final,All Open Access; Green Open Access,
Porzel R.; Pomarlan M.; Spillner L.; Bateman J.; Mildner T.; Santagiustina C.,Narrativizing Knowledge Graphs,1,1,1,#use,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142493609&partnerID=40&md5=97e9b0544dd52d6554863b5fbd4e8177,"Any natural language expression of a set of facts - that can be represented as a knowledge graph - will more or less overtly assume a specific perspective on these facts. In this paper we see the conversion of a given knowledge graph into natural language as the construction of a narrative about the assertions made by the knowledge graph. We, therefore, propose a specific pipeline that can be applied to produce linguistic narratives from knowledge graphs using an ontological layer and corresponding rules that turn a knowledge graph into a semantic specification for natural language generation. Critically, narratives are seen as necessarily committing to specific perspectives taken on the facts presented. We show how this most commonly neglected facet of producing summaries of facts can be brought under control. © 2021 Copyright for this paper by its authors.",Final,,
AlMousa M.; Benlamri R.; Khoury R.,Exploiting non-taxonomic relations for measuring semantic similarity and relatedness in WordNet,1,1,1,#use,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095852763&doi=10.1016%2fj.knosys.2020.106565&partnerID=40&md5=5573a5a128dd93ea1e4d6a4101770633,"Various applications in computational linguistics and artificial intelligence employ semantic similarity to solve challenging tasks, such as word sense disambiguation, text classification, information retrieval, machine translation, and document clustering. To our knowledge, research to date rely solely on the taxonomic relation “ISA” to evaluate semantic similarity and relatedness between terms. This paper explores the benefits of using all types of non-taxonomic relations in large linked data, such as WordNet knowledge graph, to enhance existing semantic similarity and relatedness measures. We propose a holistic poly-relational approach based on a new relation-based information content and non-taxonomic-based weighted paths to devise a comprehensive semantic similarity and relatedness measure. To demonstrate the benefits of exploiting non-taxonomic relations in a knowledge graph, we used three strategies to deploy non-taxonomic relations at different granularity levels. We conduct experiments on four well-known gold standard datasets. The results of our proposed method demonstrate an improvement over the benchmark semantic similarity methods, including the state-of-the-art knowledge graph embedding techniques, that ranged from 3.8%–23.8%, 1.3%–18.3%, 31.8%–117.2%, and 19.1%–111.1%, on all gold standard datasets MC, RG, WordSim, and Mturk, respectively. These results demonstrate the robustness and scalability of the proposed semantic similarity and relatedness measure, significantly improving existing similarity measures. © 2020 Elsevier B.V.",Final,All Open Access; Green Open Access,
Sprugnoli R.; Passarotti M.; Testori M.; Moretti G.,Extending and Using a Sentiment Lexicon for Latin in a Linked Data Framework,1,1,1,#use,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126062402&partnerID=40&md5=9b41dc7547f84161c3b88dc7d0c48d59,"In this paper we present the methodology followed to extend a Latin sentiment lexicon (called LatinAffectus), the process of inclusion of the lexicon in a knowledge base of interoperable linguistic resources for Latin and one use case performed on the treebank of Dante Alighieri's Latin works annotated following the Universal Dependencies guidelines. In addition, we report on our first attempt at linking the polarity scores of SentiWordNet 3.0 to a manually revised version of Latin WordNet. © 2021 Copyright for this paper by its authors",Final,,
Martín-Chozas P.; Ahmadi S.; Montiel-Ponsoda E.,Defying wikidata: Validation of terminological relations in the web of data,1,1,1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096530716&partnerID=40&md5=8658e3c7154fd8ea391e1f3a2b1933ab,"In this paper we present an approach to validate terminological data retrieved from open encyclopaedic knowledge bases. This need arises from the enrichment of automatically extracted terms with information from existing resources in the Linguistic Linked Open Data cloud. Specifically, the resource employed for this enrichment is WIKIDATA, since it is one of the biggest knowledge bases freely available within the Semantic Web. During the experiment, we noticed that certain RDF properties in the Knowledge Base did not contain the data they are intended to represent, but a different type of information. In this paper we propose an approach to validate the retrieved data based on four axioms that rely on two linguistic theories: the x-bar theory and the multidimensional theory of terminology. The validation process is supported by a second knowledge base specialised in linguistic data; in this case, CONCEPTNET. In our experiment, we validate terms from the legal domain in four languages: Dutch, English, German and Spanish. The final aim is to generate a set of sound and reliable terminological resources in RDF to contribute to the population of the Linguistic Linked Open Data cloud. © European Language Resources Association (ELRA), licensed under CC-BY-NC",Final,,
Gracia J.; Fäth C.; Hartung M.; Ionov M.; Bosque-Gil J.; Veríssimo S.; Chiarcos C.; Orlikowski M.,Leveraging Linguistic Linked Data for Cross-Lingual Model Transfer in the Pharmaceutical Domain,1,1,1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096625207&doi=10.1007%2f978-3-030-62466-8_31&partnerID=40&md5=4760f3754c42b51d2fd3ed570ec01f69,"We describe the use of linguistic linked data to support a cross-lingual transfer framework for sentiment analysis in the pharmaceutical domain. The proposed system dynamically gathers translations from the Linked Open Data (LOD) cloud, particularly from Apertium RDF, in order to project a deep learning-based sentiment classifier from one language to another, thus enabling scalability and avoiding the need of model re-training when transferred across languages. We describe the whole pipeline traversed by the multilingual data, from their conversion into RDF based on a new dynamic and flexible transformation framework, through their linking and publication as linked data, and finally their exploitation in the particular use case. Based on experiments on projecting a sentiment classifier from English to Spanish, we demonstrate how linked data techniques are able to enhance the multilingual capabilities of a deep learning-based approach in a dynamic and scalable way, in a real application scenario from the pharmaceutical domain. © 2020, Springer Nature Switzerland AG.",Final,All Open Access; Green Open Access,
Simov K.,Integrated Language and Knowledge Resources for a Bulgarian-Centric Knowledge Graph,1,1,1,#use #integration,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084240830&partnerID=40&md5=f16110765eac6784e8dd66b825ae97ed,"This paper reports on the integration of language and knowledge resources within CLaDA-BG infrastructure. The idea is to encode linguistic knowledge on all levels of language starting from text, grammatical annotation and lemmatization to semantic and conceptual annotation. Our goal is to support conceptual annotation of various research objects (mainly texts). One of the main applications will be the management of a Bulgaria-centric Knowledge Graph. © 2019 Digital Presentation and Preservation of Cultural and Scientific Heritage. All rights reserved.",Final,,
Sakor A.; Mulang I.O.; Singh K.; Shekarpour S.; Vidal M.-E.; Lehmann J.; Auer S.,Old is gold: Linguistic driven approach for entity and relation linking of short text,1,,1,#use,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081090168&partnerID=40&md5=d93ebebd95c7e03c292d4b885c224512,"Short texts challenge NLP tasks such as named entity recognition, disambiguation, linking and relation inference because they do not provide sufficient context or are partially malformed (e.g. wrt. capitalization, long tail entities, implicit relations). In this work, we present the Falcon approach which effectively maps entities and relations within a short text to its mentions of a background knowledge graph. Falcon overcomes the challenges of short text using a light-weight linguistic approach relying on a background knowledge graph. Falcon performs joint entity and relation linking of a short text by leveraging several fundamental principles of English morphology (e.g. compounding, headword identification) and utilizes an extended knowledge graph created by merging entities and relations from various knowledge sources. It uses the context of entities for finding relations and does not require training data. Our empirical study using several standard benchmarks and datasets show that Falcon significantly outperforms state-of-the-art entity and relation linking for short text query inventories. © 2019 Association for Computational Linguistics",Final,,
Chiarcos C.; Ionov M.,Ligt: An LLod-native vocabulary for representing interlinear glossed text as RDF,1,,1,#use,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068056004&doi=10.4230%2fOASIcs.LDK.2019.3&partnerID=40&md5=1194a6ce49aa6b6b03fd05f91410d0ce,"The paper introduces Ligt, a native RDF vocabulary for representing linguistic examples as text with interlinear glosses (IGT) in a linked data formalism. Interlinear glossing is a notation used in various fields of linguistics to provide readers with a way to understand linguistic phenomena and to provide corpus data when documenting endangered languages. This data is usually provided with morpheme-by-morpheme correspondence which is not supported by any established vocabularies for representing linguistic corpora or automated annotations. Interlinear Glossed Text can be stored and exchanged in several formats specifically designed for the purpose, but these differ in their designs and concepts, and they are tied to particular tools, so the reusability of the annotated data is limited. To improve interoperability and reusability, we propose to convert such glosses to a tool-independent representation well-suited for the Web of Data, i.e., a representation in RDF. Beyond establishing structural (format) interoperability by means of a common data representation, our approach also allows using shared vocabularies and terminology repositories available from the (Linguistic) Linked Open Data cloud. We describe the core vocabulary and the converters that use this vocabulary to convert IGT in a format of various widely-used tools into RDF. Ultimately, a Linked Data representation will facilitate the accessibility of language data from less-resourced language varieties within the (Linguistic) Linked Open Data cloud, as well as enable novel ways to access and integrate this information with (L)LOD dictionary data and other types of lexical-semantic resources. In a longer perspective, data currently only available through these formats will become more visible and reusable and contribute to the development of a truly multilingual (semantic) web. © Christian Chiarcos and Maxim Ionov.",Final,,
Cremaschi M.; Bianchi F.; Maurino A.; Pierotti A.P.,Supporting journalism by combining neural language generation and knowledge graphs,1,,1,#use,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074854423&partnerID=40&md5=deed4622ec53e2a0fcdc4b44f6eb420e,"Natural Language Generation is a field that is becoming relevant in several domains, including journalism. Natural Language Generation techniques can be of great help to journalists, allowing a substantial reduction in the time required to complete repetitive tasks. In this position paper, we enforce the idea that automated tools can reduce the effort required to journalist when writing articles; at the same time we introduce GazelLex (Gazette Lexicalization), a prototype that covers several steps of Natural Language Generation, in order to create soccer articles automatically, using data from Knowledge Graphs, leaving journalists the possibility of refining and editing articles with additional information. We shall present our first results and current limits of the approach, and we shall also describe some lessons learned that might be useful to readers that want to explore this field. Copyright 2019 for this paper by its authors.",Final,,
Chiarcos C.; Pagé-Perron É.; Khait I.; Schenk N.; Reckling L.,Towards a linked open data edition of sumerian corpora,1,,1,#use,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056772672&partnerID=40&md5=9c5604b58788ffbe8e0b5eee8b7c38aa,"Linguistic Linked Open Data (LLOD) is a flourishing line of research in the language resource community, so far mostly adopted for selected aspects of linguistics, natural language processing and the semantic web, as well as for practical applications in localization and lexicography. Yet, computational philology seems to be somewhat decoupled from the recent progress in this area: even though LOD as a concept is gaining significant popularity in Digital Humanities, existing LLOD standards and vocabularies are not widely used in this community, and philological resources are underrepresented in the LLOD cloud diagram (http://linguistic-lod.org/llod-cloud). In this paper, we present an application of Linguistic Linked Open Data in Assyriology. We describe the LLOD edition of a linguistically annotated corpus of Sumerian, as well as its linking with lexical resources, repositories of annotation terminology, and the museum collections in which the artifacts bearing these texts are kept. The chosen corpus is the Electronic Text Corpus of Sumerian Royal Inscriptions, a well curated and linguistically annotated archive of Sumerian text, in preparation for the creating and linking of other corpora of cuneiform texts, such as the corpus of Ur III administrative and legal Sumerian texts, as part of the Machine Translation and Automated Analysis of Cuneiform Languages project (https://cdli-gh.github.io/mtaac/). © LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved.",Final,,
Wang C.; Fan Y.; He X.; Zhou A.,Predicting hypernym–hyponym relations for Chinese taxonomy learning,1,,1,#use,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041836685&doi=10.1007%2fs10115-018-1166-1&partnerID=40&md5=372d88ca7d561d032b7c75d1df98b1a5,"Hypernym–hyponym (“is-a”) relations are key components in taxonomies, object hierarchies and knowledge graphs. Robustly harvesting of such relations requires the analysis of the linguistic characteristics of is-a word pairs in the target language. While there is abundant research on is-a relation extraction in English, it still remains a challenge to accurately identify such relations from Chinese knowledge sources due to the flexibility of language expression and the significant differences between the two language families. In this paper, we introduce a weakly supervised framework to extract Chinese is-a relations from user-generated categories. It employs piecewise linear projection models trained on an existing Chinese taxonomy built from Wikipedia and an iterative learning algorithm to update model parameters incrementally. A pattern-based relation selection method is proposed to prevent “semantic drift” in the learning process using bi-criteria optimization. Experimental results on the publicly available test set illustrate that the proposed approach outperforms state-of-the-art methods. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Final,,
Simov K.; Osenova P.,Special thematic section on semantic models for natural language processing,1,,1,#use,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044005006&doi=10.2478%2fcait-2018-0008&partnerID=40&md5=96a18a0b072e4fb358c5aaff3fb263de,"With the availability of large language data online, cross-linked lexical resources (such as BabelNet, Predicate Matrix and UBY) and semantically annotated corpora (SemCor, OntoNotes, etc.), more and more applications in Natural Language Processing (NLP) have started to exploit various semantic models. The semantic models have been created on the base of LSA, clustering, word embeddings, deep learning, neural networks, etc., and abstract logical forms, such as Minimal Recursion Semantics (MRS) or Abstract Meaning Representation (AMR), etc. Additionally, the Linguistic Linked Open Data Cloud has been initiated (LLOD Cloud) which interlinks linguistic data for improving the tasks of NLP. This cloud has been expanding enormously for the last four-five years. It includes corpora, lexicons, thesauri, knowledge bases of various kinds, organized around appropriate ontologies, such as LEMON. The semantic models behind the data organization as well as the representation of the semantic resources themselves are a challenge to the NLP community. The NLP applications that extensively rely on the above discussed models include Machine Translation, Information Extraction, Question Answering, Text Simplification, etc. © 2001-2018 Institute of Information and Communication Technologies at Bulgarian Academy of Sciences.",Final,All Open Access; Gold Open Access,
Rouces J.; De Melo G.; Hose K.,FrameBase: Enabling integration of heterogeneous knowledge,1,,1,#use,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027393990&doi=10.3233%2fSW-170279&partnerID=40&md5=9ac1b3786a699d788b9981f53c067da2,"Large-scale knowledge graphs such as those in the Linked Open Data cloud are typically stored as subject-predicate-object triples. However, many facts about the world involve more than two entities. While n-ary relations can be converted to triples in a number of ways, unfortunately, the structurally different choices made in different knowledge sources significantly impede our ability to connect them. They also increase semantic heterogeneity, making it impossible to query the data concisely and without prior knowledge of each individual source. This article presents FrameBase, a wide-coverage knowledge base schema that uses linguistic frames to represent and query n-ary relations from other knowledge bases, providing multiple levels of granularity connected via logical entailment. Overall, this provides a means for semantic integration from heterogeneous sources under a single schema and opens up possibilities to draw on natural language processing techniques for querying and data mining. © 2017 - IOS Press and the authors. All rights reserved.",Final,All Open Access; Green Open Access,
Chiarcos C.; Ionov M.; Rind-Pawlowski M.; Fäth C.; Schreur J.W.; Nevskaya I.,LLODifying linguistic glosses,1,,1,#use,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021193597&doi=10.1007%2f978-3-319-59888-8_7&partnerID=40&md5=ef597d9ae10042b6d31316b2cdcc00b1,"Interlinear glossed text (IGT) is a notation used in various fields of linguistics to provide readers with a way to understand the linguistic phenomena. We describe the representation of IGT data in RDF, the conversion from two popular tools, and their automated linking with resources from the Linguistic Linked Open Data (LLOD) cloud. We argue that such an LLOD edition of IGT data facilitates their reusability, their infrastructural support and their integration with external data sources. Our converters are available under an open source license, two data sets will be published along with the final version of this paper. To our best knowledge, this is the first attempt to publish IGT data sets as Linguistic Linked Open Data we are aware of. © Springer International Publishing AG 2017.",Final,,
List J.-M.; Cysouw M.; Forkel R.,Concepticon: A resource for the linking of concept lists,1,,1,#use,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037152460&partnerID=40&md5=3122a0ba89f81396469e2cb366878186,"We present an attempt to link the large amount of different concept lists which are used in the linguistic literature, ranging from Swadesh lists in historical linguistics to naming tests in clinical studies and psycholinguistics. This resource, our Concepticon, links 30 222 concept labels from 160 conceptlists to 2495 concept sets. Each concept set is given a unique identifier, a unique label, and a human-readable definition. Concept sets are further structured by defining different relations between the concepts. The resource can be used for various purposes. Serving as a rich reference for new and existing databases in diachronic and synchronic linguistics, it allows researchers a quick access to studies on semantic change, cross-linguistic polysemies, and semantic associations.",Final,,
Bennacer N.; Vioulès M.J.; López M.A.; Quercini G.,A multilingual approach to discover cross-language links in wikipedia,1,,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949963735&doi=10.1007%2f978-3-319-26190-4_36&partnerID=40&md5=c82c2a83d3eec7745f518d9a128b6b1e,"Wikipedia is a well-known public and collaborative encyclopaedia consisting of millions of articles. Initially in English, the popular website has grown to include versions in over 288 languages. These versions and their articles are interconnected via cross-language links, which not only facilitate navigation and understanding of concepts in multiple languages, but have been used in natural language processing applications, developments in linked open data, and expansion of minor Wikipedia language versions. These applications are the motivation for an automatic, robust, and accurate technique to identify cross-language links. In this paper, we present a multilingual approach called EurekaCL to automatically identify missing cross-language links in Wikipedia. More precisely, given a Wikipedia article (the source) EurekaCL uses the multilingual and semantic features of BabelNet 2.0 in order to efficiently identify a set of candidate articles in a target language that are likely to cover the same topic as the source. The Wikipedia graph structure is then exploited both to prune and to rank the candidates. Our evaluation carried out on 42,000 pairs of articles in eight language versions of Wikipedia shows that our candidate selection and pruning procedures allow an effective selection of candidates which significantly helps the determination of the correct article in the target language version. © Springer International Publishing Switzerland 2015.",Final,All Open Access; Green Open Access,
Elbedweihy K.; Wrigley S.N.; Ciravegna F.; Zhang Z.,Using BabelNet in bridging the gap between natural language queries and linked data concepts,1,,1,#WSD #use,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924389956&partnerID=40&md5=8100a50afb90f17b4fc3b428021f8274,"Many semantic search tool evaluations have reported a user preference for free natural language as a query input approach as opposed to controlled or view-based inputs. Although the exibility offered by this approach is a significant advantage, it can also be a major difficulty. Allowing users complete freedom in the choice of terms increases the difficulty for the search tools to match terms with the underlying data. This causes either a mismatch which affects precision, or a missing match which affects recall. In this paper, we present an empirical investigation on the use of named entity recognition, word sense disambiguation, and ontology-based heuristics in an approach attempting to bridge this gap between user terms and ontology concepts, properties and entities. We use the dataset provided by the Question Answering over Linked Data (QALD-2) workshop in our analysis and tests.",Final,,
Sànchez-Rada J.F.; Iglesias C.A.,Onyx: Describing emotions on the web of data,1,,1,#use #sentiment anlysis,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921891280&partnerID=40&md5=27c09a1447f4dd117e1549445a220b66,"Textual emotion analysis is a new field whose aim is to detect emotions in user generated content. It complements Sentiment Analysis in the characterization of users subjective opinions and feelings. Nevertheless, there is a lack of available lexical and semantic emotion resources that could foster the development of emotion analysis services. Some of the barriers for developing such resources are the diversity of emotion theories and the absence of a vocabulary to express emotion characteristics. This article presents a semantic vocabulary, called Onyx, intended to provide support to represent emotion characteristics in lexical resources and emotion analysis services. Onyx follows the Linked Data principles as it is aligned with the Provenance Ontology. It also takes a linguistic Linked Data approach: it is aligned with the Provenance Ontology, it represents lexical resources as linked data, and has been integrated with Lemon, an increasingly popular RDF model for representing lexical entries. Furthermore, it does not prescribe any emotion model and can be linked to heterogeneous emotion models expressed as Linked Data. Onyx representations can also be published using W3C EmotionML markup, based on the proposed mapping.",Final,,
Aggarwal N.; Polajnar T.; Buitelaar P.,Cross-lingual natural language querying over the web of data,1,,1,#use,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884972019&doi=10.1007%2f978-3-642-38824-8_13&partnerID=40&md5=d9a6a71b02be64505ee73179c1c85e0a,"The rapid growth of the Semantic Web offers a wealth of semantic knowledge in the form of Linked Data and ontologies, which can be considered as large knowledge graphs of marked up Web data. However, much of this knowledge is only available in English, affecting effective information access in the multilingual Web. A particular challenge arises from the vocabulary gap resulting from the difference in the query and the data languages. In this paper, we present an approach to perform cross-lingual natural language queries on Linked Data. Our method includes three components: entity identification, linguistic analysis, and semantic relatedness. We use Cross-Lingual Explicit Semantic Analysis to overcome the language gap between the queries and data. The experimental results are evaluated against 50 German natural language queries. We show that an approach using a cross-lingual similarity and relatedness measure outperforms other systems that use automatic translation. We also discuss the queries that can be handled by our approach. © 2013 Springer-Verlag Berlin Heidelberg.",Final,,
Augenstein I.; Padó S.; Rudolph S.,LODifier: Generating linked data from unstructured text,,,1,#use,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861720649&doi=10.1007%2f978-3-642-30284-8_21&partnerID=40&md5=97a20dcedfeacd87da0a9ac6928831df,"The automated extraction of information from text and its transformation into a formal description is an important goal in both Semantic Web research and computational linguistics. The extracted information can be used for a variety of tasks such as ontology generation, question answering and information retrieval. LODifier is an approach that combines deep semantic analysis with named entity recognition, word sense disambiguation and controlled Semantic Web vocabularies in order to extract named entities and relations between them from text and to convert them into an RDF representation which is linked to DBpedia and WordNet. We present the architecture of our tool and discuss design decisions made. An evaluation of the tool on a story link detection task gives clear evidence of its practical potential. © 2012 Springer-Verlag.",Final,All Open Access; Bronze Open Access,
Pinheiro V.; Furtado V.; Pequeno T.; Ferreira C.,Towards a common sense base in Portuguese for the linked open data cloud,1,,1,#use #kg,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858427776&doi=10.1007%2f978-3-642-28885-2_15&partnerID=40&md5=44c852eeccc94e8bf61c459775b10a50,"The Linked Open Data (LOD) cloud is a promising reality since the major content producers are offering their data on an open and linked network, through RDF (Resource Description Framework), with the aim of providing Semantic Web applications with a single global database for retrieval of related content and to perform inferences over the network. However, bases with Portuguese-language content are still incipient. In this paper we present the process of inclusion of the InferenceNet - the first resource with common sense and inferentialist knowledge in Portuguese language - on the LOD. Our main goal is to leverage the use and development of Semantic Web applications by content producers in Portuguese language. We develop and evaluated a platform, called SemWidgets, for the creation and execution of widgets able to access and reason over InferenceNet and the open linked data, like DBPedia, Yago, and Article Search API of the New York Times. © 2012 Springer-Verlag.",Final,,
Aggarwal N.; Buitelaar P.,A system description of natural language query over DBpedia,1,,1,#use #accessibility,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893280764&partnerID=40&md5=a5abfafedd1db93d78ad06f4e1057074,"This paper describes our system, which is developed as a first step towards implementing a methodology for natural language querying over semantic structured information (semantic web). This work focuses on interpretation of natural language queries (NL-Query) to facilitate querying over Linked Data. This interpretation includes query annotation with Linked Data concepts (classes and instances), a deep linguistic analysis and semantic similarity/relatedness to generate potential SPARQL queries for a given NL-Query. We evaluate our approach on QALD-2 test dataset and achieve a F1 score of 0.46, an average precision of 0.44 and an average recall of 0.48.",Final,,
Hayashi Y.,Direct and indirect linking of lexical objects for evolving lexical linked data,1,,1,#use #task #synset,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891124416&partnerID=40&md5=4dfb614a158c5969b4e336e79321ede8,"Servicization of language resources in a Web-based environment has opened up the potential for dynamically combined virtual lexical resources. Evolving lexical linked data could be realized, provided being recovered/discovered links among lexical resources are properly organized and maintained. This position paper examines a scenario, in which lexical semantic resources are cross-linguistically enriched, and sketches how this scenario could come about while discussing necessary ingredients. The discussions naturally include how the existing lexicon modeling framework could be applied and should be extended.",Final,,
Wang Z.; Li L.; Zeng D.,Knowledge-Enhanced Natural Language Inference Based on Knowledge Graphs,1,,1,#use #naturalLanguageInference,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118819345&partnerID=40&md5=3cb022a173acf84d242a962c1bc4b235,"Natural Language Inference (NLI) is a vital task in natural language processing. It aims to identify the logical relationship between two sentences. Most of the existing approaches make such inference based on semantic knowledge obtained through training corpus. The adoption of background knowledge is rarely seen or limited to a few specific types. In this paper, we propose a novel Knowledge Graph-enhanced NLI (KGNLI) model to leverage the usage of background knowledge stored in knowledge graphs in the field of NLI. KGNLI model consists of three components: a semantic-relation representation module, a knowledge-relation representation module, and a label prediction module. Different from previous methods, various kinds of background knowledge can be flexibly combined in the proposed KGNLI model. Experiments on four benchmarks, SNLI, MultiNLI, SciTail, and BNLI, validate the effectiveness of our model. © 2020 COLING 2020 - 28th International Conference on Computational Linguistics, Proceedings of the Conference. All rights reserved.",Final,,
Casadei S.,Semiotic Knowledge Models for Personal Knowledge Repositories,0/1,,1,#use,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179546774&doi=10.5220%2f0012209100003598&partnerID=40&md5=233ec7719a4b45185dcb5753aed35c4b,"Knowledge graphs have been used successfully to represent and acquire general knowledge and have also been proposed for personal knowledge representations. While general knowledge data can be modelled statistically as being a noisy projection of universal (and crisp) entities, categories, and relationships, personal knowledge data requires a more refined model: each user’s peculiarities and fluctuations in associating words with meanings and meanings with words should be tracked and analysed instead of being treated as noise and averaged out. This position paper describes a semiotic knowledge model whose primitives are the signification events which occur when symbols such as words and linguistic expressions are associated with an instantaneous meaning. Semiotic structures constructed from these primitives with users’ active participation, enable them to create, update, modify, organize, re-organize and curate detailed and comprehensive representations of their own personal knowledge by means of their own personal terminologies, taxonomies, and organizational schemes. Copyright © 2023 by SCITEPRESS – Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0).",Final,All Open Access; Hybrid Gold Open Access,
Draicchio F.; Gangemi A.; Presutti V.; Nuzzolese A.G.,FRED: From natural language text to RDF and OWL in one click,1,1,1,#use,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893803453&doi=10.1007%2f978-3-642-41242-4_36&partnerID=40&md5=3e70d41fd698f9811e5b8d19df26b6a8,"FRED is an online tool for converting text into internally well-connected and quality linked-data-ready ontologies in web-service-acceptable time. It implements a novel approach for ontology design from natural language sentences. In this paper we present a demonstration of such tool combining Discourse Representation Theory (DRT), linguistic frame semantics, and Ontology Design Patterns (ODP). The tool is based on Boxer which implements a DRT-compliant deep parser. The logical output of Boxer enriched with semantic data from Verbnet or FrameNet frames is transformed into RDF/OWL by means of a mapping model and a set of heuristics following ODP best-practice [5] of OWL ontologies and RDF data design. © Springer-Verlag 2013.",Final,All Open Access; Bronze Open Access,
Lesnikova T.; David J.; Euzenat J.,Interlinking English and Chinese RDF data using BabelNet,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959182734&doi=10.1145%2f2682571.2797089&partnerID=40&md5=14dd737f9c6e954599d219a883a53ab9,"Linked data technologies make it possible to publish and link structured data on the Web. Although RDF is not about text, many RDF data providers publish their data in their own language. Cross-lingual interlinking aims at discovering links between identical resources across knowledge bases in different languages. In this paper, we present a method for interlinking RDF resources described in English and Chinese using the BabelNet multilingual lexicon. Resources are represented as vectors of identifiers and then similarity between these resources is computed. The method achieves an F-measure of 88%. The results are also compared to a translation-based method. © 2015 ACM.",Final,All Open Access; Green Open Access,
Ciroku F.; De Giorgis S.; Gangemi A.; Martinez-Pandiani D.S.; Presutti V.,Automated multimodal sensemaking: Ontology-based integration of linguistic frames and visual data,1,1,1,#use,2024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174832331&doi=10.1016%2fj.chb.2023.107997&partnerID=40&md5=aa66d8255e2636105d6d877f8ee1fcf1,"Frame evocation from visual data is an essential process for multimodal sensemaking, due to the multimodal abstraction provided by frame semantics. However, there is a scarcity of data-driven approaches and tools to automate it. We propose a novel approach for explainable automated multimodal sensemaking by linking linguistic frames to their physical visual occurrences, using ontology-based knowledge engineering techniques. We pair the evocation of linguistic frames from text to visual data as “framal visual manifestations”. We present a deep ontological analysis of the implicit data model of the Visual Genome image dataset, and its formalization in the novel Visual Sense Ontology (VSO). To enhance the multimodal data from this dataset, we introduce a framal knowledge expansion pipeline that extracts and connects linguistic frames – including values and emotions – to images, using multiple linguistic resources for disambiguation. It then introduces the Visual Sense Knowledge Graph (VSKG), a novel resource. VSKG is a queryable knowledge graph that enhances the accessibility and comprehensibility of Visual Genome's multimodal data, based on SPARQL queries. VSKG includes frame visual evocation data, enabling more advanced forms of explicit reasoning; analysis and sensemaking. Our work represents a significant advancement in the automation of frame evocation and multimodal sense-making, performed in a fully interpretable and transparent way, with potential applications in various fields, including the fields of knowledge representation, computer vision, and natural language processing. © 2023",Final,,
Chiarcos C.; Ionov M.,Linking discourse marker inventories,1,,1,#use,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115090315&doi=10.4230%2fOASIcs.LDK.2021.40&partnerID=40&md5=8ce3d81f124faee294626d7eb577c118,"The paper describes the first comprehensive edition of machine-readable discourse marker lexicons. Discourse markers such as and, because, but, though or thereafter are essential communicative signals in human conversation, as they indicate how an utterance relates to its communicative context. As much of this information is implicit or expressed differently in different languages, discourse parsing, context-adequate natural language generation and machine translation are considered particularly challenging aspects of Natural Language Processing. Providing this data in machine-readable, standard-compliant form will thus facilitate such technical tasks, and moreover, allow to explore techniques for translation inference to be applied to this particular group of lexical resources that was previously largely neglected in the context of Linguistic Linked (Open) Data. © Christian Chiarcos and Maxim Ionov; licensed under Creative Commons License CC-BY 4.0",Final,,
Postiglione M.,Towards an Italian Healthcare Knowledge Graph,1,1,1,#use,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118970715&doi=10.1007%2f978-3-030-89657-7_29&partnerID=40&md5=943210e16e59138126728bf8aa3fd786,"Electronic Health Records (EHRs), Big Data, Knowledge Graphs (KGs) and machine learning can potentially be a great step towards the technological shift from the one-size-fit-all medicine, where treatments are based on an equal protocol for all the patients, to the precision medicine, which takes count of all their individual information: lifestyle, preferences, health history, genomics, and so on. However, the lack of data which characterizes low-resource languages is a huge limitation for the application of the above-mentioned technologies. In this work, we will try to fill this gap by means of transformer language models and few-shot approaches and we will apply similarity-based deep learning techniques on the constructed KG for downstream applications. The proposed architecture is general and thus applicable to any low-resource language. © 2021, Springer Nature Switzerland AG.",Final,,
Gabryszak A.; Krause S.; Hennig L.; Xu F.; Uszkoreit H.,Relation- and phrase-level linking of FrameNet with sar-graphs,1,,1,#use,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037086784&partnerID=40&md5=0b41c874f72fe4702c69393a28f34610,"Recent research shows the importance of linking linguistic knowledge resources for the creation of large-scale linguistic data. We describe our approach for combining two English resources, FrameNet and sar-graphs, and illustrate the benefits of the linked data in a relation extraction setting. While FrameNet consists of schematic representations of situations, linked to lexemes and their valency patterns, sar-graphs are knowledge resources that connect semantic relations from factual knowledge graphs to the linguistic phrases used to express instances of these relations. We analyze the conceptual similarities and differences of both resources and propose to link sar-graphs and FrameNet on the levels of relations/frames as well as phrases. The former alignment involves a manual ontology mapping step, which allows us to extend sar-graphs with new phrase patterns from FrameNet. The phrase-level linking, on the other hand, is fully automatic. We investigate the quality of the automatically constructed links and identify two main classes of errors.",Final,,
Hosseini H.; Mansouri M.; Bagheri E.,A systemic functional linguistics approach to implicit entity recognition in tweets,-1,0/1,0/1,#use #ner,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130179375&doi=10.1016%2fj.ipm.2022.102957&partnerID=40&md5=5c39a8d58f6495316fd81056929b8b92,"The identification of knowledge graph entity mentions in textual content has already attracted much attention. The major assumption of existing work is that entities are explicitly mentioned in text and would only need to be disambiguated and linked. However, this assumption does not necessarily hold for social content where a significant portion of information is implied. The focus of our work in this paper is to identify whether textual social content include implicit mentions of knowledge graph entities or not, hence forming a two-class classification problem. To this end, we adopt the systemic functional linguistic framework that allows for capturing meaning expressed through language. Based on this theoretical framework we systematically introduce two classes of features, namely syntagmatic and paradigmatic features, for implicit entity recognition. In our experiments, we show the utility of these features for the task, report on ablation studies, measure the impact of each feature subset on each other and also provide a detailed error analysis of our technique. © 2022 Elsevier Ltd",Final,,
Wu S.; Li Y.; Zhang D.; Zhou Y.; Wu Z.,Diverse and informative dialogue generation with context-specific commonsense knowledge awareness,-1,0/1,0/1,#use #commonsense #chinese #textgeneration,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098764003&partnerID=40&md5=06cd1ae45bf6e9d828a4ddc212e64d99,"Generative dialogue systems tend to produce generic responses, which often leads to boring conversations. For alleviating this issue, Recent studies proposed to retrieve and introduce knowledge facts from knowledge graphs. While this paradigm works to a certain extent, it usually retrieves knowledge facts only based on the entity word itself, without considering the specific dialogue context. Thus, the introduction of the context-irrelevant knowledge facts can impact the quality of generations. To this end, this paper proposes a novel commonsense knowledge-aware dialogue generation model, ConKADI. We design a Felicitous Fact mechanism to help the model focus on the knowledge facts that are highly relevant to the context; furthermore, two techniques, Context-Knowledge Fusion and Flexible Mode Fusion are proposed to facilitate the integration of the knowledge in the ConKADI. We collect and build a large-scale Chinese dataset aligned with the commonsense knowledge for dialogue generation. Extensive evaluations over both an open-released English dataset and our Chinese dataset demonstrate that our approach ConKADI outperforms the state-of-the-art approach CCM, in most experiments. © 2020 Association for Computational Linguistics",Final,,
Iglesias C.Á.; Sánchez-Rada J.F.,Sentiment Analysis meets Linguistic Linked Data: An overview of the State of the Art,1,,1,#use,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126004020&partnerID=40&md5=17ad64d61f470f48ea762b6d6ccc710b,"Sentiment Analysis has received plenty of attention from both industry and academia because its application can reveal new insights from social interactions. The wide range of final users of these services includes public services, businesses, and individuals. Linked data technologies provide an effective and seamless way for integrating services and interlinking language resources. This paper provides an introduction to the main approaches, applications, and datasets. © 2021 Copyright for this paper by its authors",Final,,
Calixto I.; Raganato A.; Pasini T.,Wikipedia Entities as Rendezvous across Languages: Grounding Multilingual Language Models by Predicting Wikipedia Hyperlinks,1,,1,#use,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128159332&partnerID=40&md5=ceeed2bdf67e50e491da5199f5ac7045,"Masked language models have quickly become the de facto standard when processing text. Recently, several approaches have been proposed to further enrich word representations with external knowledge sources such as knowledge graphs. However, these models are devised and evaluated in a monolingual setting only. In this work, we propose a language-independent entity prediction task as an intermediate training procedure to ground word representations on entity semantics and bridge the gap across different languages by means of a shared vocabulary of entities. We show that our approach effectively injects new lexical-semantic knowledge into neural models, improving their performance on different semantic tasks in the zero-shot crosslingual setting. As an additional advantage, our intermediate training does not require any supplementary input, allowing our models to be applied to new datasets right away. In our experiments, we use Wikipedia articles in up to 100 languages and already observe consistent gains compared to strong baselines when predicting entities using only the English Wikipedia. Further adding extra languages lead to improvements in most tasks up to a certain point, but overall we found it non-trivial to scale improvements in model transferability by training on ever increasing amounts of Wikipedia languages. © 2021 Association for Computational Linguistics.",Final,,
Sakor A.; Singh K.; Patel A.; Vidal M.-E.,Falcon 2.0: An Entity and Relation Linking Tool over Wikidata,1,,1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095866274&doi=10.1145%2f3340531.3412777&partnerID=40&md5=89836c6a6ea4d848ca13698761f2d222,"The Natural Language Processing (NLP) community has significantly contributed to the solutions for entity and relation recognition from a natural language text, and possibly linking them to proper matches in Knowledge Graphs (KGs). Considering Wikidata as the background KG, there are still limited tools to link knowledge within the text to Wikidata. In this paper, we present Falcon 2.0, the first joint entity and relation linking tool over Wikidata. It receives a short natural language text in the English language and outputs a ranked list of entities and relations annotated with the proper candidates in Wikidata. The candidates are represented by their Internationalized Resource Identifier (IRI) in Wikidata. Falcon 2.0 resorts to the English language model for the recognition task (e.g., N-Gram tiling and N-Gram splitting), and then an optimization approach for the linking task. We have empirically studied the performance of Falcon 2.0 on Wikidata and concluded that it outperforms all the existing baselines. Falcon 2.0 is open source and can be reused by the community; all the required instructions of Falcon 2.0 are well-documented at our GitHub repository (https://github.com/SDM-TIB/falcon2.0). We also demonstrate an online API, which can be run without any technical expertise. Falcon 2.0 and its background knowledge bases are available as resources at https://labs.tib.eu/falcon/falcon2/. © 2020 Owner/Author.",Final,All Open Access; Bronze Open Access; Green Open Access,
Ivanov V.; Solnyshkina M.,A method for assessment of text complexity based on knowledge graphs,1,,1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105003287&partnerID=40&md5=c02abc5d81e98943e3c6b7caaaca1928,"The study explores the problem of assessing text complexity. In this paper we focus on measuring conceptual complexity and propose using knowledge graphs to this end. On the first stage of the research, RuThes-Lite thesaurus, a linguistic knowledge base with a total size of over 100,000 text entries (words and collocations), was used to elicit concepts in the texts of schoolbooks and represent text fragments as graphs. In the second series of experiments, we assessed complexity of English texts using knowledge graphs WordNet and Wikidata. Finally, we identified graph-based semantic characteristics of texts impacting complexity. The most significant research findings include identification of statistically significant correlations of the selected features, such as node degree, number of connected nodes, average shortest path, with text complexity. © 2020 Copyright for this paper by its authors.",Final,,
Zhao C.; Walker M.; Chaturvedi S.,Bridging the structural gap between encoding and decoding for data-to-text generation,1,,1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095506853&partnerID=40&md5=3fd340163697efc401cb15375abcf4fd,"Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose DUALENC, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text. © 2020 Association for Computational Linguistics",Final,,
Moussallem D.; Ngomo A.-C.N.; Buitelaar P.; Arcan M.,Utilizing knowledge graphs for neural machine translation augmentation,1,,1,#use,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077242522&doi=10.1145%2f3360901.3364423&partnerID=40&md5=00ed4866d01ddb7772e12ec8e6bf6b07,"While neural networks have led to substantial progress in machine translation, their success depends heavily on large amounts of training data. However, parallel training corpora are not always readily available. Moreover, out-of-vocabulary words - -mostly entities and terminological expressions - -pose a difficult challenge to Neural Machine Translation systems. Recent efforts have tried to alleviate the data sparsity problem by augmenting the training data using different strategies, such as external knowledge injection. In this paper, we hypothesize that knowledge graphs enhance the semantic feature extraction of neural models, thus optimizing the translation of entities and terminological expressions in texts and consequently leading to better translation quality. We investigate two different strategies for incorporating knowledge graphs into neural models without modifying the neural network architectures. Additionally, we examine the effectiveness of our augmented models on domain-specific texts and ontologies. Our knowledge-graph-augmented neural translation model, dubbed KG-NMT, achieves significant and consistent improvements of +3 BLEU, METEOR and chrF3 on average on the newstest datasets between 2015 and 2018 for the WMT English-German translation task. © 2019 ACM.",Final,,
Marginean A.,Question answering over biomedical linked data with Grammatical Framework,1,,1,#use #accessibility,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010934356&doi=10.3233%2fSW-160223&partnerID=40&md5=801d07da07668112d1560beb59e2cbea,"The blending of linked data with ontologies leverages the access to data. GFMed introduces grammars for a controlled natural language targeted towards biomedical linked data and the corresponding controlled SPARQL language. The grammars are described in Grammatical Framework and introduce linguistic and SPARQL phrases mostly about drugs, diseases and relationships between them. The semantic and linguistic chunks correspond to Description Logic constructors. Problems and solutions for querying biomedical linked data with Romanian, besides English, are also considered in the context of GF. © 2017-IOS.",Final,,
Ji K.; Wang S.; Carlson L.,Multilingual dictionary linking and aggregation: Quality from consistency,1,,1,#use,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992573934&partnerID=40&md5=51d48c2d56423bce5da6b05b8084ca46,"The growth of Web-accessible dictionaries and term data has led to a proliferation of platforms distributing the same lexical resources in different combinations and packagings. Finding the right word or translation is like finding a needle in a haystack. The quantity of the data is undercut by the doubtful quality of the resources. Our aim is to cut down the quantity and raise the quality by matching and aggregating entries within and across dictionaries. In this exploratory paper, our goal is to see how far we can get by using information extracted from multiple dictionaries themselves. Our hypothesis is that the more limited quantity of data in dictionaries is compensated by their richer structure and more concentrated information content. We hope to take advantage of the structure of dictionaries by basing quality criteria and measures on linguistic and terminological considerations. The plan of campaign is to derive quality criteria to recognise well-constructed dictionary entries from a model dictionary, and then attempt to convert the criteria into language-independent frequency-based measures. As a model dictionary we use the Princeton WordNet. The measures derived from it are tested against data extracted from BabelNet. © 2016, CEUR-WS. All rights reserved.",Final,,