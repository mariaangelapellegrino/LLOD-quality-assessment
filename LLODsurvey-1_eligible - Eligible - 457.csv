Authors,Title,Reviewer1,Reviewer2,Overall,Motivation,Year,Link,Abstract,Publication Stage,Open Access,Manually added
Chen L.-P.; Shih Y.-L.; Chen C.-T.; Ku T.; Hsieh W.-T.; Chiu H.-S.; Yang R.-D.,English-to-traditional Chinese cross-lingual link discovery in articles with wikipedia corpus,-1,,-1,#excluded #outofscope,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883015565&partnerID=40&md5=6e2872c73a5d9b1703f494652aa89227,"In this paper, we design a processing flow to produce linked data in articles, providing anchor-based term's additional information and related terms in different languages (English to Chinese). Wikipedia has been a very important corpus and knowledge bank. Although Wikipedia describes itself not a dictionary or encyclopedia, it is if high potential values in applications and data mining researches. Link discovery is a useful IR application, based on Data Mining and NLP algorithms and has been used in several fields. According to the results of our experiment, this method does make the result has improved.",Final,,
Sorrentino S.; Bergamaschi S.; Fusari E.,A semantic multi-lingual method for publishing linked open data,-1,,-1,#excluded #notavailable,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903553546&partnerID=40&md5=abbae146690bd2b9d5468aadddfcaf98,"Nowadays, there has been an increment of open data initiatives promoting the freely publication of data produced by public administrations (such as public spending, health care, education etc.). However, the great majority of these data are published in an unstructured format (such as spreadsheets or CSV) and is typically accessed only by closed communities. To address this problem, we propose a semiautomatic multi-lingual and semantic method for facilitating resource providers in publishing public data into the Linked Open Data (LOD) cloud, and for helping consumers (companies and citizens) in efficiently accessing and querying them. The method has been applied on a real case on a set of data provided in Italian.",Final,,
Buchmann R.A.; Karagiannis D.,Modelling mobile app requirements for semantic traceability,-1,,-1,#excluded #outofscope,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937826800&doi=10.1007%2fs00766-015-0235-1&partnerID=40&md5=9cde5f230420f803d22d454e1dd911e8,"The paper presents a modelling method aimed to support the definition and elicitation of requirements for mobile apps through an approach that enables semantic traceability for the requirements representation. Business process-centricity is employed in order to capture requirements in a knowledge structure that retains procedural knowledge from stakeholders and can be traversed by semantic queries in order to trace domain-specific contextual information for the modelled requirements. Consequently, instead of having requirements represented as natural language items that are documented by diagrammatic models, the communication channels are switched: semantically interlinked conceptual models become the requirements representation, while free text can be used for requirements annotations/metadata. Thus, the method establishes a knowledge externalization channel between business stakeholders and app developers, also tackling the Twin Peaks bridging challenge (between requirements and early designs). The method is presented using its modelling procedure as a guiding thread, with each step illustrated by case-based samples of the modelling language and auxiliary functionality. The design work is encompassed by an existing metamodelling framework and introduces a taxonomy for modelling relations, since the metamodel is the key enabler for the goal of semantic traceability. The research was driven by the ComVantage EU research project, concerned with mobile app support for collaborative business process execution. Therefore, the project provides context for the illustrating examples; however, generalization possibilities beyond the project scope will also be discussed, with respect to both motivation and outcome. © 2015, Springer-Verlag London.",Final,,
Markowitz E.; Balasubramanian K.; Mirtaheri M.; Annavaram M.; Galstyan A.; Steeg G.V.,StATIK: Structure and Text for Inductive Knowledge Graph Completion,-1,,-1,#excluded #outofscope #kgcompletion not linguistic features,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137336754&partnerID=40&md5=dcd256325a283a1ebc2179470216ec6a,"Knowledge graphs (KGs) often represent knowledge bases that are incomplete. Machine learning models can alleviate this by helping automate graph completion. Recently, there has been growing interest in completing knowledge bases that are dynamic, where previously unseen entities may be added to the KG with many missing links. In this paper, we present StATIK-Structure And Text for Inductive Knowledge Completion. StATIK uses Language Models to extract the semantic information from text descriptions, while using Message Passing Neural Networks to capture the structural information. StATIK achieves state of the art results on three challenging inductive baselines. We further analyze our hybrid model through detailed ablation studies. © Findings of the Association for Computational Linguistics: NAACL 2022 - Findings.",Final,,
Zhao Y.; Cai X.; Wu Y.; Zhang H.; Zhang Y.; Zhao G.; Jiang N.,MoSE: Modality Split and Ensemble for Multimodal Knowledge Graph Completion,-1,,-1,#excluded #outofscope #kgcompletion not linguistic features,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149441752&partnerID=40&md5=d8b426357d0ebabc4b4a88ab9dbda59d,"Multimodal knowledge graph completion (MKGC) aims to predict missing entities in MKGs. Previous works usually share relation representation across modalities. This results in mutual interference between modalities during training, since for a pair of entities, the relation from one modality probably contradicts that from another modality. Furthermore, making a unified prediction based on the shared relation representation treats the input in different modalities equally, while their importance to the MKGC task should be different. In this paper, we propose MoSE, a Modality Split representation learning and Ensemble inference framework for MKGC. Specifically, in the training phase, we learn modality-split relation embeddings for each modality instead of a single modality-shared one, which alleviates the modality interference. Based on these embeddings, in the inference phase, we first make modality-split predictions and then exploit various ensemble methods to combine the predictions with different weights, which models the modality importance dynamically. Experimental results on three KG datasets show that MoSE outperforms state-of-the-art MKGC methods. Codes are available at https://github.com/OreOZhao/MoSE4MKGC. © 2022 Association for Computational Linguistics.",Final,,
Ge T.; Wang Y.; De Melo G.; Li H.; Chen B.,Visualizing and curating knowledge graphs over time and space,-1,,-1,#excluded not linguistic features,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016486947&doi=10.18653%2fv1%2fp16-4005&partnerID=40&md5=3dd121ce08e7cc4d531c210d79fe08b2,"Publicly available knowledge repositories, such as Wikipedia and Freebase, benefit significantly from volunteers, whose contributions ensure that the knowledge keeps expanding and is kept up-to-date and accurate. User interactions are often limited to hypertext, tabular, or graph visualization interfaces. For spatio-temporal information, however, other interaction paradigms may be better-suited. We present an integrated system that combines crowdsourcing, automatic or semi-automatic knowledge harvesting from text, and visual analytics. It enables users to analyze large quantities of structured data and unstructured textual data from a spatio-temporal perspective and gain deep insights that are not easily observed in individual facts. © 2016 Association for Computational Linguistics.",Final,All Open Access; Hybrid Gold Open Access,
Vaisman A.,Publishing OLAP cubes on the semantic web,-1,,-1,#notavailable #excluded,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978761828&doi=10.1007%2f978-3-319-39243-1_2&partnerID=40&md5=42e4e5d9ac8a2fdf6954962636b65709,"The availability of large repositories of semantically annotated data on the web is opening new opportunities for enhancing Decision-Support Systems. In addition, the advent of initiatives such as Open Data and Open Government, together with the Linked Open Data paradigm, are promoting publication and sharing of multidimensional data (MD) on the web. In this paper we address the problem of representing MD data using Semantic Web (SW) standards. We discuss how MD data can be represented and queried directly over the SW, without the need to download data sets into local data warehouses. We first comment on the RDF Data Cube Vocabulary (QB), the current W3C recommendation, and show that it is not enough to appropriately represent and query MD data on the web. In order to be able to support useful Online Analytical Process (OLAP) analysis, extension to QB, denoted QB4OLAP, has been proposed. We provide an in-depth comparison between these two proposals, and show that extending QB with QB4OLAP can be done without re-writing the observations, (the largest part of a QB data set). We provide extensive examples of the QB4OLAP representation, using a portion of the Eurostat data set and the wellknown Northwind database. Finally, we present a high-level query language, called QL, that allows OLAP users not familiar with SW concepts or languages, to write and execute OLAP operators without any knowledge of RDF or SPARQL, the standard data model and query language, respectively, for the SW. QL queries are automatically translated into SPARQL (using the QB4OLAP metadata) and executed over an endpoint. © Springer International Publishing Switzerland 2016.",Final,,
Lee J.; Chung C.; Lee H.; Jo S.; Whang J.J.,VISTA: Visual-Textual Knowledge Graph Representation Learning,-1,,-1,#excluded #outofscope,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183298983&partnerID=40&md5=dba38781f8c9640990b6a0cbaf387666,"Knowledge graphs represent human knowledge using triplets composed of entities and relations. While most existing knowledge graph embedding methods only consider the structure of a knowledge graph, a few recently proposed multimodal methods utilize images or text descriptions of entities in a knowledge graph. In this paper, we propose visual-textual knowledge graphs (VTKGs), where not only entities but also triplets can be explained using images, and both entities and relations can accompany text descriptions. By compiling visually expressible commonsense knowledge, we construct new benchmark datasets where triplets themselves are explained by images, and the meanings of entities and relations are described using text. We propose VISTA, a knowledge graph representation learning method for VTKGs, which incorporates the visual and textual representations of entities and relations using entity encoding, relation encoding, and triplet decoding transformers. Experiments show that VISTA outperforms state-of-the-art knowledge graph completion methods in real-world VTKGs. © 2023 Association for Computational Linguistics.",Final,,
Mihindukulasooriya N.; Dash S.; Bagchi S.; Chowdhury F.; Gliozzo A.; Farkash A.; Glass M.; Gokhman I.; Hassanzadeh O.; Pham N.; Rossiello G.; Rozenberg B.; Sagron Y.; Subramanian D.; Takahashi T.; Tateishi T.; Vu L.,Unleashing the Potential of Data Lakes with Semantic Enrichment Using Foundation Models,-1,,-1,#excluded #outofscope,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184379489&partnerID=40&md5=3575ea110a78ffde7449db418c24400b,"Nowadays most organizations are managing data lakes containing heterogeneous data from various sources. However, the lack of adequate metadata often transforms these data lakes into data swamps, making it challenging to locate relevant data for critical organizational tasks and consequently limiting their utility. Recent advancements in large language models and foundation models have enabled the automation of metadata generation using generative AI models and the use of generated metadata for mapping tabular data into semantically richer glossaries, taxonomies, or ontologies. In this talk, we will present a semantic enrichment process that generates table metadata such as descriptive table captions, tags, expanded column names, and column descriptions and then uses that information to map table columns to concepts in a given business glossary or an ontology. Furthermore, during this process, we represent both table metadata and business glossaries as knowledge graphs and connect them by mapping columns to business concepts. As a result, the enrichment process makes the data in data lakes more meaningful to the organization and enhances downstream tasks, including improved table search and discovery, efficient table joins, and advanced business analytics. © 2023 Copyright for this paper by its authors.",Final,,
Su Y.; Han X.; Zhang Z.; Lin Y.; Li P.; Liu Z.; Zhou J.; Sun M.,CokeBERT: Contextual knowledge selection and embedding towards enhanced pre-trained language models,-1,,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115923209&doi=10.1016%2fj.aiopen.2021.06.004&partnerID=40&md5=ed03d32aeb98fe97751bf6a6ad2a6cf0,"Several recent efforts have been devoted to enhancing pre-trained language models (PLMs) by utilizing extra heterogeneous knowledge in knowledge graphs (KGs), and achieved consistent improvements on various knowledge-driven NLP tasks. However, most of these knowledge-enhanced PLMs embed static sub-graphs of KGs (“knowledge context”), regardless of that the knowledge required by PLMs may change dynamically according to specific text (“textual context”). In this paper, we propose a novel framework named Coke to dynamically select contextual knowledge and embed knowledge context according to textual context for PLMs, which can avoid the effect of redundant and ambiguous knowledge in KGs that cannot match the input text. Our experimental results show that Coke outperforms various baselines on typical knowledge-driven NLP tasks, indicating the effectiveness of utilizing dynamic knowledge context for language understanding. Besides the performance improvements, the dynamically selected knowledge in Coke can describe the semantics of text-related knowledge in a more interpretable form than the conventional PLMs. Our implementation and datasets are publicly available. © 2021 The Authors",Final,All Open Access; Gold Open Access; Green Open Access,
Oramas S.; Sordo M.; Espinosa-Anke L.,A rule-based approach to extracting relations from music tidbits,-1,,-1,#excluded,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968563052&doi=10.1145%2f2740908.2741709&partnerID=40&md5=b04597175eefe1cff3dc5ba56630c779,"This paper presents a rule based approach to extracting re- lations from unstructured music text sources. The proposed approach identifies and disambiguates musical entities in text, such as songs, bands, persons, albums and music gen- res. Candidate relations are then obtained by traversing the dependency parsing tree of each sentence in the text with at least two identified entities. A set of syntactic rules based on part of speech tags are defined to filter out spurious and irrel- evant relations. The extracted entities and relations are - nally represented as a knowledge graph. We test our method on texts from songfacts.com, a website that provides tidbits with facts and stories about songs. The extracted relations are evaluated intrinsically by assessing their linguistic qual- ity, as well as extrinsically by assessing the extent to which they map an existing music knowledge base. Our system produces a vast percentage of linguistically correct relations between entities, and is able to replicate a significant part of the knowledge base.",Final,All Open Access; Green Open Access,
Both A.; Avdiyenko L.; Lemke C.,Computing geo-spatial motives from linked data for search-driven applications,-1,,-1,#excluded #outofscope no linguistic features,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930741546&partnerID=40&md5=e4aa5bd0e9147f3d5414941f8b97af45,"The Web of Data puts a vast and ever-increasing amount of information at the disposal of its users. In the era of big data, interpreting and exploiting these information is both a highly active research area and a key issue for users in industry trying to gain a competitive edge. One current problem in industry with many potential application areas is finding a common theme for varying features by generating higher level summaries. We introduce the notion of motives to describe these common themes. Motives can be identified for all sorts of entities such as geo-spatial regions (e.g., ""cultural regions"") or holidays (e.g., ""win- ter holidays"", ""activity holidays""). These motives are closer to common language and human conversations than ordinary keywords. Since users prefer formulating their information needs using everyday language, which expresses their understanding of the world, the poten- tial for a strong industrial impact for search applications can be de- rived. However, capturing the users' often vaguely formulated intentions and matching them to appropriate retrieval operations on the available knowledge bases is a challenging issue. Yet, it is an important step on the way of providing the best possible search experience to users. This paper presents our work in progress on computing motives for geo- spatial regions. Following a long term agenda, we are evaluating the requirements for identifying such motives in large data sets. At this point, we can show that out-of-the-box machine learning methods can be used on Linked Data to train a model for computation of geo-spatial motives with good accuracy. Copyright © 2015 for the individual papers by the papers' authors.",Final,,
Meehan A.; Brennan R.; Lewis D.; O'Sullivan D.,Mapping representation based on meta-data and spin for localization workflows,-1,,-1,#excluded #outofscope no linguistic features,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924955908&partnerID=40&md5=9a58986dd708238421d6fecbd9d2576e,"The localization industry currently deploys language translation workflows based on heterogeneous tool-chains. Standardized tool interchange formats such as XLIFF (XML Localization Interchange File Format) have had some impact on enabling more agile translation workflows. However the rise of new tools based on machine translation technology and the growing demand for enterprise linked data applications has created new interoperability challenges as workflows need to encompass a broader range of tools. In this paper we present an approach of representing mappings between RDF-based representations of multilingual content and meta-data. To represent the mappings, we use a combination of SPARQL Inferencing Notation (SPIN) and meta-data. Our approach allows the mapping representation to be published as Linked Data. In contrast to other frameworks such as R2R, the mappings are executed via a standard SPARQL processor. The objective is to provide a more agile approach to translation workflows and greater interoperability between software tools by leveraging the ongoing innovation in the Multilingual Web field. Our use case is a Language Technology retraining workflow where publishing mappings leads to new opportunities for interoperability and end-to-end tool-chain analytics. We present the results from an initial experiment which compared our approach of executing and representing mappings to that of a similar approach - The R2R Framework.",Final,,
Liu X.; Chen M.; Qin J.,Interlinking cross language metadata using heterogeneous graphs and wikipedia,-1,,-1,#excluded not linguistic features,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013648969&partnerID=40&md5=adb01bcdabcb11bcbb9c5891ee9c07d2,"Cross-language metadata are essential in helping users overcome language barriers in information discovery and recommendation. The construction of cross-language vocabulary, however, is usually costly and intellectually laborious. This paper addresses these problems by proposing a Cross-Language Metadata Network (CLMN) approach, which uses Wikipedia as the intermediary for cross-language metadata linking. We conducted a proof-of-concept experiment with key metadata in two digital libraries and in two different languages without using machine translation. The experiment result is encouraging and suggests that the CLMN approach has the potential not only to interlink metadata in different languages with reasonable rate of precision and quality but also to construct cross-language metadata vocabulary. Limitations and further research are also discussed. © 2014, Dublin Core metadata initiative. All rights reserved.",Final,,
Biswas D.; Linzbach S.; Dimitrov D.; Jabeen H.; Dietze S.,Broadening BERT vocabulary for Knowledge Graph Construction using Wikipedia2Vec,-1,,-1,#excluded #outofscope,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179557571&partnerID=40&md5=1609abf8ea196ad7f449e16ff3bd92f7,"Recent advancements in natural language processing (NLP) have been driven by the utilization of large language models like BERT. These models, pre-trained on extensive textual data, capture linguistic and relational knowledge. Therefore, cloze-style prompts, which involve filling in missing words in a sentence, can be used to solve knowledge-intensive NLP tasks with the help of a language model. The ""Knowledge Base Construction from Pre-trained Language Models (LM-KBC 2023)"" challenge aims to harness language models’ potential for knowledge graph construction through prompts. In particular, contestants are challenged to infer the correct Wikidata ID of objects, given a prompt used to link subject, relation, and object. Automatically inferring the correct objects would help in reducing the need for an expensive manual graph population. Our proposed approach in Track 1 focuses on expanding BERT’s vocabulary with a task-specific one (i.e., Wikipedia2Vec) and facilitating its usage through prompt tuning with OPTIPROMPT. © 2023 CEUR-WS. All rights reserved.",Final,,
Gupta D.; Berberich K.,Weaving Text into Tables,-1,-1,-1,#excluded #outofscope no semantic web technologies,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095863899&doi=10.1145%2f3340531.3417442&partnerID=40&md5=fbf6df476c38a720c7cc8d20b25a044a,"In this paper, we showcase JIGSAW, a system that is able to shape unstructured text into structured tables for user-defined schemas. In short, to structure text into tables, JIGSAW leverages the lexico-syntactic structure imposed by linguistic annotations (e.g., part-of-speech, named entities, temporal and numerical expressions) on natural language text. We describe how challenging knowledge-centric tasks such as question answering, summarization, and analytics can be greatly simplified with the help of JIGSAW. © 2020 Owner/Author.",Final,All Open Access; Bronze Open Access,
Ruseti S.; Mirea A.; Rebedea T.; Trausan-Matu S.,QAnswer - Enhanced entity matching for question answering over linked data,-1,-1,-1,#excluded no linguistic features,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982843402&partnerID=40&md5=1f37b2483f4710ea1f8a2f3238080c2e,"QAnswer is a question answering system that uses DBpedia as a knowledge base and converts natural language questions into a SPARQL query. In order to improve the match between entities and relations and natural language text, we make use of Wikipedia to extract lexicalizations of the DBpedia entities and then match them with the question. These entities are validated on the ontology, while missing ones can be inferred. The proposed system was tested in the QALD-5 challenge and it obtained a F1 score of 0.30, which placed QAnswer in the second position in the challenge, despite the fact that the system used only a small subset of the properties in DBpedia, due to the long extraction process.",Final,,
Beyene M.; Portier P.-E.; Atnafu S.; Calabretto S.,Tensor Factorization for cross lingual entity co-reference resolution in the linked open data,-1,-1,-1,#excluded no linguistic features,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962739480&doi=10.1145%2f2857218.2857221&partnerID=40&md5=72043c8051e7adb679e1200263c74644,"The main objective of this research was to identify co-referent entities located in several linked open data (LOD) sources that are described in various natural languages. The problem is approached from two perspectives. First, we do a multi-scale analysis of the RDF graph to discover structural similarities of entities. This was implemented as a tensor decomposition of the RDF graph with each predicate corresponding to a horizontal slide of the tensor. Hereafter, we used the term ""structural evidence"" to refer to the result of this analysis. Second, for each entity, we associated textual data coming from the Web of documents. Thus, after some preprocessing (viz. removing empty words, applying a weighting scheme such as tf-idf, ⋯), we represented each entity in a high dimensional space with each dimension corresponding to a term. Next, through a Singular Value Decomposition (SVD), we find a subspace such that the sum of squared distances from the original space to the sub space is minimized. This dimensionality reduction allows us to find language independent similarities between entities. Hereafter, we use the term ""textual evidence"" to refer to the result of this analysis. Since the similarity information coming from the structural and the textual evidence are complementary to each other, a global similarity score is computed by aggregating the two evidences. We adopt a linear opinion pool, an approach which is commonly used in belief aggregation as an aggregation mechanism. In the end, for any given entity, we obtained a global similarity vector. The higher component values of this vector correspond to potential co-referent entities. All algorithms are implemented in Python. According to the experiment result conducted on the French and English DBpedia, our approach can bring high results. © 2015 ACM.",Final,,
Hakimov S.; Unger C.; Walter S.; Cimiano P.,Applying semantic parsing to question answering over linked data: Addressing the lexical gap,-1,-1,-1,#excluded,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948799577&doi=10.1007%2f978-3-319-19581-0_8&partnerID=40&md5=016d37f8a56437dea5c4290f9ffdfbdd,"Question answering over linked data has emerged in the past years as an important topic of research in order to provide natural language access to a growing body of linked open data on the Web. In this paper we focus on analyzing the lexical gap that arises as a challenge for any such question answering system. The lexical gap refers to the mismatch between the vocabulary used in a user question and the vocabulary used in the relevant dataset. We implement a semantic parsing approach and evaluate it on the QALD-4 benchmark, showing that the performance of such an approach suffers from training data sparseness. Its performance can, however, be substantially improved if the right lexical knowledge is available. To show this, we model a set of lexical entries by hand to quantify the number of entries that would be needed. Further, we analyze if a state-of-the-art tool for inducing ontology lexica from corpora can derive these lexical entries automatically. We conclude that further research and investments are needed to derive such lexical knowledge automatically or semi-automatically. © Springer International Publishing Switzerland 2015.",Final,All Open Access; Green Open Access,
Tran P.N.; Nguyen D.T.,Mapping expansion of natural language entities to DBpedia's components for querying linked data,-1,-1,-1,#excluded #outofscope,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926200010&doi=10.1145%2f2701126.2701212&partnerID=40&md5=ee141ba8e35f1f40de6dd5df5c8c2faa,This paper introduces the MEQLD method (Mapping Expansion of Natural Language Entities to DBpedia's Components for Querying Linked Data) that we propose to perform a part of the Task 1 (Multilingual Question Answering) [15] of CLEF 2013 lab QALD-3 (Multilingual Question Answering over Linked Data) [14]. MEQLD investigates to improve the mapping extension of (lexical) entities of English questions into DBpedia's components for creating query in SPARQL Query Language [12]. This paper will focus on resolving the most difficult testing questions of QALD-3 [18] that all their submitted systems have no good evaluation.,Final,,
Dima C.,Answering natural language questions with Intui3,-1,-1,-1,#excluded #outofscope,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981172784&partnerID=40&md5=6588b24ece46e909c89bbacbf6b7a499,"Intui3 is one of the participating systems at the fourth evaluation campaign on multilingual question answering over linked data, QALD4. The system accepts as input a question formulated in natural language (in English), and uses syntactic and semantic information to construct its interpretation with respect to a given database of RDF triples (in this case DBpedia 3.9). The interpretation is mapped to the corresponding SPARQL query, which is then run against a SPARQL endpoint to retrieve the answers to the initial question. Intui3 competed in the challenge called Task 1: Multilingual question answering over linked data, which offered 200 training questions and 50 test questions in 7 different languages. It obtained an F-measure of 0.24 by providing a correct answer to 10 of the test questions and a partial answer to 4 of them.",Final,,
Narducci F.; Palmonari M.; Semeraro G.,CroSeR: Cross-language semantic retrieval of open government data,-1,-1,-1,#excluded #outofscope,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899928218&doi=10.1007%2f978-3-319-06028-6_98&partnerID=40&md5=4f1821db1a3c7a6fe02d240734e8ca19,"CroSer (Cross-language Semantic Retrieval) is an ir system able to discover links between e-gov services described in different languages. CroSeR supports public administrators to link their own source catalogs of e-gov services described in any language to a target catalog whose services are described in English and are available in the Linked Open Data (lod) cloud. Our system is based on a cross-language semantic matching method that i) translates service labels in English using a machine translation tool, ii) extracts a Wikipedia-based semantic representation from the translated service labels using Explicit Semantic Analysis (esa), iii) evaluates the similarity between two services using their Wikipedia-based representations. The user selects a service in a source catalog and exploits the ranked list of matches suggested by CroSeR to establish a relation (of type narrower, equivalent, or broader match) with other services in the English catalog. The method is independent from the language adopted in the source catalog and it does not assume the availability of information about the services other than very short text descriptions used as service labels. CroSeR is a web application accessible via http://siti-rack.siti.disco.unimib.it:8080/croser/. © 2014 Springer International Publishing Switzerland.",Final,,
Narducci F.; Palmonari M.; Semeraro G.,Cross-language semantic retrieval and linking of e-gov services,-1,-1,-1,#excluded #outofscope,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891943734&doi=10.1007%2f978-3-642-41338-4_9&partnerID=40&md5=62d2a3605eddce6d96e8da09fd427fc9,"Public administrations are aware of the advantages of sharing Open Government Data in terms of transparency, development of improved services, collaboration between stakeholders, and spurring new economic activities. Initiatives for the publication and interlinking of government service catalogs as Linked Open Data (lod) support the interoperability among European administrations and improve the capability of foreign citizens to access services across Europe. However, linking service catalogs to reference lod catalogs requires a significant effort from local administrations, preventing the uptake of interoperable solutions at a large scale. The web application presented in this paper is named CroSeR (Cross-language Service Retriever) and supports public bodies in the process of linking their own service catalogs to the lod cloud. CroSeR supports different European languages and adopts a semantic representation of e-gov services based on Wikipedia. CroSeR tries to overcome problems related to the short textual descriptions associated to a service by embodying a semantic annotation algorithm that enriches service labels with emerging Wikipedia concepts related to the service. An experimental evaluation carried-out on e-gov service catalogs in five different languages shows the effectiveness of our model. © 2013 Springer-Verlag.",Final,All Open Access; Bronze Open Access; Green Open Access,
Ding X.; Zhang Y.; Liu T.; Duan J.,Knowledge-driven event embedding for stock prediction,-1,-1,-1,#excluded #outofscope,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051146159&partnerID=40&md5=b392aa1f2bf3a29ea48ad7df6175ff10,"Representing structured events as vectors in continuous space offers a new way for defining dense features for natural language processing (NLP) applications. Prior work has proposed effective methods to learn event representations that can capture syntactic and semantic information over text corpus, demonstrating their effectiveness for downstream tasks such as event-driven stock prediction. On the other hand, events extracted from raw texts do not contain background knowledge on entities and relations that they are mentioned. To address this issue, this paper proposes to leverage extra information from knowledge graph, which provides ground truth such as attributes and properties of entities and encodes valuable relations between entities. Specifically, we propose a joint model to combine knowledge graph information into the objective function of an event embedding learning model. Experiments on event similarity and stock market prediction show that our model is more capable of obtaining better event embeddings and making more accurate prediction on stock market volatilities. © 1963-2018 ACL.",Final,,
Zhang S.; Gao Y.; Yang S.,Multilingual Knowledge Graph Completion Based on Structure Features of the Dual-Branch,-1,-1,-1,#excluded #outofscope,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150708413&doi=10.1051%2fwujns%2f2023281045&partnerID=40&md5=0243693989e4259de111b7e1a109b8a7,"With the development of information fusion, knowledge graph completion tasks have received a lot of attention. some studies investigate the broader underlying problems of linguistics, while embedding learning has a narrow focus. This poses significant challenges due to the heterogeneity of coarse-graining patterns. Then, to settle the whole matter, a framework for completion is designed, named Triple Encoder-Scoring Module (TEsm). The model employs an alternating two-branch structure that fuses local features into the interaction pattern of the triplet itself by perfectly combining distance and structure models. Moreover, it is mapped to a uniform shared space. Upon completion, an ensemble inference method is proposed to query multiple predictions from different graphs using a weight classifier. Experiments show that the experimental dataset used for the completion task is DBpedia, which contains five different linguistic subsets.. Our extensive experimental results demonstrate that TEsm can efficiently and smoothly solve the optimal completion task, validating the performance of the proposed model. © 2023 Wuhan University.",Final,All Open Access; Green Open Access; Hybrid Gold Open Access,
Zhang H.; Jiang C.,Verb-driven machine reading comprehension with dual-graph neural network,-1,-1,-1,#excluded #outofscopeno semantic web technologies,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177181063&doi=10.1016%2fj.patrec.2023.11.008&partnerID=40&md5=5c86f6f33010e859a8a8bc157d66fe9d,"Logical reasoning of context is vital for reading comprehension, which requires to explore the logical relationship through sentence structure. However, previous methods of logical symbols and graph-based models do not make full explore the relationships among entities. In this paper, we present a verb-driven dual-graph network (VDGN) that utilizes core verbs of sentences to model the inter-sentence relationship by the ability of verbs to express linguistic context and the shortest dependency path to model the relationship between entities of intra-sentence. We construct a context graph and a query graph respectively through the above method. In order to predict the answer correctly, our framework fuses information from the context graph and the query graph applying a bi-directional attention mechanism on graph data. We evaluate our approach on two public logical reasoning machine reading comprehension(MRC) datasets: ReClor and LogiQA. Experiments on representative benchmark datasets demonstrate the effectiveness of our approach. © 2023 Elsevier B.V.",Final,,
Lymperaiou M.; Stamou G.,The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges,-1,-1,-1,#excluded #outofscope,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166465476&partnerID=40&md5=8fb195719401d3640ea348259374011d,"Recent advancements in visiolinguistic (VL) learning have allowed the development of multiple models and techniques that offer several impressive implementations, able to currently resolve a variety of tasks that require the collaboration of vision and language. Current datasets used for VL pre-training only contain a limited amount of visual and linguistic knowledge, thus significantly limiting the generalization capabilities of many VL models. External knowledge sources such as knowledge graphs (KGs) and Large Language Models (LLMs) are able to cover such generalization gaps by filling in missing knowledge, resulting in the emergence of hybrid architectures. In the current survey, we analyze tasks that have benefited from such hybrid approaches. Moreover, we categorize existing knowledge sources and types, proceeding to discussion regarding the KG vs LLM dilemma and its potential impact to future hybrid approaches.  © 2023 Copyright for this paper by its authors.",Final,,
Zhang F.; Zhang Z.; Ao X.; Gao D.; Zhuang F.; Wei Y.; He Q.,Mind the Gap: Cross-Lingual Information Retrieval with Hierarchical Knowledge Enhancement,-1,-1,-1,#excluded #outofscope,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140824664&partnerID=40&md5=4f9beece0f00a07c1ac016c832300531,"Cross-Lingual Information Retrieval (CLIR) aims to rank the documents written in a language different from the user's query. The intrinsic gap between different languages is an essential challenge for CLIR. In this paper, we introduce the multilingual knowledge graph (KG) to the CLIR task due to the sufficient information of entities in multiple languages. It is regarded as a “silver bullet” to simultaneously perform explicit alignment between queries and documents and also broaden the representations of queries. And we propose a model named CLIR with hierarchical knowledge enhancement (HIKE) for our task. The proposed model encodes the textual information in queries, documents and the KG with multilingual BERT, and incorporates the KG information in the query-document matching process with a hierarchical information fusion mechanism. Particularly, HIKE first integrates the entities and their neighborhood in KG into query representations with a knowledge-level fusion, then combines the knowledge from both source and target languages to further mitigate the linguistic gap with a language-level fusion. Finally, experimental results demonstrate that HIKE achieves substantial improvements over state-of-the-art competitors. Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",Final,,
Ponciano C.; Schaffert M.; Würriehausen F.; Ponciano J.-J.,Publish and Enrich Geospatial Data as Linked Open Data,-1,-1,-1,#excluded #outofscope,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146197990&partnerID=40&md5=fb8bb4d5a9bcf3c330d4fddedd2e0038,"The rapid growth of geospatial data (at least 20% every year) makes spatial data increasingly heterogeneous. With the emergence of Semantic Web technologies, more and more approaches are trying to group these data in knowledge graphs, allowing to link data together and to facilitate their sharing, use and maintenance. These approaches face the problem of homogenisation of these data which are not unified in the structure of the data on the one hand and on the other hand have a vocabulary that varies greatly depending on the application domain for which the data are dedicated and the language in which they are described. In order to solve this problem of homogenisation, we present in this paper the foundations of a framework allowing to group efficiently heterogeneous spatial data in a knowledge base. This knowledge base is based on an ontology linked to Schema.org and DCAT-AP, and provides a data structure compatible with GeoSPARQL. This framework allows the integration of geospatial data independently of their original language by translating them using Neural Machine Translation. Copyright © 2022 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.",Final,,
Bansal R.; Aggarwal M.; Bhatia S.; Kaur J.N.; Krishnamurthy B.,CoSe-Co: Text Conditioned Generative CommonSense Contextualizer,-1,-1,-1,#excluded #outofscope,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138416742&partnerID=40&md5=ec59166b5a37455e8796a5a2cdd887c8,"Pre-trained Language Models (PTLMs) have been shown to perform well on natural language tasks. Many prior works have leveraged structured commonsense present in the form of entities linked through labeled relations in Knowledge Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static module which limits coverage since KGs contain finite knowledge. Generative methods train PTLMs on KG triples to improve the scale at which knowledge can be obtained. However, training on symbolic KG entities limits their applicability in tasks involving natural language text where they ignore overall context. To mitigate this, we propose a CommonSense Contextualizer (CoSeCo) conditioned on sentences as input to make it generically usable in tasks for generating knowledge relevant to the overall context of input text. To train CoSe-Co, we propose a novel dataset comprising of sentence and commonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and contain novel entities not present in the underlying KG. We augment generated knowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading to improvements over current best methods on CSQA, ARC, QASC and OBQA datasets. We also demonstrate its applicability in improving performance of a baseline model for paraphrase generation task. © 2022 Association for Computational Linguistics.",Final,,
Kumar A.; Bharadwaj A.G.; Starly B.; Lynch C.,FabKG: A Knowledge graph of Manufacturing Science domain utilizing structured and unconventional unstructured knowledge source,-1,-1,-1,#excluded #outofscope no linguistic features,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139121316&partnerID=40&md5=ce94bba41046abcba74ebfd11efa5a66,"As the demands for large-scale information processing have grown, knowledge graph-based approaches have gained prominence for representing general and domain knowledge. The development of such general representations is essential, particularly in domains such as manufacturing which intelligent processes and adaptive education can enhance. Despite the continuous accumulation of text in these domains, the lack of structured data has created information extraction and knowledge transfer barriers. In this paper, we report on work towards developing robust knowledge graphs based upon entity and relation data for both commercial and educational uses. To create the FabKG (Manufacturing knowledge graph), we have utilized textbook index words, research paper keywords, FabNER (manufacturing NER), to extract a sub knowledge base contained within Wikidata. Moreover, we propose a novel crowdsourcing method for KG creation by leveraging student notes, which contain invaluable information but are not captured as meaningful information, excluding their use in personal preparation for learning and written exams. We have created a knowledge graph containing 65000+ triples using all data sources. We have also shown the use case of domain-specific question answering and expression/formula-based question answering for educational purposes. © 2022 Association for Computational Linguistics.",Final,,
Stokes P.A.,Holistically Modelling the Medieval Book: Towards a Digital Contribution,-1,-1,-1,#excluded #outofscope,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104341916&doi=10.1515%2fang-2021-0002&partnerID=40&md5=4f1956a8f0699764341ad4e5f950bf71,"The book has long played an important role in medieval and indeed modern culture, being at thesame time a carrier of texts and images, a sign potentially of wealth and/or education, a site of enquiry for modern scholarship for literature, history, linguistics, palaeography, codicology, art history, and more. The 'archaeology of the book' can tell us about its history (or biography) as well as the cultures that produced and used it, right up to its present ownership. This multidimensionality of the object has long been known, but it has also proven a challenge to digital approaches which (like all representations) are by their nature models that involve conscious or unconscious selection of particular aspects, and that have been more successful in some aspects than others. Thisthen raises the question to what degree these different viewpoints can be brought together into something approaching a holistic view, while always allowing for the tension between standardisation and innovation, and while remembering that a 'complete model' is a tautology, neither possible nor desirable.  © 2021 Walter de Gruyter GmbH, Berlin/Boston.",Final,,
Anand V.; Ramesh R.; Jin B.; Wang Z.; Lei X.; Lin C.-Y.,MultiModal Language Modelling on Knowledge Graphs for Deep Video Understanding,-1,-1,-1,#excluded #outofscope,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119362395&doi=10.1145%2f3474085.3479220&partnerID=40&md5=0f6c2c4ebc8eb5060ace30f8ebd1967a,"The natural language processing community has had a major interest in auto-regressive [4, 13] and span-prediction based language models [7] recently, while knowledge graphs are often referenced for common-sense based reasoning and fact-checking models. In this paper, we present an equivalence representation of span-prediction based language models and knowledge-graphs to better leverage recent developments of language modelling for multi-modal problem statements. Our method performed well, especially with sentiment understanding for multi-modal inputs, and discovered potential bias in naturally occurring videos when compared with movie-data interaction-understanding. We also release a dataset of an auto-generated questionnaire with ground-truths consisting of labels spanning across 120 relationships, 99 sentiments, and 116 interactions, among other labels for finer-grained analysis of model comparisons in the community. © 2021 ACM.",Final,,
Chen W.; Chen X.; Xiong S.,Global entity alignment with Gated Latent Space Neighborhood Aggregation,-1,-1,-1,#excluded #outofscope,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123405548&partnerID=40&md5=ba6adc99e7640ec0f7cc1975e13b3747,"Existing entity alignment models mainly use the topology structure of the original knowledge graph and have achieved promising performance. However, they are still challenged by the heterogeneous topological neighborhood structures, which could cause the models to produce different representations of counterpart entities. In the paper, we propose a global entity alignment model with gated latent space neighborhood aggregation (LatsEA) to address this challenge. Latent space neighborhood is formed by calculating the similarity between the entity embeddings, it can introduce long-range neighbors to expand the topological neighborhood and reconcile the heterogeneous neighborhood structures. Meanwhile, it uses vanilla GCN to aggregate the topological neighborhood and latent space neighborhood respectively. Then, it uses an average gating mechanism to aggregate topological neighborhood information and latent space neighborhood information of the central entity. In order to further consider the interdependence between entity alignment decisions, we propose a global entity alignment strategy, i.e., formulate entity alignment as the maximum bipartite matching problem, which is effectively solved by Hungarian algorithm. Our experiments with ablation studies on three real-world entity alignment datasets prove the effectiveness of the proposed model. Latent space neighborhood information and global entity alignment decisions both contributes to the entity alignment performance improvement. © 2021 China National Conference on Computational Linguistics Published under Creative Commons Attribution 4.0 International License",Final,,
Sun J.; Zhou Y.; Zong C.,Dual Attention Network for Cross-lingual Entity Alignment,-1,-1,-1,#excluded #outofscope no linguistic features,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118162781&partnerID=40&md5=61cd41f1405fabda8471fa50432ba931,"Cross-lingual Entity alignment is an essential part of building a knowledge graph, which can help integrate knowledge among different language knowledge graphs. In the real KGs, there exists an imbalance among the information in the same hierarchy of corresponding entities, which results in the heterogeneity of neighborhood structure, making this task challenging. To tackle this problem, we propose a dual attention network for cross-lingual entity alignment (DAEA). Specifically, our dual attention consists of relation-aware graph attention and hierarchical attention. The relation-aware graph attention aims at selectively aggregating multi-hierarchy neighborhood information to alleviate the difference of heterogeneity among counterpart entities. The hierarchical attention adaptively aggregates the low-hierarchy and the high-hierarchy information, which is beneficial to balance the neighborhood information of counterpart entities and distinguish non-counterpart entities with similar structures. Finally, we treat cross-lingual entity alignment as a process of linking prediction. Experimental results on three real-world cross-lingual entity alignment datasets have shown the effectiveness of DAEA. © 2020 COLING 2020 - 28th International Conference on Computational Linguistics, Proceedings of the Conference. All rights reserved.",Final,,
Xiao D.; Wang N.; Yu J.; Zhang C.; Wu J.,A Practice of Tourism Knowledge Graph Construction based on Heterogeneous Information,-1,-1,-1,#excluded #outofscope,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123925498&partnerID=40&md5=d8a760f15ff7c49db567686e5f7538ac,"The increasing amount of semi-structured and unstructured data on tourism websites brings a need for information extraction (IE) so as to construct a Tourism-domain Knowledge Graph (TKG), which is helpful to manage tourism information and develop downstream applications such as tourism search engine, recommendation and Q & A. However, the existing TKG is deficient, and there are few open methods to promote the construction and widespread application of TKG. In this paper, we present a systematic framework to build a TKG for Hainan, collecting data from popular tourism websites and structuring it into triples. The data is multi-source and heterogeneous, which raises a great challenge for processing it. So we develop two pipelines of processing methods for semi-structured data and unstructured data respectively. We refer to tourism InfoBox for semi-structured knowledge extraction and leverage deep learning algorithms to extract entities and relations from unstructured travel notes, which are colloquial and high-noise, and then we fuse the extracted knowledge from two sources. Finally, a TKG with 13 entity types and 46 relation types is established, which totally contains 34,079 entities and 441,371 triples. The systematic procedure proposed by this paper can construct a TKG from tourism websites, which can further applied to many scenarios and provide detailed reference for the construction of other domain-specific knowledge graphs. © 2020 China National Conference on Computational Linguistics Published under Creative Commons Attribution 4.0 International License",Final,,
Shang C.; Dash S.; Chowdhury M.F.M.; Mihindukulasooriya N.; Gliozzo A.,Taxonomy construction of unseen domains via graph-based cross-domain knowledge transfer,-1,-1,-1,#excluded #outofscope,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101645961&partnerID=40&md5=8b71815e0578f3c59caba73dcc253e7c,"Extracting lexico-semantic relations as graph-structured taxonomies, also known as taxonomy construction, has been beneficial in a variety of NLP applications. Recently Graph Neural Network (GNN) has shown to be powerful in successfully tackling many tasks. However, there has been no attempt to exploit GNN to create taxonomies. In this paper, we propose Graph2Taxo, a GNN-based cross-domain transfer framework for the taxonomy construction task. Our main contribution is to learn the latent features of taxonomy construction from existing domains to guide the structure learning of an unseen domain. We also propose a novel method of directed acyclic graph (DAG) generation for taxonomy construction. Specifically, our proposed Graph2Taxo uses a noisy graph constructed from automatically extracted noisy hyponym-hypernym candidate pairs, and a set of taxonomies for some known domains for training. The learned model is then used to generate taxonomy for a new unknown domain given a set of terms for that domain. Experiments on benchmark datasets from science and environment domains show that our approach attains significant improvements correspondingly over the state of the art. © 2020 Association for Computational Linguistics",Final,,
Liu Y.; Yang T.; You Z.; Fan W.; Yu P.S.,Commonsense evidence generation and injection in reading comprehension,-1,-1,-1,#excluded #outofscope no linguistic features,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112400683&partnerID=40&md5=beb4c3d7e5ce103f367754e3f5ac9179,"Human tackle reading comprehension not only based on the given context itself but often rely on the commonsense beyond. To empower the machine with commonsense reasoning; in this paper, we propose a Commonsense Evidence Generation and Injection framework in reading comprehension, named CEGI. The framework injects two kinds of auxiliary commonsense evidence into comprehensive reading to equip the machine with the ability of rational thinking. Specifically, we build two evidence generators: one aims to generate textual evidence via a language model; the other aims to extract factual evidence (automatically aligned text-triples) from a commonsense knowledge graph after graph completion. Those evidences incorporate contextual commonsense and serve as the additional inputs to the reasoning model. Thereafter, we propose a deep contextual encoder to extract semantic relationships among the paragraph, question, option, and evidence. Finally, we employ a capsule network to extract different linguistic units (word and phrase) from the relations, and dynamically predict the optimal option based on the extracted units. Experiments on the CosmosQA dataset demonstrate that the proposed CEGI model outperforms the current state-ofthe- art approaches and achieves the highest accuracy (83.6%) on the leaderboard.  © 2020 Association for Computational Linguistics.",Final,,
Ahmed A.F.; Sherif M.A.; Ngomo A.-C.N.,Do your resources sound similar?: On the impact of using phonetic similarity in link discovery,-1,-1,-1,#excluded #outofscope no semantic web technologies,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077281634&doi=10.1145%2f3360901.3364426&partnerID=40&md5=4e795910464eb701e47e6e66aba297ce,"An increasing number of heterogeneous datasets abiding by the Linked Data paradigm is published everyday. Discovering links between these datasets is thus central to achieving the vision behind the Data Web. Declarative Link Discovery (LD) frameworks rely on complex Link Specification (LS) to express the conditions under which two resources should be linked. Complex LS combine similarity measures with thresholds to determine whether a given predicate holds between two resources. State of the art LD frameworks rely mostly on string-based similarity measures such as Levenshtein and Jaccard. However, string-based similarity measures often fail to catch the similarity of resources with phonetically similar property values when these property values are represented using different string representation (e.g., names and street labels). In this paper, we evaluate the impact of using phonetics-based similarities in the process of LD. Moreover, we evaluate the impact of phonetic-based similarity measures on a state-of-the-art machine learning approach used to generate LS. Our experiments suggest that the combination of string-based and phonetic-based measures can improve the F-measures achieved by LD frameworks on most datasets. © 2019 ACM.",Final,,
Moussallem D.; Wauer M.; Ngomo A.-C.N.,Machine Translation using Semantic Web Technologies: A Survey,-1,-1,-1,#excluded #outofscope,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050401657&doi=10.1016%2fj.websem.2018.07.001&partnerID=40&md5=17bff552c618b5eb882b13c045373e4c,"A large number of machine translation approaches have recently been developed to facilitate the fluid migration of content across languages. However, the literature suggests that many obstacles must still be dealt with to achieve better automatic translations. One of these obstacles is lexical and syntactic ambiguity. A promising way of overcoming this problem is using Semantic Web technologies. This article presents the results of a systematic review of machine translation approaches that rely on Semantic Web technologies for translating texts. Overall, our survey suggests that while Semantic Web technologies can enhance the quality of machine translation outputs for various problems, the combination of both is still in its infancy. © 2018 Elsevier B.V.",Final,All Open Access; Green Open Access,
Garcia-Silva A.; Gomez-Perez J.M.,Not just about size - A study on the role of distributed word representations in the analysis of scientific publications,-1,-1,-1,#excluded #outofscope,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048346653&partnerID=40&md5=2f26e252d5b0328167684c77e6280863,"The emergence of knowledge graphs in the scholarly communication domain and recent advances in artificial intelligence and natural language processing bring us closer to a scenario where intelligent systems can assist scientists over a range of knowledge-intensive tasks. In this paper we present experimental results about the generation of word embeddings from scholarly publications for the intelligent processing of scientific texts extracted from SciGraph. We compare the performance of domain-specific embeddings with existing pre-trained vectors generated from very large and general purpose corpora. Our results suggest that there is a trade-off between corpus speciflcity and volume. Embeddings from domain-specific scientific corpora effectively capture the semantics of the domain. On the other hand, obtaining comparable results through general corpora can also be achieved, but only in the presence of very large corpora of well formed text. Furthermore, we also show that the degree of overlapping between knowledge areas is directly related to the performance of embeddings in domain evaluation tasks. © 2018 CEUR Workshop Proceedings. All rights reserved.",Final,,
Han X.; Liu Z.; Sun M.,Neural knowledge acquisition via mutual attention between knowledge graph and text,-1,-1,-1,#excluded #outofscope,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060500143&partnerID=40&md5=17c79a5a40150bd0e5fae1214d4a8f93,"We propose a general joint representation learning framework for knowledge acquisition (KA) on two tasks, knowledge graph completion (KGC) and relation extraction (RE) from text. In this framework, we learn representations of knowledge graphs (KGs) and text within a unified parameter sharing semantic space. To achieve better fusion, we propose an effective mutual attention between KGs and text. The reciprocal attention mechanism enables us to highlight important features and perform better KGC and RE. Different from conventional joint models, no complicated linguistic analysis or strict alignments between KGs and text are required to train our models. Experiments on relation extraction and entity link prediction show that models trained under our joint framework are significantly improved in comparison with other baselines. Most existing methods for KGC and RE can be easily integrated into our framework due to its flexible architectures. The source code of this paper can be obtained from https://github.com/thunlp/JointNRE. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",Final,,
Trandabat D.; Gifu D.,Social Media and the Web of Linked Data,-1,-1,-1,#excluded,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027959093&doi=10.1109%2fJCDL.2017.7991624&partnerID=40&md5=91788d3e7156d6dfb7efc8586fee346f,"Written texts have perhaps never been so widely used as they are in today's social media context, with people constantly writing, sharing, commenting, getting involved. At the same time, Linked Data is emerging as an increasingly important topic, and research in this area has resulted in massive amounts of structured linguistic data. In this climate, we intend to analyze how linked data can help to structure and extract meaning from social media's short, informal and context dependent texts, with an emphasis on real-life applications. © 2017 IEEE.",Final,,
Both F.; Thoma S.; Rettinger A.,Cross-modal knowledge transfer: Improving the word embedding of apple by looking at oranges,-1,-1,-1,#excluded #outofscope,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040602084&doi=10.1145%2f3148011.3148026&partnerID=40&md5=824cabcd87f1f2c599a636242b7ee82f,"Capturing knowledge via learned latent vector representations of words, images and knowledge graph (KG) entities has shown state of-the-art performance in computer vision, computational linguistics and KG tasks. Recent results demonstrate that the learning of such representations across modalities can be beneficial, since each modality captures complementary information. However, those approaches are limited to concepts with cross-modal alignments in the training data which are only available for just a few concepts. Especially for visual objects exist far fewer embeddings than for words or KG entities. We investigate whether a word embedding (e.g., for ""apple"") can still capture information from other modalities even if there is no matching concept within the other modalities (i.e., no images or KG entities of apples but of oranges as pictured in the title analogy). The empirical results of our knowledge transfer approach demonstrate that word embeddings do benefit from extrapolating information across modalities even for concepts that are not represented in the other modalities. Interestingly, this applies most to concrete concepts (e.g., dragonfly) while abstract concepts (e.g., animal) benefit most if aligned concepts are available in the other modalities. © 2017 Copyright held by the owner/author(s).",Final,,
Chiarcos C.; Cimiano P.; Declerck T.; McCrae J.P.,Linguistic Linked Open Data (LLOD) Introduction and Overview,-1,,-1,#excluded #workshopreview,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059046541&partnerID=40&md5=9f2ae4ae81b4f4a20c4a076f5c4b661f,"The explosion of information technology has led to a substantial growth in quantity, diversity and complexity of linguistic data accessible over the internet. The lack of interoperability between linguistic and language resources represents a major challenge that needs to be addressed, in particular, if information from different sources is to be combined, like, say, machine-readable lexicons, corpus data and terminology repositories. For these types of resources, domain- specific standards have been proposed, yet, issues of interoperability between different types of resources persist, commonly accepted strategies to distribute, access and integrate their information have yet to be established, and technologies and infrastructures to address both aspects are still under development. The goal of the 2nd Workshop on Linked Data in Linguistics (LDL-2013) has been to bring together researchers from various fields of linguistics, natural language processing, and information technology to present and discuss principles, case studies, and best practices for representing, publishing and linking linguistic data collections, including corpora, dictionaries, lexical networks, translation memories, thesauri, etc., infrastructures developed on that basis, their use of existing standards, and the publication and distribution policies that were adopted. © LDL 2013.All right reserved.",Final,,
Ryman A.G.; Le Hors A.J.; Speicher S.,OSLC resource shape a language for defining constraints on linked data,-1,-1,-1,#excluded,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916627887&partnerID=40&md5=714ac7f0d20d83c7e56598ec8356022c,"IBM has for several years been employing a read/write usage of Linked Data as an architectural style for integrating a suite of applications. We are encouraged by the work done by the W3C Linked Data Platform Working Group which is chartered to produce a W3C Recommendation for HTTP-based (RESTful) application integration patterns using read/write Linked Data . The Linked Data Platform Recommendation will provide the industry with a solid foundation to build on. Yet, more work will need to be done to address in a standard way the needs of enterprise solutions that use Linked Data as an application integration platform. One such need is a type definition language that can be used to communicate and validate constraints on RDF data. This paper explains the need for such a language, why standards like RDFS and OWL are not suitable answers and, finally, introduces OSLC Resource Shapes as a proposed solution.",Final,,
Gracia J.; Montiel-Ponsoda E.; Cimiano P.; Gómez-Pérez A.; Buitelaar P.; McCrae J.,Challenges for the multilingual Web of Data,-1,-1,-1,#excluded,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857791906&doi=10.1016%2fj.websem.2011.09.001&partnerID=40&md5=2bd276fb961603349bea90176b48e706,"The Web has witnessed an enormous growth in the amount of semantic information published in recent years. This growth has been stimulated to a large extent by the emergence of Linked Data. Although this brings us a big step closer to the vision of a Semantic Web, it also raises new issues such as the need for dealing with information expressed in different natural languages. Indeed, although the Web of Data can contain any kind of information in any language, it still lacks explicit mechanisms to automatically reconcile such information when it is expressed in different languages. This leads to situations in which data expressed in a certain language is not easily accessible to speakers of other languages. The Web of Data shows the potential for being extended to a truly multilingual web as vocabularies and data can be published in a language-independent fashion, while associated language-dependent (linguistic) information supporting the access across languages can be stored separately. In this sense, the multilingual Web of Data can be realized in our view as a layer of services and resources on top of the existing Linked Data infrastructure adding (i) linguistic information for data and vocabularies in different languages, (ii) mappings between data with labels in different languages, and (iii) services to dynamically access and traverse Linked Data across different languages. In this article, we present this vision of a multilingual Web of Data. We discuss challenges that need to be addressed to make this vision come true and discuss the role that techniques such as ontology localization, ontology mapping, and cross-lingual ontology-based information access and presentation will play in achieving this. Further, we propose an initial architecture and describe a roadmap that can provide a basis for the implementation of this vision. © 2011 Elsevier B.V. All rights reserved.",Final,All Open Access; Green Open Access,
Yang Y.; Singh P.; Yao J.; Au Yeung C.-M.; Zareian A.; Wang X.; Cai Z.; Salvadores M.; Gibbins N.; Hall W.; Shadbolt N.,Distributed human computation framework for linked data co-reference resolution,-1,-1,-1,#excluded,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960061916&doi=10.1007%2f978-3-642-21034-1_3&partnerID=40&md5=264031c840ba1fd772cd249da2afb7f5,"Distributed Human Computation (DHC) is used to solve computational problems by incorporating the collaborative effort of a large number of humans. It is also a solution to AI-complete problems such as natural language processing. The Semantic Web with its root in AI has many research problems that are considered as AI-complete. E.g. co-reference resolution, which involves determining whether different URIs refer to the same entity, is a significant hurdle to overcome in the realisation of large-scale Semantic Web applications. In this paper, we propose a framework for building a DHC system on top of the Linked Data Cloud to solve various computational problems. To demonstrate the concept, we are focusing on handling the co-reference resolution when integrating distributed datasets. Traditionally machine-learning algorithms are used as a solution for this but they are often computationally expensive, error-prone and do not scale. We designed a DHC system named iamResearcher, which solves the scientific publication author identity co-reference problem when integrating distributed bibliographic datasets. In our system, we aggregated 6 million bibliographic data from various publication repositories. Users can sign up to the system to audit and align their own publications, thus solving the co-reference problem in a distributed manner. The aggregated results are dereferenceable in the Open Linked Data Cloud. © 2011 Springer-Verlag Berlin Heidelberg.",Final,All Open Access; Bronze Open Access; Green Open Access,
Ferrara A.; Nikolov A.; Scharffe F.,Data linking for the semantic web,-1,-1,-1,#excluded,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857578944&doi=10.4018%2fjswis.2011070103&partnerID=40&md5=0a41deed4891b8a445ca725d1cec6536,"By specifying that published datasets must link to other existing datasets, the 4th linked data principle ensures a Web of data and not just a set of unconnected data islands. The authors propose in this paper the term data linking to name the problem of finding equivalent resources on the Web of linked data. In order to perform data linking, many techniques were developed, finding their roots in statistics, database, natural language processing and graph theory. The authors begin this paper by providing background information and terminological clarifications related to data linking. Then a comprehensive survey over the various techniques available for data linking is provided. These techniques are classified along the three criteria of granularity, type of evidence, and source of the evidence. Finally, the authors survey eleven recent tools performing data linking and we classify them according to the surveyed techniques. © 2011 IGI Global.",Final,All Open Access; Green Open Access,
Sun W.; Celli F.; Morshed A.; Jaques Y.; Keizer J.,Design and implementation of a SOLR plug-in for Chinese-English cross-language query expansion based on SKOS thesauri,-1,,-1,#excluded #notavailable,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856889354&doi=10.1007%2f978-3-642-25992-0_51&partnerID=40&md5=748eae0a3b5aa1c7b35f698d8b7eb6fa,"Given that existing studies for query expansion techniques for Chinese-English are relatively few and their level of standardization low, in order to improve efficiency of Chinese-English cross-language retrieval, this paper discusses the design and implementation of a SOLR plug-in for Chinese-English cross-language query expansion based on SKOS thesauri and used within the AGRIS agricultural bibliographic system. The paper also elaborates the key techniques involved in the plug-in. Finally, taking the AGRIS data resources as an example, the paper shows application examples for segmentation of mixed Chinese and English, user query parsing and AGRIS retrieval system etc., techniques that have improved the Chinese-English cross-language retrieval efficiency to a certain extent, and laid a technical foundation for research about knowledge retrieval and discovery in related fields. © 2011 Springer-Verlag.",Final,All Open Access; Green Open Access,
Hulliyah K.; Kusuma H.T.,Application of knowledge graph for making Text Summarization (Analizing a text of educational issues),-1,-1,-1,#excluded #outofscope not lingusitic features,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052351173&doi=10.1109%2fICT4M.2010.5971919&partnerID=40&md5=649b688264fcf2756bc2c2e0fc8b129d,"Text Summerization, is a topic that is related to the fields of philosophy and linguistics [2] are also included in the social sciences, so often sought by researchers in computer science who prefer something in the field of exact sciences. Interestingly, the Text Summerization applications, is a system that will make a summary or conclusion of tens or hundreds of texts with similar themes in an overview that produces new knowledge. This system becomes very important if you have a problem cases to take a series of conclusions from existing text. The purpose of this research is to develop knowledge-graph method to do case studies to determine whether the knowledge graph that can be used as an efficient instrument in analyzing a text with the same theme in large numbers, then the results become much simpler in the form of graphs that can be used as new knowledge. © 2010 IEEE.",Final,,
Thoutenhoofd E.D.,The development of a filemaker Pro database for the morphemic analysis of productive forms in BSL,-1,,-1,#excluded #notavailable,2001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989380176&doi=10.1075%2fsll.4.12.08tho&partnerID=40&md5=ad5a8b81a32162efdc034f684e4e4248,"This article reports on a ‘rapid application development’ or RAD process to construct a research database in entry-level commercial database software, in this case FileMaker Pro. The database was required for a sign linguistic investigation into the morphology of the productive lexicon of British Sign Language (cf. Brennan’s article in this volume). Although at an early stage of development, the productive lexicon database (PLD) is an open and modifiable set of loosely-linked data files which can be reconfigured and remodelled according to user requirements, research aims or commercial objectives. The example offered is that of a trilingual sign/spoken language dictionary. The PLD’s flexible data model allows, as a matter of principle, for the combination of datasets irrespective of linguistic conventions used for data description. It is suggested that data models of this kind therefore open opportunities for research collaboration and commercial exploitation without necessitating detailed prior agreement on linguistic data description conventions or standards. © 2001 John Benjamins Publishing Company.",Final,,
,Mixedemotions: Social semantic emotion analysis for innovative multilingual big data analytics markets,-1,,-1,#excluded #notpeerreviewed,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084018220&partnerID=40&md5=82531f0de0132d37ea389517ab75d1f1,"Emotion analysis is central to tracking customer and user behaviour and satisfaction, which can be observed from user interaction in the form of explicit feedback through email, call centre interaction, social media comments, etc., as well as implicit acknowledgment of approval or rejection through facial expressions, speech or other non-verbal feedback. In Europe specifically, but increasingly also globally, an added factor here is that user feedback can be in multiple languages, in text as well as in speech and audio-visual content. This implies different cultural backgrounds and thus different ways to produce and perceive emotions in everyday interactions, beyond the fact of having specific rules for encoding and decoding emotions in each language. Making sense of accumulated user interaction from different (‘mixed’) data sources, modalities and languages is challenging and has not yet been explored in fullness in an industrial context. Commercial solutions exist but do not address the multilingual aspect in a robust and large-scale setting and do not scale up to huge data volumes that need to be processed, or the integration of emotion analysis observations across data sources and/or modalities on a meaningful level, i.e. keeping track of entities involved as well the connections between them (who said what? to whom? in the context of which event, product, service?) The MixedEmotions project will implement an integrated Big Linked Data platform for emotion analysis across heterogeneous data sources, languages and modalities, building on existing state-of-the-art tools, services and approaches that will enable the tracking of emotional aspects of user interaction and feedback on an entity level. The platform will provide an integrated solution for: Large-scale emotion analysis and fusion on heterogeneous, multilingual, text, speech, video and social media data streams, leveraging open access and proprietary data sources, exploiting also social context by leveraging social network graphs Semantic-level emotion information aggregation and integration through robust extraction of social semantic knowledge graphs for emotion analysis along multidimensional clusters The platform will be developed and evaluated in the context of three cross-domain pilot projects that are representative of a variety of data analytics markets: Social TV, Brand Reputation Management, Call Centre Operations. © 2015 European Association for Machine Translation. All rights reserved.",Final,,
Patton E.W.; McGuinness D.L.,Connecting science data using semantics and information extraction,-1,-1,-1,#excluded #outofscope no linguistic features,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939491390&partnerID=40&md5=0f783dcd1f20db9e4bfbc37c5886f4c0,"We are developing prototypes that explicate our vision of connecting personal medical data to scientific literature as well as to emerging grey literature (e.g., community forums) to help people find and understand information relevant to complex medical journeys. We focus on robust combinations of natural language processing along with linked data and knowledge representation to build knowledge graphs that help people make sense of current conditions and enable new manners of scientific hypothesis generation. We present our work in the context of a breast cancer use case. We discuss the benefits of biomedical linked data resources and describe some potential assistive technology for navigating rich, diverse medical content.",Final,,
Qiao F.; Zhu X.,Domain Intelligent QA user intention recognition based on keyword separation,-1,-1,-1,#excluded #outofscope no linguistic features,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099694451&doi=10.1109%2fICCST50977.2020.00049&partnerID=40&md5=808a2d0906815dff59e6d367e744b377,"In view of the disadvantages of current intelligent QA user intention recognition technology, which can't make good use of the detailed features in user questions, this paper proposes a layered method to identify user intention. This method first identifies the keywords in the user problem as the user's first level intention, then distinguishes the sentence pattern of the user problem according to the number of keywords, and determines whether to identify the second level intention according to the sentence pattern of the problem. Then, the key word features are fused to identify the user's problem type, and the result of problem type recognition is regarded as the second layer of user's intention. Finally, the user's two layers of intentions are integrated and regarded as the user's overall intentions, and retrieved in the Knowledge Graph. Experiments show that the F1 value of user problem type recognition is increased by 6% after keyword features are added. The method of keyword separation is used to identify the user's intention in question and answer, which separates the two layers of user's intention. For the problem sentence pattern that is not necessary to recognize the second layer of user's intention, the judgment of user's intention is no longer performed, which reduces the complexity of user's intention recognition, and opens up space for the case that keywords need to be processed. In addition, keyword features are added to the second layer of user intention recognition model, which makes better use of the detailed features in user question statements and improves the effect of user intention recognition.  © 2020 IEEE.",Final,,
Chiarcos C.,Get! Mimetypes! Right!,-1,1,-1,#related,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115075174&doi=10.4230%2fOASIcs.LDK.2021.5&partnerID=40&md5=07f2f8c24f0ace3cceefe7e36dbf53ad,"This paper identifies three technical requirements - availability of data, sustainable hosting and resolvable URIs for hosted data - as minimal pre-conditions for Linguistic Linked Open Data technology to develop towards a mature technological ecosystem that third party applications can build upon. While a critical amount of data is available (and it continues to grow), there does not seem to exist a hosting solution that combines the prospects of long-term availability with an unrestricted capability to support resolvable URIs. In particular, data hosting services do currently not allow data to be declared as RDF content by means of their media type (mime type), so that the capability of clients to recognize formats and to resolve URIs on that basis is severely limited. © Christian Chiarcos; licensed under Creative Commons License CC-BY 4.0",Final,,
Ramnath K.; Sari L.; Hasegawa-Johnson M.; Yoo C.,Worldly Wise (WoW) - Cross-Lingual Knowledge Fusion for Fact-based Visual Spoken-Question Answering,0,-1,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137712266&partnerID=40&md5=cef852d346571919b08d1629b1e076ba,"Although Question-Answering has long been of research interest, its accessibility to users through a speech interface and its support to multiple languages have not been addressed in prior studies. Towards these ends, we present a new task and a synthetically-generated dataset to do Fact-based Visual Spoken-Question Answering (FVSQA). FVSQA is based on the FVQA dataset, which requires a system to retrieve an entity from Knowledge Graphs (KGs) to answer a question about an image. In FVSQA, the question is spoken rather than typed. Three sub-tasks are proposed: (1) speech-to-text based, (2) end-to-end, without speech-to-text as an intermediate component, and (3) cross-lingual, in which the question is spoken in a language different from that in which the KG is recorded. The end-to-end and cross-lingual tasks are the first to require world knowledge from a multi-relational KG as a differentiable layer in an end-to-end spoken language understanding task, hence the proposed reference implementation is called Worldly-Wise (WoW). WoW is shown to perform end-to-end cross-lingual FVSQA at same levels of accuracy across 3 languages - English, Hindi, and Turkish. © 2021 Association for Computational Linguistics.",Final,,
Pan J.Z.; Edelstein E.; Bansky P.; Wyner A.,A knowledge graph based approach to social science surveys,0,-1,-1,#excluded #outofscope no linguistic features,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118212118&doi=10.1162%2fdint_a_00107&partnerID=40&md5=5cbe3b4320d6fbe1437bfd2b7caa966c,"Recent success of knowledge graphs has spurred interest in applying them in open science, such as on intelligent survey systems for scientists. However, efforts to understand the quality of candidate survey questions provided by these methods have been limited. Indeed, existing methods do not consider the type of on-the-fly content planning that is possible for face-to-face surveys and hence do not guarantee that selection of subsequent questions is based on response to previous questions in a survey. To address this limitation, we propose a dynamic and informative solution for an intelligent survey system that is based on knowledge graphs. To illustrate our proposal, we look into social science surveys, focusing on ordering the questions of a questionnaire component by their level of acceptance, along with conditional triggers that further customise participants’ experience. Our main findings are: (i) evaluation of the proposed approach shows that the dynamic component can be beneficial in terms of lowering the number of questions asked per variable, thus allowing more informative data to be collected in a survey of equivalent length; and (ii) a primary advantage of the proposed approach is that it enables grouping of participants according to their responses, so that participants are not only served appropriate follow-up questions, but their responses to these questions may be analysed in the context of some initial categorisation. We believe that the proposed approach can easily be applied to other social science surveys based on grouping definitions in their contexts. The knowledge-graph-based intelligent survey approach proposed in our work allows online questionnaires to approach face-to-face interaction in their level of informativity and responsiveness, as well as duplicating certain advantages of interview-based data collection. © 2021 Chinese Academy of Sciences.",Final,All Open Access; Gold Open Access; Green Open Access,
Augenstein I.; Gentile A.L.; Norton B.; Zhang Z.; Ciravegna F.,Mapping keywords to Linked Data resources for automatic query expansion,0,-1,-1,#excluded encyclopedic knowledge rather than linguistic,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922580693&partnerID=40&md5=8965215840be6c3d4d6bbd95e2170a49,"Linked Data is a gigantic, constantly growing and extremely valuable resource, but its usage is still heavily dependent on (i) the familiarity of end users with RDF's graph data model and its query language, SPARQL, and (ii) knowledge about available datasets and their contents. Intelligent keyword search over Linked Data is currently being investigated as a means to overcome these barriers to entry in a number of different approaches, including semantic search engines and the automatic conversion of natural language questions into structured queries. Our work addresses the specific challenge of mapping keywords to Linked Data resources, and proposes a novel method for this task. By exploiting the graph structure within Linked Data we determine which properties between resources are useful to discover, or directly express, semantic similarity. We also propose a novel scoring function to rank results. Experiments on a publicly available dataset show a 17% improvement in Mean Reciprocal Rank over the state of the art.",Final,,
Peng Y.; Alam M.; Bonald T.,Ontology Matching using Textual Class Descriptions,0,-1,-1,#excluded encyclopedic knowledge rather than linguistic,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180776271&partnerID=40&md5=5115bca4fda57f89bac2412fb02a8506,"In this paper, we propose TEXTO, a TEXT-based Ontology matching system. This matcher leverages the rich semantic information of classes available in most ontologies by a combination of a pre-trained word embedding model and a pre-trained language model. Its performance is evaluated on the datasets of the OAEI Common Knowledge Graphs Track, augmented with the description of each class, and a new dataset based on the refreshed alignment of Schema.org and Wikidata. Our results demonstrate that TEXTO outperforms all state-of-art matchers in terms of precision, recall and F1 score. In particular, we show that almost perfect class alignment can be achieved using textual content only, excluding any structural information like the graph of classes or the instances of each class. © 2023 Copyright for this paper by its authors.",Final,,
Faulhaber C.B.; Rodríguez Ó.P.,PHILOBIBLON AS A DIGITAL TOOL FOR HISTORIANS OF MEDIEVAL IBERIA,0,-1,-1,#excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172665109&doi=10.21001%2fitma.2023.16.15&partnerID=40&md5=7c0c4d10eed7b65ccf47e98c535a97d7,"This article provides a succinct review of the history and technological development of PhiloBiblon, one of the pioneer Digital Humanities projects for the study of the primary sources of the medieval and early modern literatures of the Iberian Peninsula. The warp and weft of its history are the four bibliographies that comprise it: BETA (Bibliografía Española de Textos Antiguos), BITECA (Bibliografia de Textos Antics Catalans, Valencians i Balears), BITAGAP (Bibliografia de Textos Antigos Galegos e Portugueses), and BIPA (Bibliografía de la Poesía Áurea). We describe how the program functions on the web, paying particular attention to the use of PhiloBiblon's key identifiers. Then we explain the proposed evolution of the project from siloed databases to Linked Open Data via FactGrid, a Database for Historians. Precisely because of this pending change, we wish to show medievalists other than literary and linguistic specialists, especially historians, how to make good use of PhiloBiblon. © 2023 Consolidated Medieval Studies Research Group. All rights reserved.",Final,All Open Access; Green Open Access,
Zahera H.M.; Vollmers D.; Sherif M.A.; Ngomo A.-C.N.,MultPAX: Keyphrase Extraction Using Language Models and Knowledge Graphs,0,-1,-1,#excluded no linguistic features,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141678744&doi=10.1007%2f978-3-031-19433-7_18&partnerID=40&md5=5b3b91928f79921283b3ebdb9ea8ed4e,"Keyphrase extraction aims to identify a small set of phrases that best describe the content of text. The automatic generation of keyphrases has become essential for many natural language applications such as text categorization, indexing, and summarization. In this paper, we propose MultPAX, a multitask framework for extracting present and absent keyphrases using pre-trained language models and knowledge graphs. In particular, our framework contains three components: first, MultPAX identifies present keyphrases from an input document. Then, MultPAX links with external knowledge graphs to get more relevant phrases. Finally, MultPAX ranks the extracted phrases based on their semantic relatedness to the input document and return top-k phrases as a final output. We conducted several experiments on four benchmark datasets to evaluate the performance of MultPAX against different state-of-the-art baselines. The evaluation results demonstrate that our approach significantly outperforms the state-of-the-art baselines, with a significance t-test p&lt; 0.041. Our source code and datasets are public available at https://github.com/dice-group/MultPAX. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Final,,
Hu L.; Yang T.; Zhang L.; Zhong W.; Tang D.; Shi C.; Duan N.; Zhou M.,Compare to the knowledge: Graph neural fake news detection with external knowledge,1,-1,-1,#excluded no linguistic features,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118918173&partnerID=40&md5=d45c2848d5b7bf759bf5812f6d2aabf0,"Nowadays, fake news detection, which aims to verify whether a news document is trusted or fake, has become urgent and important. Most existing methods rely heavily on linguistic and semantic features from the news content, and fail to effectively exploit external knowledge which could help determine whether the news document is trusted. In this paper, we propose a novel end-to-end graph neural model called CompareNet, which compares the news to the knowledge base (KB) through entities for fake news detection. Considering that fake news detection is correlated with topics, we also incorporate topics to enrich the news representation. Specifically, we first construct a directed heterogeneous document graph for each news incorporating topics and entities. Based on the graph, we develop a heterogeneous graph attention network for learning the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news content. The contextual entity representations are then compared to the corresponding KB-based entity representations through a carefully designed entity comparison network, to capture the consistency between the news content and KB. Finally, the topic-enriched news representation combining the entity comparison features are fed into a fake news classifier. Experimental results on two benchmark datasets demonstrate that CompareNet significantly outperforms state-of-the-art methods. © 2021 Association for Computational Linguistics",Final,,
Murnane E.L.; Haslhofer B.; Lagoze C.,RESLVE: Leveraging user interest to improve entity disambigution on short text,1,0/1,-1,#excluded,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893101000&partnerID=40&md5=3cfbafcf3d45ec74a33b0a5b0445c79e,"We address the Named Entity Disambiguation (NED) problem for short, user-generated texts on the social Web. In such settings, the lack of linguistic features and sparse lexical context result in a high degree of ambiguity and sharp performance drops of nearly 50% in the accuracy of conventional NED systems. We handle these challenges by developing a model of user-interest with respect to a personal knowledge context; and Wikipedia, a particularly well-established and reliable knowledge base, is used to instantiate the procedure. We conduct systematic evaluations using individuals' posts from Twitter, YouTube, and Flickr and demonstrate that our novel technique is able to achieve substantial performance gains beyond state-of-the-art NED methods.",Final,,
Hulpus I.; Štajner S.; Stuckenschmidt H.,A spreading activation framework for tracking conceptual complexity of texts,1,-1,-1,#excluded lack of linguistic features,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084068774&partnerID=40&md5=f03a787428be66767cfcf9de97df3a56,"We propose an unsupervised approach for assessing conceptual complexity of texts, based on spreading activation. Using DBpedia knowledge graph as a proxy to long-term memory, mentioned concepts become activated and trigger further activation as the text is sequentially traversed. Drawing inspiration from psycholinguistic theories of reading comprehension, we model memory processes such as semantic priming, sentence wrap-up, and forgetting. We show that our models capture various aspects of conceptual text complexity and significantly outperform current state of the art. © 2019 Association for Computational Linguistics",Final,,
Weber T.,A Philological Perspective on Meta-scientific Knowledge Graphs,1,1,-1,#excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090093271&doi=10.1007%2f978-3-030-55814-7_19&partnerID=40&md5=3f4d2aef0a3fc9fb6633180ad0c66968,"This paper discusses knowledge graphs and networks on the scientific process from a philological viewpoint. Relevant themes are: the smallest entities of scientific discourse; the treatment of documents or artefacts as texts and commentaries in discourse; and links between context, (co)text, and data points. As an illustration of the micro-level approach, version control of linguistic examples is discussed as a possible field of application in this discipline. This underlines the claim for data points to be treated like unique entities, which possess metadata of the datum and any text generating knowledge from it. © 2020, Springer Nature Switzerland AG.",Final,,
Nebhi K.,A rule-based relation extraction system using DBpedia and syntactic parsing,1,-1,-1,#excluded,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102494696&partnerID=40&md5=2235e2f8d78d98c9ae8145a9d119736c,"In this paper, we present a rule-based relation extraction approach which uses DBpedia and linguistic information provided by the syntactic parser Fips. Our goal is twofold: (i) the morpho-syntactic patterns are defined using the syntactic parser Fips to identify relations between named entities (ii) the RDF triples extracted from DBpedia are used to improve RE task by creating gazetteer relations.",Final,,
Zhang Z.; Yufan; Li B.; Zeng Z.Z.,Review of research on synonym equivalence relation mining,1,-1,-1,#related,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184654616&doi=10.1109%2fIEIR59294.2023.10391221&partnerID=40&md5=735bdf0bdadc4ce4bb4cd8d5135bb05d,"Synonym discovery, also known as synonym relation mining or synonym extraction, aims to identify and establish synonymous relationships between words, phrases, or sentences. The primary objective of this relationship mining is to enhance the performance of natural language processing (NLP) tasks, such as information retrieval, question-answering systems, text summarization, and machine translation. Considering that there are still numerous areas and issues awaiting further research in synonym relation mining, this paper provides a comprehensive review of the research methods employed in this field over the past two decades. The review is organized into four main categories. The first category explores the u e of language models for synonym extraction, including techniques such as word embeddings and the recent BERT model. The second category focuses on computing semantic similarity in synonym semantic spaces. The third category examines neural network-based approaches for synonym relation mining. Lastly, the fourth category delves into constructing synonym relationships based on knowledge graphs. Additionally, this paper provides an outlook and summary of potential future developments in this direction, with the aim of offering valuable guidance for future research in the field.  © 2023 IEEE.",Final,,
Park S.; Shim H.; Lee G.G.,ISOFT at QALD-4: Semantic similarity-based question answering system over linked data,1,-1,-1,#excluded,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961289185&partnerID=40&md5=c383cd424bb80b9855cb185024e51f2a,"We present a question answering system over linked data. We use natural language processing tools to extract slots and SPARQL templates from the question. Then, we use semantic similarity to map a natural language question to a SPARQL query. We combine important words to avoid loss of meaning, and compare combined words with uniform resource identifiers (URIs) from a knowledgebase (KB). This process is more powerful than comparing each word individually. Using our method, the problem of mapping a phrase of a user question to URIs from a KB can be more easily solved than without our method; this method improves the F-measure of the system.",Final,,
Zloch M.; Acosta M.; Hienert D.; Conrad S.; Dietze S.,Charaterizing RDF graphs through graph-based measures - Framework and assessment,1,-1,-1,#related,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113925797&doi=10.3233%2fSW-200409&partnerID=40&md5=8a539180b654048818b928276b1d1bca,"The topological structure of RDF graphs inherently differs from other types of graphs, like social graphs, due to the pervasive existence of hierarchical relations (TBox), which complement transversal relations (ABox). Graph measures capture such particularities through descriptive statistics. Besides the classical set of measures established in the field of network analysis, such as size and volume of the graph or the type of degree distribution of its vertices, there has been some effort to define measures that capture some of the aforementioned particularities RDF graphs adhere to. However, some of them are redundant, computationally expensive, and not meaningful enough to describe RDF graphs. In particular, it is not clear which of them are efficient metrics to capture specific distinguishing characteristics of datasets in different knowledge domains (e.g., Cross Domain vs. Linguistics). In this work, we address the problem of identifying a minimal set of measures that is efficient, essential (non-redundant), and meaningful. Based on 54 measures and a sample of 280 graphs of nine knowledge domains from the Linked Open Data Cloud, we identify an essential set of 13 measures, having the capacity to describe graphs concisely. These measures have the capacity to present the topological structures and differences of datasets in established knowledge domains. © 2021 - The authors. Published by IOS Press.",Final,All Open Access; Bronze Open Access; Green Open Access,
Gómez A.,Linguistic linked data: Paving the way towards maximising (Re)usability of linguistic resources,1,-1,-1,#related,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964890766&partnerID=40&md5=71922cbb60c25db3c4e8974f2fa79e41,[No abstract available],Final,,
Patraşcu M.I.; Haja G.; Clim M.R.; Tamba E.,Romanian dictionaries. Projects of digitization and linked data,1,-1,-1,#excluded #notavailable,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964022185&doi=10.1007%2f978-3-319-32942-0_8&partnerID=40&md5=d71a3288cb1a1ad09ac93a40a080908f,"In the context of globalization and of interest for linked data, Romanian lexicography tries to harmonize to this trends by aligning its resources and adapting to the necessities of a diversity of users. The lexicographic tradition of the Romanian language passed through various periods, from glosses and small bilingual dictionaries, written in Slavonic alphabet (17th–19th century), to scholar dictionaries from the 20th century, written in Latin alphabet. This tradition was highlighted by different projects, some of them presented in this article, and these projects will continue to emphasize the Romanian language features in order to make accessible the Romanian language for the users and to offer the public research materials and resources of the Romanian culture. © Springer International Publishing Switzerland 2016.",Final,,
McCrae J.P.,Linked data for lexicons (Invited tutorial),1,-1,-1,#excluded #notavailable,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964039339&partnerID=40&md5=6d8a146e51470caa340c92d862028118,"Lexicons form a crucial part of how we build natural language systems that allow humans to interact with machines and to build web applications that can use web standards such as OWL but express them in natural language, we developed a vocabulary called lemon (Lexicon Model for Ontologies). This tutorial details the model and enables participants to apply it in line with common patterns of usage. © Springer International Publishing Switzerland 2016.",Final,,
Ide N.,Annotations as linked data – interoperability (Invited tutorial),1,-1,-1,#excluded #tutorial,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964053076&partnerID=40&md5=74680de0ee792fbfba39187bbf2fb86e,"This tutorial considers Semantic Web representations of linguistically-annotated corpora and related resources — in particular, ontological data — specifically from the perspective of interoperability. © Springer International Publishing Switzerland 2016.",Final,,
Gómez-Pérez A.; Gracia J.; Vila-Suero D.,Multilingual linked data generation from language resources (Invited tutorial),1,-1,-1,#excluded #tutorial,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964060201&partnerID=40&md5=b961de65722066ee7137d3df1d5140e0,"This document briefly describes the tutorial on multilingual linked data generation from language resources imparted at EUROLAN’15. In such course, a theoretical overview of linguistic linked data was given, followed by a hands-on session that covered the main steps and aspects in the lifecycle of linguistic linked data generation and publication. The focus was very practical and the participants had the opportunity of generating linked data of some sample resources by themselves. © Springer International Publishing Switzerland 2016.",Final,,
Declerck T.,Building ontolex resources using protégé (Invited tutorial),1,-1,-1,#excluded #tutorial,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964066428&partnerID=40&md5=1d49d491a980db0772d8944ef46cb064,"In this tutorial the aim was to introduce both to the use of the Protégé ontology editor and to the building of lexical resources on the basis of an ontological model for such resources, the Ontolex model, which has been developed in the context of a W3C Community Group. The overall goal was to show how lexical data can be encoded in such a way that they can be published in the emerging Linguistic Linked (Open) Data. © Springer International Publishing Switzerland 2016.",Final,,
McCrae J.P.; Moran S.; Hellmann S.; Brümmer M.,Multilingual linked data,1,-1,-1,#related,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929314245&doi=10.3233%2fSW-150178&partnerID=40&md5=09615eb60517073ee3e7a2dadd1b178a,"The interaction of natural language processing and the Semantic Web have lead to the creation of a new paradigm known as Linguistic Linked Open Data (LLOD), whereby traditional language resources are made available as linked data. Conversely, the publication of corpora, machine-readable dictionaries as linked data has opened new resources to Semantic Web researchers and allowed new tools to be developed. In this special issue, we present recent development of tools and resources for creating and publishing language resources as linked data and tools to exploit this data to enable a multilingual Semantic Web. © 2015 - IOS Press and the authors. All rights reserved.",Final,All Open Access; Bronze Open Access,
A. Hassan I.; Ojo A.; Porwol L.,A lexical resource for identifying public services names on the social web,1,,-1,#notavailable #excluded,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960414008&doi=10.1007%2f978-3-319-27237-5_14&partnerID=40&md5=347831e00599e9a27681c492d9aef37d,"Discovery of government-related resources on the social web through mentions of government-related terms requires domain-specific lexical resources. This chapter describes an approach for developing a Lexical Resource for Public Services Names and how it could be exploited. Central to our technical approach is the development of a Semantic Alignment Algorithm, which organizes a set of public service names automatically captured from government websites in a semantic network based on a semantic relatedness measure (Explicit Semantic Analysis—ESA). To demonstrate the use of the developed lexicon, we: (1) clustered the United Kingdom and Irish Government public services catalogue for easier access to related services on citizens portals and (2) developed a Named Entity Recognizer (NER) to identify mentions of public service related information in a twitter stream. Evaluation of the semantic relations in the developed lexical resource computed by our semantic alignment algorithm showed the accuracy (specifically the F-Score ranged from 0.65 to 0.93. © Springer International Publishing Switzerland 2015.",Final,,
Bretschneider C.; Oberkampf H.; Zillner S.,UIMA2LOD: Integrating UIMA text annotations into the linked open data cloud,1,-1,-1,#excluded #notavailable,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951828238&doi=10.1007%2f978-3-319-24543-0_2&partnerID=40&md5=1d1ffe005801345da14770b89aedfb6e,"The LOD cloud is becoming the de-facto standard for sharing and connecting pieces of data, information and knowledge on the Web. As of today, means for the seamless integration of structured data into the LOD cloud are available. However, algorithms for integrating information enclosed in unstructured text sources are missing. In order to foster the (re)use of the high percentage of unstructured text, automatic means for the integration of their content are needed. We address this issue by proposing an approach for conceptual representation of textual annotations which distinguishes linguistic from semantic annotations and their integration. Additionally, we implement a generic UIMA pipeline that automatically creates a LOD graph from texts that (1) implements the proposed conceptual representation, (2) extracts semantically classified entities, (3) links to existing LOD datasets and (4) generates RDF graphs from the extracted information. We show the application and benefits of the approach in a case study on a medical corpus. © Springer International Publishing Switzerland 2015.",Final,,
Costa T.; Leal J.P.,Reducing large semantic graphs to improve semantic relatedness,1,-1,-1,#excluded lack of linguistic,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952662774&doi=10.1007%2f978-3-319-27653-3_23&partnerID=40&md5=0781900743552cc4a1a615c5f31f4226,"In the previous research the authors developed a family of semantic measures that are adaptable to any semantic graph, being automatically tuned with a set of parameters. The research presented in this paper extends this approach by also tuning the graph. This graph reduction procedure starts with a disconnected graph and incrementally adds edge types, until the quality of the semantic measure cannot be further improved. The validation performed used the three most recent versions of WordNet and, in most cases, this approach improves the quality of the semantic measure. © Springer International Publishing Switzerland 2015.",Final,All Open Access; Green Open Access,
Declerck T.,Harmonizing Lexical Data for their Linking to Knowledge Objects in the Linked Data Framework,1,-1,-1,#related,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123847447&partnerID=40&md5=391046e533952cd6d855ec4aa1fa2120,"In this position paper we discuss some of the experiences we made in describing lexical data using representation formalisms that are compatible for the publication of such data in the Linked Data framework. While we see a huge potential in the emerging Linguistic Linked Open Data, also supporting the publication of less-resourced language data on the same platform as for mainstream languages, we are wondering if, parallel to the widening of linking language data to both other language data and encyclopaedic knowledge present in the Linked Data cloud, it would not be beneficial to give more focus more on harmonization and merging of RDF encoded lexical data, instead of establishing links between such resources in the Linked Data. © COLING 2014. All rights reserved.",Final,,
Asprino L.; Presutti V.,Observing LOD: Its Knowledge Domains and the Varying Behavior of Ontologies Across Them,1,-1,-1,#related,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149402038&doi=10.1109%2fACCESS.2023.3250105&partnerID=40&md5=67157753284e85cc6828165806c03600,"Linked Open Data (LOD) is the largest, collaborative, distributed, and publicly-accessible Knowledge Graph (KG) uniformly encoded in the Resource Description Framework (RDF) and formally represented according to the semantics of the Web Ontology Language (OWL). LOD provides researchers with a unique opportunity to study knowledge engineering as an empirical science: to observe existing modelling practices and possibly understanding how to improve knowledge engineering methodologies and knowledge representation formalisms. Following this perspective, several studies have analysed LOD to identify (mis-)use of OWL constructs or other modelling phenomena e.g. class or property usage, their alignment, the average depth of taxonomies. A question that remains open is whether there is a relation between observed modelling practices and knowledge domains (natural science, linguistics, etc.): do certain practices or phenomena change as the knowledge domain varies? Answering this question requires an assessment of the domains covered by LOD as well as a classification of its datasets. Existing approaches to classify LOD datasets provide partial and unaligned views, posing additional challenges. In this paper, we introduce a classification of knowledge domains, and a method for classifying LOD datasets and ontologies based on it. We classify a large portion of LOD and investigate whether a set of observed phenomena have a domain-specific character. © 2013 IEEE.",Final,All Open Access; Gold Open Access; Green Open Access,
Khan A.F.; Chiarcos C.; Declerck T.; Gifu D.; García E.G.-B.; Gracia J.; Ionov M.; Labropoulou P.; Mambrini F.; Mccrae J.P.; Pagé-Perron É.; Passarotti M.; Muñoz S.R.; Truica C.-O.,When linguistics meets web technologies. Recent advances in modelling linguistic linked data,1,-1,-1,#related,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140849273&doi=10.3233%2fSW-222859&partnerID=40&md5=60240ef879df5b5e8305eddddd7f9621,"This article provides a comprehensive and up-to-date survey of models and vocabularies for creating linguistic linked data (LLD) focusing on the latest developments in the area and both building upon and complementing previous works covering similar territory. The article begins with an overview of some recent trends which have had a significant impact on linked data models and vocabularies. Next, we give a general overview of existing vocabularies and models for different categories of LLD resource. After which we look at some of the latest developments in community standards and initiatives including descriptions of recent work on the OntoLex-Lemon model, a survey of recent initiatives in linguistic annotation and LLD, and a discussion of the LLD metadata vocabularies META-SHARE and lime. In the next part of the paper, we focus on the influence of projects on LLD models and vocabularies, starting with a general survey of relevant projects, before dedicating individual sections to a number of recent projects and their impact on LLD vocabularies and models. Finally, in the conclusion, we look ahead at some future challenges for LLD models and vocabularies. The appendix to the paper consists of a brief introduction to the OntoLex-Lemon model.  © 2022 - The authors. Published by IOS Press.",Final,All Open Access; Bronze Open Access; Green Open Access,
Bosque-Gil J.; Cimiano P.; Dojchinovski M.,Editorial of the Special Issue on Latest Advancements in Linguistic Linked Data,1,-1,-1,#related,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140851009&doi=10.3233%2fSW-223251&partnerID=40&md5=9939198c383a2828a82cfb1436f7133e,"Since the inception of the Open Linguistics Working Group in 2010, there have been numerous efforts in transforming language resources into Linked Data. The research field of Linguistic Linked Data (LLD) has gained in importance, visibility and impact, with the Linguistic Linked Open Data (LLOD) cloud gathering nowadays over 200 resources. With this increasing growth, new challenges have emerged concerning particular domain and task applications, quality dimensions, and linguistic features to take into account. This special issue aims to review and summarize the progress and status of LLD research in recent years, as well as to offer an understanding of the challenges ahead of the field for the years to come. The papers in this issue indicate that there are still aspects to address for a wider community adoption of LLD, as well as a lack of resources for specific tasks and (interdisciplinary) domains. Likewise, the integration of LLD resources into Natural Language Processing (NLP) architectures and the search for long-term infrastructure solutions to host LLD resources continue to be essential points to which to attend in the foreseeable future of the research line.  © 2022 - The authors. Published by IOS Press.",Final,All Open Access; Bronze Open Access; Green Open Access,
Deshpande A.; Ruiter D.; Mosbach M.; Klakow D.,StereoKG: Data-Driven Knowledge Graph Construction for Cultural Knowledge and Stereotypes,1,-1,-1,#excluded #outofscope,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139104669&partnerID=40&md5=7440e0daa331119fc6732f0e01996c97,"Analyzing ethnic or religious bias is important for improving fairness, accountability, and transparency of natural language processing models. However, many techniques rely on human-compiled lists of bias terms, which are expensive to create and are limited in coverage. In this study, we present a fully data-driven pipeline for generating a knowledge graph (KG) of cultural knowledge and stereotypes. Our resulting KG covers 5 religious groups and 5 nationalities and can easily be extended to include more entities. Our human evaluation shows that the majority (59.2%) of non-singleton entries are coherent and complete stereotypes. We further show that performing intermediate masked language model training on the verbalized KG leads to a higher level of cultural awareness in the model and has the potential to increase classification performance on knowledge-crucial samples on a related task, i.e., hate speech detection. © 2022 Association for Computational Linguistics.",Final,,
Martín-Chozas P.; Montiel-Ponsoda E.; Carvalho S.; Costa R.,TermTrends: Trends in Terminology Generation and Modelling,1,-1,-1,#tutorial #excluded,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142452288&partnerID=40&md5=2d89c78c1d8f85671b0041affd5343a1,"This document presents the objectives, content and organisation of the TermTrends tutorial within EKAW 2022. The tutorial intends to give an overview of current techniques and tools for terminology generation, as well as of standardisation approaches for terminological data. Thus, the first part of the tutorial is a theoretical block that includes an introduction to the terminological work, current standards for terminology modelling and two use cases on legal and medical terminology. The second part is a hands-on block that deals with terminological resources and tools. The tutorial is linked to the conference through a series of topics, such as knowledge acquisition and ontology engineering; and it is suitable for both an expert and non-expert audience. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0)",Final,,
Declerck T.; Gracia J.; McCrae J.P.,"COST Action ""European network for Web-centred linguistic data science"" (NexusLinguarum); [Acción COST ""Red europea para la ciencia de datos lingüísticos centrada en la web"" (NexusLinguarum)]",1,-1,-1,#related,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100976108&doi=10.26342%2f2020-65-11&partnerID=40&md5=1452cb92162078615384e2f7b8d42a15,"We present the current state of the large ""European network for Web-centred linguistic data science"". In its first phase, the network has put in place several working groups to deal with specific topics. The network also already implemented a first round of Short Term Scientific Missions (STSM). © 2020 Sociedad Espanola para el Procesamiento del Lenguaje Natural. All rights reserved.",Final,,
Cimiano P.; Chiarcos C.; McCrae J.P.; Gracia J.,"Linguistic Linked Data: Representation, Generation and Applications",1,-1,-1,#related,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115737502&doi=10.1007%2f978-3-030-30225-2&partnerID=40&md5=9490e5cabe8ef19303537976b029bd3b,"This is the first monograph on the emerging area of linguistic linked data. Presenting a combination of background information on linguistic linked data and concrete implementation advice, it introduces and discusses the main benefits of applying linked data (LD) principles to the representation and publication of linguistic resources, arguing that LD does not look at a single resource in isolation but seeks to create a large network of resources that can be used together and uniformly, and so making more of the single resource. The book describes how the LD principles can be applied to modelling language resources. The first part provides the foundation for understanding the remainder of the book, introducing the data models, ontology and query languages used as the basis of the Semantic Web and LD and offering a more detailed overview of the Linguistic Linked Data Cloud. The second part of the book focuses on modelling language resources using LD principles, describing how to model lexical resources using Ontolex-lemon, the lexicon model for ontologies, and how to annotate and address elements of text represented in RDF. It also demonstrates how to model annotations, and how to capture the metadata of language resources. Further, it includes a chapter on representing linguistic categories. In the third part of the book, the authors describe how language resources can be transformed into LD and how links can be inferred and added to the data to increase connectivity and linking between different datasets. They also discuss using LD resources for natural language processing. The last part describes concrete applications of the technologies: representing and linking multilingual WordNets, applications in digital humanities and the discovery of language resources. Given its scope, the book is relevant for researchers and graduate students interested in topics at the crossroads of natural language processing/computational linguistics and the Semantic Web/linked data. It appeals to Semantic Web experts who are not proficient in applying the Semantic Web and LD principles to linguistic data, as well as to computational linguists who are used to working with lexical and linguistic resources wanting to learn about a new paradigm for modelling, publishing and exploiting linguistic resources. © Springer Nature Switzerland AG 2020.",Final,,
Declerck T.; McCrae J.; Hartung M.; Gracia J.; Chiarcos C.; Montiel E.; Cimiano P.; Revenko A.; Sauri R.; Lee D.; Racioppa S.; Nasir J.; Orlikowski M.; Lanau-Coronas M.; Fäth C.; Rico M.; Elahi M.F.; Khvalchik M.; Gonzalez M.; Cooney K.,Recent developments for the linguistic linked open data infrastructure,1,-1,-1,#related,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096555711&partnerID=40&md5=ec6a9696a5ce1ca3bf58021ca1a6086d,"In this paper we describe the contributions made by the European H2020 project “Prêt-à-LLOD” ('Ready-to-use Multilingual Linked Language Data for Knowledge Services across Sectors') to the further development of the Linguistic Linked Open Data (LLOD) infrastructure. Prêt-à-LLOD aims to develop a new methodology for building data value chains applicable to a wide range of sectors and applications and based around language resources and language technologies that can be integrated by means of semantic technologies. We describe the methods implemented for increasing the number of language data sets in the LLOD. We also present the approach for ensuring interoperability and for porting LLOD data sets and services to other infrastructures, as well as the contribution of the projects to existing standards. © European Language Resources Association (ELRA), licensed under CC-BY-NC",Final,,
Faralli S.; Finocchi I.; Ponzetto S.P.; Velardi P.,WebIsAGraph: A very large hypernymy graph from a web corpus,1,,-1,#outofscope #excluded,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074808871&partnerID=40&md5=eca0a652e88be374a3995f78c37590b0,"In this paper, we present WebIsAGraph, a very large hypernymy graph compiled from a dataset of is-a relationships extracted from the CommonCrawl. We provide the resource together with a Neo4j plugin to enable efficient searching and querying over such large graph. We use WebIsAGraph to study the problem of detecting polysemous terms in a noisy terminological knowledge graph, thus quantifying the degree of polysemy of terms found in is-a extractions from Web text. Copyright © 2019 for this paper by its authors.",Final,,
Grazioso M.; Cera V.; Di Maro M.; Origlia A.; Cutugno F.,From linguistic linked open data to multimodal natural interaction: A case study,1,,-1,#outofscope #excluded,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060155802&doi=10.1109%2fiV.2018.00060&partnerID=40&md5=73aebaecd4858dc7be9e53707492a0cc,"We present here the conversion of Linguistic Linked Open Data into Semantic Maps to be used to produce contents in a set of technological applications for Cultural Heritage. The paper describes the architectural data collection and annotation procedure adopted in the Cultural Heritage Orienting Multimodal Experiences (CHROME) project (PRIN 2015 funded by Italian University and Research Ministry). Such data will be used in Multimodal Dialogue Systems to obtain precise information about Architectural Heritage, by means of pointing gestures or verbal requests. In particular, we design conversational agents accessing fine-detailed semantic data linked to available 3D models of historical buildings. The starting point of our scientific approach is the Getty Vocabulary on Art & Architecture Thesaurus, integrated with the Getty Thesaurus of Geographic Names (TGN) and the Union List of Artist Names (ULAN). These data are related to 3D mesh of the considered buildings in order to associate abstract concepts to architectural elements. In the field of 3D architectural investigation, a significant amount of research has been conducted to allow domain experts to represent semantic data while keeping spatial references. We will discuss how this will make it possible to support multimodal user interaction and generate Cultural Heritage presentations. © 2018 IEEE.",Final,,
Lin K.; Du W.; Wang X.; Wang M.; Yang Z.,How to enhance Chinese word segmentation using knowledge graphs,1,,-1,#excluded #outofscope,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055572191&doi=10.1109%2fICCSE.2018.8468759&partnerID=40&md5=90ea9acebbb9534d188abc5013b20850,"Chinese word segmentation is a very important problem for Chinese information processing. Chinese word segmentation results are the basis for computers to understand natural language. However, unlike most Western languages, Chinese words do not have fixed symbols like white space as word segmentation marks. Moreover, Chinese has a very complex grammar, and the word segmentation criteria are varied with the contexts. Therefore, Chinese word segmentation is a very difficult task. Many existing works have proposed many algorithms to solve this problem. However, to our best knowledge, none of them could outperform all the other methods. In this paper, we develop a novel algorithm based on semantics and contexts. We propose a semantic-based word similarity measure using the concept hierarchy in knowledge graphs, and use this measure to prune the different results which are generated by several state-of-the-art Chinese word segmentation methods. The idea is to respectively compute the concept similarity of these words to other words in the text, and choose the word with the highest concept similarity score. To evaluate the effectiveness of the proposed approach, we conduct a series of experiment on two real datasets. The results show that our method outperforms all the state-of-the-art algorithms by filtering out wrong results and retaining correct ones. © 2018 IEEE.",Final,,
Wang C.; Ma X.; Chen J.; Chen J.,Information extraction and knowledge graph construction from geoscience literature,1,,-1,#outofscope #excluded,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039732196&doi=10.1016%2fj.cageo.2017.12.007&partnerID=40&md5=036ffb8dd33000dbb8efc6fd3ba9afa3,"Geoscience literature published online is an important part of open data, and brings both challenges and opportunities for data analysis. Compared with studies of numerical geoscience data, there are limited works on information extraction and knowledge discovery from textual geoscience data. This paper presents a workflow and a few empirical case studies for that topic, with a focus on documents written in Chinese. First, we set up a hybrid corpus combining the generic and geology terms from geology dictionaries to train Chinese word segmentation rules of the Conditional Random Fields model. Second, we used the word segmentation rules to parse documents into individual words, and removed the stop-words from the segmentation results to get a corpus constituted of content-words. Third, we used a statistical method to analyze the semantic links between content-words, and we selected the chord and bigram graphs to visualize the content-words and their links as nodes and edges in a knowledge graph, respectively. The resulting graph presents a clear overview of key information in an unstructured document. This study proves the usefulness of the designed workflow, and shows the potential of leveraging natural language processing and knowledge graph technologies for geoscience. © 2017 Elsevier Ltd",Final,All Open Access; Bronze Open Access,
Bosque-Gil J.; Gracia J.; Montiel-Ponsoda E.; Gómez-Pérez A.,Models to represent linguistic linked data,1,,-1,#related,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054913966&doi=10.1017%2fS1351324918000347&partnerID=40&md5=b5cebc1af75f655aaff483150cec76fd,"As the interest of the Semantic Web and computational linguistics communities in linguistic linked data (LLD) keeps increasing and the number of contributions that dwell on LLD rapidly grows, scholars (and linguists in particular) interested in the development of LLD resources sometimes find it difficult to determine which mechanism is suitable for their needs and which challenges have already been addressed. This review seeks to present the state of the art on the models, ontologies and their extensions to represent language resources as LLD by focusing on the nature of the linguistic content they aim to encode. Four basic groups of models are distinguished in this work: models to represent the main elements of lexical resources (group 1), vocabularies developed as extensions to models in group 1 and ontologies that provide more granularity on specific levels of linguistic analysis (group 2), catalogues of linguistic data categories (group 3) and other models such as corpora models or service-oriented ones (group 4). Contributions encompassed in these four groups are described, highlighting their reuse by the community and the modelling challenges that are still to be faced. Copyright © Cambridge University Press 2018.",Final,All Open Access; Green Open Access,
Tamper M.; Leskinen P.; Apajalahti K.; Hyvönen E.,Using Biographical Texts as Linked Data for Prosopographical Research and Applications,1,,-1,#outofscope #excluded,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055421322&doi=10.1007%2f978-3-030-01762-0_11&partnerID=40&md5=0ed343b00888b403c6d13c74a49eab45,"This paper argues that representing texts as semantic Linked Data provides a useful basis for analyzing their contents in Digital Humanities research and for Cultural Heritage application development. The idea is to transform Cultural Heritage texts into a knowledge graph and a Linked Data service that can be used flexibly in different applications via a SPARQL endpoint. The argument is discussed and evaluated in the context of biographical and prosopographical research and a case study where over 13 000 life stories form biographical collections of Biographical Centre of the Finnish Literature Society were transformed into RDF, enriched by data linking, and published in a SPARQL endpoint. Tools for biography and prosopography, data clustering, network analysis, and linguistic analysis were created with promising first results. © 2018, Springer Nature Switzerland AG.",Final,All Open Access; Green Open Access,
Bhowmik R.; De Melo G.,Generating fine-grained open vocabulary entity type descriptions,1,,-1,#excluded #outofscope,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063099681&doi=10.18653%2fv1%2fp18-1081&partnerID=40&md5=15fca4c5111b0b729e1f41e051c865a6,"While large-scale knowledge graphs provide vast amounts of structured facts about entities, a short textual description can often be useful to succinctly characterize an entity and its type. Unfortunately, many knowledge graph entities lack such textual descriptions. In this paper, we introduce a dynamic memory-based network that generates a short open vocabulary description of an entity by jointly leveraging induced fact embeddings as well as the dynamic context of the generated sequence of words. We demonstrate the ability of our architecture to discern relevant information for more accurate generation of type description by pitting the system against several strong baselines. © 2018 Association for Computational Linguistics",Final,All Open Access; Green Open Access; Hybrid Gold Open Access,
Krek S.; Kosem I.; McCrae J.P.; Navigli R.; Pedersen B.S.; Tiberius C.; Wissik T.,European lexicographic infrastructure (ELEXIS),1,,-1,#excluded #outofscope,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059436497&partnerID=40&md5=67932db39b0565fc3f18228fe8d6fa0d,"In the paper we describe a new EU infrastructure project dedicated to lexicography. The project is part of the Horizon 2020 program, with a duration of four years (2018-2022). The result of the project will be an infrastructure which will (1) enable efficient access to high quality lexicographic data, and (2) bridge the gap between more advanced and less-resourced scholarly communities working on lexicographic resources. One of the main issues addressed by the project is the fact that current lexicographic resources have different levels of (incompatible) structuring, and are not equally suitable for application in in Natural Language Processing and other fields. The project will therefore develop strategies, tools and standards for extracting, structuring and linking lexicographic resources to enable their inclusion in Linked Open Data and the Semantic Web, as well as their use in the context of digital humanities. © Lexicography in Global Contexts.",Final,,
Abián D.; Guerra F.; Martínez-Romanos J.; Trillo-Lado R.,Wikidata and DBpedia: A Comparative Study,1,,-1,#related,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045296691&doi=10.1007%2f978-3-319-74497-1_14&partnerID=40&md5=9b47e08635e69788aa40eb404f57694a,"DBpedia and Wikidata are two online projects focused on offering structured data from Wikipedia in order to ease its exploitation on the Linked Data Web. In this paper, a comparison of these two widely-used structured data sources is presented. This comparison considers the most relevant data quality dimensions in the state of the art of the scientific research. As fundamental differences between both projects, we can highlight that Wikidata has an open centralised nature, whereas DBpedia is more popular in the Semantic Web and the Linked Open Data communities and depends on the different linguistic editions of Wikipedia. © Springer International Publishing AG 2018.",Final,,
Rouces J.; De Melo G.; Hose K.,Addressing structural and linguistic heterogeneity in the Web,1,,-1,#related,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059701206&doi=10.3233%2fAIC-170745&partnerID=40&md5=6358c1b60ba67b3e0f6dbecd04747d5a,"An increasing number of structured knowledge bases have become available on the Web, enabling many new forms of analyses and applications. However, the fact that the data is being published by different parties with different vocabularies and ontologies means that there is a high degree of heterogeneity and no common schema. At the same time, the abundance of different human languages across unstructured data presents a similar problem, because most text mining tools only cater to the English language. This paper presents solutions for these two kinds of heterogeneity. It introduces Klint, aWeb-based system that automatically creates mappings to transform knowledge from heterogeneous sources into FrameBase, which is a broad linked data schema that enables the representation of a wide range of knowledge. With Klint, a user can review and edit the mappings with a streamlined interface, which in turn allows for human-level accuracy with minimum human effort. The paper further describes how FrameBase can be extended to support multilingual labels, which can aid in extending current tools for integrating English text into FrameBase knowledge. © 2018 Wolters Kluwer Medknow Publications. All rights reserved.",Final,All Open Access; Green Open Access,
Soares R.; Edelstein E.; Pan J.Z.; Wyner A.,Knowledge driven intelligent survey systems for linguists,1,,-1,#related,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057271563&doi=10.1007%2f978-3-030-04284-4_1&partnerID=40&md5=f5905e9fe73b222826b1f08a9b247892,"In this paper, we propose Knowledge Graph (KG), an articulated underlying semantic structure, to be a semantic bridge between human and systems. To illustrate our proposal, we focus on KG based intelligent survey systems. In state of the art systems, knowledge is hard-coded or implicit in these systems, making it hard for researchers to reuse, customise, link, or transmit the structured knowledge. Furthermore, such systems do not facilitate dynamic interaction based on the semantic structure. We design and implement a knowledge-driven intelligent survey system which is based on knowledge graph, a widely used technology that facilitates sharing and querying hypotheses, survey content, results, and analyses. The approach is developed, implemented, and tested in the field of Linguistics. Syntacticians and morphologists develop theories of grammar of natural languages. To evaluate theories, they seek intuitive grammaticality (well-formedness) judgments from native speakers, which either support a theory or provide counter-evidence. Our preliminary experiments show that a knowledge graph based linguistic survey can provide more nuanced results than traditional document-based grammaticality judgment surveys by allowing for tagging and manipulation of specific linguistic variables. © Springer Nature Switzerland AG 2018.",Final,All Open Access; Green Open Access,
Casanovas P.; Rodríguez-Doncel V.; González-Conejero J.,The role of pragmatics in the web of data,1,,-1,#excluded #outofscope,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028543909&doi=10.1007%2f978-3-319-44601-1_12&partnerID=40&md5=3c501a8367f7f460f5e2f3324c496e69,"This chapter is an introduction to the Semantic Web, the Web of Data, regulatory models, and the law. It does not take anything for granted. The first part of the chapter describes the languages of the Semantic Web, and shows how the perspective of the Web of Services and Linked Data is related to the conditions under which services can be offered, managed and used. The Web has been massively populated with both data and services. Semantically structured data, the Linked Data Cloud, allows and fosters human-machine interaction. Linked Data aims at creating ecosystems to facilitate the browsing, discovery, exploitation and reuse of datasets for applications. Licensed Linked Data is offered along with information about the rights involved. Rights Expression Languages are able to regulate half-automatically the use and reuse of content. The second part of the chapter shows that the nature of law is experiencing a deep transformation in the cloud. What links the information flow, social intelligence, rights management, and modelling in the Web of Data is the pragmatic approach —what we call the pragmatic turn, i.e. the representation of users’ needs and contexts to facilitate the automated interactive and collective management of knowledge. Both ontology building and knowledge acquisition share this perspective. The Web of Data brings about new challenges on agency, knowledge, communication, and the coordination of actions. Institutions can regulate both human and machine behaviours within these new environments. Licensed Linked Data, Licensed Linguistic Linked Data, Right Expression Languages, Semantic Web Regulatory Models, Electronic Institutions, Artificial Socio-cognitive Systems are examples of regulatory and institutional design (Regulations by Design). Regulatory systems become more complex in the cloud, in order to be simpler. © Springer International Publishing Switzerland 2017.",Final,,
McCrae J.P.; Chiarcos C.; Bond F.; Cimiano P.; Declerck T.; De Melo G.; Gracia J.; Hellmann S.; Klimek B.; Moran S.; Osenova P.; Pareja-Lora A.; Pool J.,The open linguistics working group: Developing the linguistic linked open data cloud,1,,-1,#related,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034244297&partnerID=40&md5=79ee215efebe47247607e41d1aaee252,"The Open Linguistics Working Group (OWLG) brings together researchers from various fields of linguistics, natural language processing, and information technology to present and discuss principles, case studies, and best practices for representing, publishing and linking linguistic data collections. A major outcome of our work is the Linguistic Linked Open Data (LLOD) cloud, an LOD (sub-)cloud of linguistic resources, which covers various linguistic databases, lexicons, corpora, terminologies, and metadata repositories. We present and summarize five years of progress on the development of the cloud and of advancements in open data in linguistics, and we describe recent community activities. The paper aims to serve as a guideline to introduce and involve researchers with the community and more generally with Linguistic Linked Open Data.",Final,,
Gangemi A.; Alam M.; Presutti V.,Word frame disambiguation: Evaluating linguistic linked data on frame detection,1,,-1,#related,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992561536&partnerID=40&md5=d9169a3bca9dc7dd19252fb15937c39f,"The usefulness of FrameNet is affected by its limited coverage and non-standard semantics. This paper presents some strategies based on Linguistic Linked Open Data to fully exploit and broaden its coverage. These strategies lead to the creation of a novel resource, Framester, which serves as a hub between FrameNet, WordNet, VerbNet, BabelNet, DBpedia, Yago, DOLCE-Zero, as well as other resources. We also present a Word Frame Disambiguation, an application performing frame detection from text using Framester as a base. The results are comparable in precision to the state-of-the-art machine learning tool, but with a much higher coverage. © 2016, CEUR-WS. All rights reserved.",Final,,
Almeida B.; Roche C.; Costa R.,Terminology and ontology development in the domain of Islamic archaeology,1,,-1,#excluded #outofscope,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984796638&partnerID=40&md5=67bb192dcec02562a13487c7d536fc58,"This paper describes an example regarding the terminology of Islamic pottery artefacts in Portuguese and Spanish in the context of an ongoing Ph D project. The approach followed in this paper places knowledge representation at the core of terminology work. More specifically, the development of an ontology, i.e. a formal and computational conceptualisation, enables the integration of a multilingual termbase in the semantic web as linked data, targeted at experts and students of archaeology. This approach allows for the preservation of linguistic diversity, as reflected by the different linguistic practices engaged by Portuguese and Spanish archaeologists in scholarly communication.",Final,,
Di Nunzio G.M.,Digital geolinguistics: On the use of linked open data for data-level interoperability between geolinguistic resources,1,,-1,#outofscope #excluded,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923698965&partnerID=40&md5=3a4a5a8e6715bb11c5e12785e343f742,"The Open Language Archives Community which recently celebrated its first 10 years of activity, is a worldwide network dedicated to collecting information on language resources and developing standard protocols for interoperability. In this context, Linked Open Data paradigm is very promising, because it eases interoperability between different systems by allowing the definition of data-driven models and applications. In this talk, we give an overview of present geolinguistics projects and an approach which moves the focus from the systems handling the linguistic data to the data themselves. As a concrete example, we present a geolinguistic application build upon a real linguistic dataset which provides linguists with a system for investigating variations among closely related languages. © 2013 for the individual papers by the papers' authors.",Final,,
Chiarcos C.; Hellmann S.; Nordhoff S.; Moran S.; Littauer R.; Eckle-Kohler J.; Gurevych I.; Hartmann S.; Matuschek M.; Meyer C.M.,The open linguistics working group,1,,-1,#related,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929349760&partnerID=40&md5=23a55468023dc095631d0483075e7543,"This paper describes the Open Linguistics Working Group (OWLG) of the Open Knowledge Foundation (OKFN). The OWLG is an initiative concerned with linguistic data by scholars from diverse fields, including linguistics, NLP, and information science. The primary goal of the working group is to promote the idea of open linguistic resources, to develop means for their representation and to encourage the exchange of ideas across different disciplines. This paper summarizes the progress of the working group, goals that have been identified, problems that we are going to address, and recent activities and ongoing developments. Here, we put particular emphasis on the development of a Linked Open Data (sub-)cloud of linguistic resources that is currently being pursued by several OWLG members.",Final,,
Chiarcos C.; Hellmann S.; Nordhoff S.,Towards a linguistic linked open data cloud: The open linguistics working group,1,,-1,#related,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869199991&partnerID=40&md5=16b47f25f046793e265102f1832e1432,"The Open Linguistics Working Group (OWLG) is an initiative of experts from different fields concerned with linguistic data, including academic linguistics (e.g. typology, corpus linguistics), applied linguistics (e.g. computational linguistics, lexicography and language documentation), and NLP (e.g. from the Semantic Web community). The primary goals of the working group are 1) promoting the idea of open linguistic resources, 2) developing means for their representation, and 3) encouraging the exchange of ideas across different disciplines. To a certain extent, the activities of the Open Linguistics Working Group converge towards the creation of a Linguistic Linked Open Data cloud, which is a topic addressed from different angles by several members of the Working Group. In this article, some of these currently on-going activities are presented and described.",Final,,
Păiş V.-F.; Mititelu V.B.,Linguistic linked open data for speech processing,1,,-1,#related,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140660011&partnerID=40&md5=ff76df373fe23e6056d6db34aae6e4af,"Linguistic Linked Open Data is becoming an important format for language resources, due to its power to ensure resources' interoperability and integration, which makes them more easily findable, accessible, interoperable and retrievable (i.e., FAIR). Romanian has fragmentary support with respect to the development of language resources and processing tools. Thus, in an effort to promote the existing ones, their conversion to Linked Data is a natural step. We focus here on resources for speech processing and present the RoLEX lexicon and the RTASC corpus that are relevant for this task: the former is the largest lexicon with phonetic transcription for Romanian words and their inflected forms, while the latter is a bimodal corpus relevant for training a robot's speech component for a specifically designed microworld. Both are converted to the Linguistic Linked Open Data format, linked to other existing resources for Romanian and made available as downloadable files and for query by means of SPARQL language. Some scenarios for exploiting the power of Linked Data format of these resources are also described here. © 2022 Nova Science Publishers, Inc..",Final,,
Reformat M.Z.; Tan L.; D'Aniello G.; Gaeta M.,Emotion-based Analysis of Reviews using Knowledge Graph,1,,-1,#outofscope #excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182523199&doi=10.1109%2fWI-IAT59888.2023.00017&partnerID=40&md5=6657adddcf2169d3ec099ce179ce29d6,"This study introduces a novel approach to sentiment analysis, combining the finesse of Knowledge Graphs with the unique perspective offered by the SenticNet and the Hourglass Model of Emotions. We aim to interpret, quantify, and categorize emotional responses in various reviews and offer, at the same time, a more nuanced understanding of user emotions than traditional sentiment analysis methods. Our process begins with extracting emotional indicators from reviews by linking their descriptive words with states of different emotions and further expressing them using terms and phrases defined in the Hourglass Model of Emotions. All the data is incorporated into a Knowledge Graph, where each review is connected to all its related aspects, description words, and their synonyms and emotions. We enhance the results with linguistic terms describing emotions of identified aspects to learn more about the justification behind the determined sentiments of the reviews.  © 2023 IEEE.",Final,,
Edelstein E.; Pan J.Z.; Soares R.; Wyner A.,Knowledge-Driven Intelligent Survey Systems Towards Open Science,1,,-1,#outofscope #excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081887293&doi=10.1007%2fs00354-020-00087-y&partnerID=40&md5=7ae25110f5bd1e32da7860ca861f339d,"In this paper, we propose Knowledge Graph (KG), an articulated underlying semantic structure, as a semantic bridge between humans, systems, and scientific knowledge. To illustrate our proposal, we focus on KG-based intelligent survey systems. In state-of-the-art systems, information is hard-coded or implicit, making it hard for researchers to reuse, customise, link, or transmit structured knowledge. Furthermore, such systems do not facilitate dynamic interaction based on semantic structure. We design and implement a knowledge-driven intelligent survey system which is based on knowledge graph, a widely used technology that facilitates sharing and querying hypotheses, survey content, results, and analyses. The approach is developed, implemented, and tested in the field of Linguistics. Syntacticians and morphologists develop theories of grammar of natural languages. To evaluate theories, they seek intuitive grammaticality (well-formedness) judgments from native speakers, which either support hypotheses or provide counter-evidence. Our preliminary experiments show that a knowledge graph-based linguistic survey can provide more nuanced results than the traditional document-based grammaticality judgment surveys by allowing for tagging and manipulation of specific linguistic variables. © 2020, The Author(s).",Final,All Open Access; Green Open Access; Hybrid Gold Open Access,
Maynard D.; Bontcheva K.; Augenstein I.,Natural Language Processing for the Semantic Web,1,,-1,#not available #excluded,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048908627&doi=10.2200%2fS00741ED1V01Y201611WBE015&partnerID=40&md5=f32a59c28be5f2f2b831a74eb3e35da5,"This book introduces core natural language processing (NLP) technologies to non-experts in an easily accessible way, as a series of building blocks that lead the user to understand key technologies, why they are required, and how to integrate them into Semantic Web applications. Natural language processing and Semantic Web technologies have different, but complementary roles in data management. Combining these two technologies enables structured and unstructured data to merge seamlessly. Semantic Web technologies aim to convert unstructured data to meaningful representations, which benefit enormously from the use of NLP technologies, thereby enabling applications such as connecting text to Linked Open Data, connecting texts to each other, semantic searching, information visualization, and modeling of user behavior in online networks. The first half of this book describes the basic NLP processing tools: tokenization, part-of-speech tagging, and morphological analysis, in addition to the main tools required for an information extraction system (named entity recognition and relation extraction) which build on these components. The second half of the book explains how Semantic Web and NLP technologies can enhance each other, for example via semantic annotation, ontology linking, and population. These chapters also discuss sentiment analysis, a key component in making sense of textual data, and the difficulties of performing NLP on social media, as well as some proposed solutions. The book finishes by investigating some applications of these tools, focusing on semantic search and visualization, modeling user behavior, and an outlook on the future. © Copyright 2017 by Morgan & Claypool.",Final,All Open Access; Green Open Access,
Lašek I.; Vojtáš P.,Various approaches to text representation for named entity disambiguation,1,-1,-1,#excluded #outofscope encyclopedic rather than linguistic features,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882991679&doi=10.1108%2fIJWIS-05-2013-0016&partnerID=40&md5=cbe1c40e4a5d696a54734d0baf2c6e3c,"Purpose - The purpose of this paper is to focus on the problem of named entity disambiguation. The paper disambiguates named entities on a very detailed level. To each entity is assigned a concrete identifier of a corresponding Wikipedia article describing the entity. Design/methodology/approach - For such a fine-grained disambiguation a correct representation of the context is crucial. The authors compare various context representations: bag of words representation, linguistic representation and structured co-occurrence representation. Models for each representation are described and evaluated. They also investigate the possibilities of multilingual named entity disambiguation. Findings - Based on this evaluation, the structured co-occurrence representation provides the best disambiguation results. It showed up that this method could be successfully applied also on other languages, not only on English. Research limitations/implications - Despite its good results the structured co-occurrence context representation has several limitations. It trades precision for recall, which might not be desirable in some use cases. Also it is not able to disambiguate two different types of entities, which are mentioned under the same name in the same text. These limitations can be overcome by combination with other described methods. Practical implications - The authors provide a ready-made web service, which can be directly plugged in existing applications using a REST interface. Originality/value - The paper proposes a new approach to named entity disambiguation exploiting various context representation models (bag of words, linguistic and structural representation). The authors constructed a comprehensive dataset based on all English Wikipedia articles for named entity disambiguation. They evaluated and compared the individual context representation models on this dataset. They evaluate the support of multiple languages. Copyright © 2013 Emerald Group Publishing Limited. All rights reserved.",Final,,
Mimouni N.,Modeling legal documents as typed linked data for relational querying,1,-1,-1,#excluded #outofscope,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911941699&partnerID=40&md5=88489de0390e051945943395fa93f41c,"Access to legal knowledge is particularly challenging to information retrieval systems. Not only is legal knowledge usually expressed in linguistically complex forms, but it is also structurally sophisticated (e.g pieces of legislation applicable to a case, version in force of a legal document, other related sources). Modeling the collection of documents in such complex domains requires taking into account the semantic content of the documents as well as their relational structure since documents are usually related to each other by various types of links. In this paper we describe two approaches for modeling and querying a collection of interlinked legal documents. The first approach is based on Formal Concept Analysis and Relational Concept Analysis to model and query the collection of documents. The second approach uses semantic web techniques (RDF, OWL and SPARQL). Different types of relational queries are discussed.",Final,,
Huang Z.; Li Z.; Jiang H.; Cao T.; Lu H.; Yin B.; Subbian K.; Sun Y.; Wang W.,Multilingual Knowledge Graph Completion with Self-Supervised Adaptive Graph Alignment,1,,-1,#excluded content rather than linguistic features,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142893526&partnerID=40&md5=4ae7fe3d7d3b61eee6fea8f653a78a48,"Predicting missing facts in a knowledge graph (KG) is crucial as modern KGs are far from complete. Due to labor-intensive human labeling, this phenomenon deteriorates when handling knowledge represented in various languages. In this paper, we explore multilingual KG completion, which leverages limited seed alignment as a bridge, to embrace the collective knowledge from multiple languages. However, language alignment used in prior works is still not fully exploited: (1) alignment pairs are treated equally to maximally push parallel entities to be close, which ignores KG capacity inconsistency; (2) seed alignment is scarce and new alignment identification is usually in a noisily unsupervised manner. To tackle these issues, we propose a novel self-supervised adaptive graph alignment (SS-AGA) method. Specifically, SS-AGA fuses all KGs as a whole graph by regarding alignment as a new edge type. As such, information propagation and noise influence across KGs can be adaptively controlled via relation-aware attention weights. Meanwhile, SS-AGA features a new pair generator that dynamically captures potential alignment pairs in a self-supervised paradigm. Extensive experiments on both the public multilingual DBPedia KG and newly-created industrial multilingual E-commerce KG empirically demonstrate the effectiveness of SS-AGA. © 2022 Association for Computational Linguistics.",Final,,
Baisa V.; El Maarouf I.; Rychlý P.; Rambousek A.,Software and data for Corpus Pattern Analysis,1,-1,-1,#excluded #outscope,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013663967&partnerID=40&md5=06de7a846fc4e6389fdf3f462d911d91,"This report describes the tools and resources developed to support Corpus Pattern Analysis (CPA)-a corpus-based method for building patterns dictionaries. The tools are an annotation of concordance in Sketch Engine, a special CPA editor for editing Pattern Dictionary of English Verbs (PDEV), dedicated servlets based on the Dictionary Editing and Browsing platform and a public interface for browsing the PDEV. The resources are SemEval 2015 Task 15 dataset and LEMON API. © Tribun EU 2015.",Final,,
McCrae J.P.; Cimiano P.,Linghub: A linked data based portal supporting the discovery of language resources,1,1,-1,#related,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954569919&partnerID=40&md5=7ed5f6e247231bc3636337434087b8ff,"Language resources are an essential component of any natural language processing system and such systems can only be applied to new languages and domains if appropriate resources can be found. Currently the task of finding new language resources for a particular task or application is complicated by the fact that records about such resources are stored in different repositories with different models, different quality and search mechanisms. To remedy this situation, we present Linghub, a new portal that aggregates and indexes data from a range of sources and repositories and applied the Linked Data Principles to expose all the metadata under a common interface. Furthermore, we use faceted browsing and SPARQL queries to show how this can help to answer real user problems extracted from a mailing list for linguists. © Copyright 2015 for the individual papers by the papers' authors.",Final,,
Semedo D.,Towards Open-domain Vision and Language Understanding with Wikimedia,1,-1,-1,#excluded #outofscope encyclopedic rather than linguistic features,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107652202&doi=10.1145%2f3442442.3452346&partnerID=40&md5=797b040c8eb054bed1ec67608ea6b968,"Current state-of-The-Art task-Agnostic visio-linguistic approaches, such as ViLBERT [2], are limited to domains in which texts have a visual materialization (e.g. a person running). This work describes a project towards achieving the next generation of models, that can deal with open-domain media, and learn visio-linguistic representations that reflect data's context, by jointly reasoning over media, a domain knowledge-graph and temporal context. This ambition will be leveraged by a Wikimedia data framework, comprised by comprehensive and high-quality data, covering a wide range of social, cultural, political and other type of events. Towards this goal, we propose a research setup comprised by an open-domain data framework and a set of novel independent research tasks. © 2021 ACM.",Final,All Open Access; Bronze Open Access,
Baker T.,"Libraries, languages of description, and linked data: A Dublin Core perspective",1,,-1,#related,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857719886&doi=10.1108%2f07378831211213256&partnerID=40&md5=d117b210adcdfbb3b25c7a078f5949be,"Purpose: Library-world ""languages of description"" are increasingly being expressed using the resource description framework (RDF) for compatibility with linked data approaches. This article aims to look at how issues around the Dublin Core, a small ""metadata element set,"" exemplify issues that must be resolved in order to ensure that library data meet traditional standards for quality and consistency while remaining broadly interoperable with other data sources in the linked data environment. Design/methodology/approach: The article focuses on how the Dublin Core - originally seen, in traditional terms, as a simple record format - came increasingly to be seen as an RDF vocabulary for use in metadata based on a ""statement"" model, and how new approaches to metadata evolved to bridge the gap between these models. Findings: The translation of library standards into RDF involves the separation of languages of description, per se, from the specific data formats into which they have for so long been embedded. When defined with ""minimal ontological commitment,"" languages of description lend themselves to the sort of adaptation that is inevitably a part of any human linguistic activity. With description set profiles, the quality and consistency of data traditionally required for sharing records among libraries can be ensured by placing precise constraints on the content of data records - without compromising the interoperability of the underlying vocabularies in the wider linked data context. Practical implications: In today's environment, library data must continue to meet high standards of consistency and quality, yet it must be possible to link or merge the data with sources that follow other standards. Placing constraints on the data created, more than on the underlying vocabularies, allows both requirements to be met. Originality/value: This paper examines how issues around the Dublin Core exemplify issues that must be resolved to ensure library data meet quality and consistency standards while remaining interoperable with other data sources. © Emerald Group Publishing Limited.",Final,,
Hellmann S.; Brekle J.; Auer S.,Leveraging the crowdsourcing of lexical resources for bootstrapping a linguistic data cloud,1,,-1,#related #apporach #linguistics #survey,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892985119&doi=10.1007%2f978-3-642-37996-3_13&partnerID=40&md5=39e17920ff15f2ba08cc38a56108b12c,"We present a declarative approach implemented in a comprehensive open-source framework based on DBpedia to extract lexical-semantic resources - an ontology about language use - from Wiktionary. The data currently includes language, part of speech, senses, definitions, synonyms, translations and taxonomies (hyponyms, hyperonyms, synonyms, antonyms) for each lexical word. Main focus is on flexibility to the loose schema and configurability towards differing language-editions of Wiktionary. This is achieved by a declarative mediator/wrapper approach. The goal is to allow the addition of languages just by configuration without the need of programming, thus enabling the swift and resource-conserving adaption of wrappers by domain experts. The extracted data is as fine granular as the source data in Wiktionary and additionally follows the lemon model. It enables use cases like disambiguation or machine translation. By offering a linked data service, we hope to extend DBpedia's central role in the LOD infrastructure to the world of Open Linguistics. © Springer-Verlag 2013.",Final,All Open Access; Green Open Access,
Yu X.,Design of Modern English-Chinese Bilingual Interactive AI Teaching Platform for International Shipping Based on Structured Data Sources,1,0,-1,#excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184134659&doi=10.1109%2fECICE59523.2023.10383057&partnerID=40&md5=0827aadb1f1dc3ebefe971b6aea5f077,"Along with the rapid growth of the modern economy, China has emerged as the most influential shipping power in the world. In international shipping, English is the standard language and a key component of international trade and logistics activities. It is important to use modern technological tools that assist learners and practitioners in improving and mastering English in trading. The efficiency and quality of trade business strengthen international cooperation and competitiveness [1]. Computational linguistics, as a cross-disciplinary academic, needs to flourish and progress in the rapid development of information technology and globalization. However, due to the specificity of language, there is no institution to collect English and related words for the industry on a large scale and construct a knowledge base for efficient organizational learning and training. The interaction between the knowledge bases makes the industry's language information disclosed and circulated efficiently. This study was carried out to collect and analyze the semantics of the industry and provide shared retrieval through the construction of patterns of knowledge graphs and the use of modern artificial intelligence (AI) techniques. Relevant theories and technologies were integrated using computational linguistics for knowledge bases for trade and were constructed on an interactive teaching platform to provide a solid foundation for effective language learning.  © 2023 IEEE.",Final,,
Chen Y.-H.; Lu E.J.-L.; Lin S.-C.,Ontology-based Dynamic Semantic Annotation for Social Image Retrieval,1,0,0,#use #annotation automatic semantic image annotation model for social image retrieval,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090381498&doi=10.1109%2fMDM48529.2020.00074&partnerID=40&md5=0f913291ac5297a8ef08dcb0c1851b52,"To mitigate the semantic gap between a visual feature and linguistic representation in social image retrieval, researchers have proposed an automatic image annotation approach, which employs multiple visual features and text matching to label images. This paper used a linked open data approach and ontology to construct a model, an automatic semantic image annotation model for social image retrieval. These models enable users to label images through automatic semantic annotation and to identify the underlying intents of semantics, thereby fulfilling user needs and enhancing the retrieval accuracy. © 2020 IEEE.",Final,,
Bourgonje P.; Moreno-Schneider J.; Nehring J.; Rehm G.; Sasaki F.; Srivastava A.,Towards a platform for curation technologies: Enriching text collections with a semantic-web layer,1,,1,#use ,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994527341&doi=10.1007%2f978-3-319-47602-5_14&partnerID=40&md5=bd87b516c5ba0f26a86c73d8b6c06189,"In an attempt to put a Semantic Web-layer that provides linguistic analysis and discourse information on top of digital content, we develop a platform for digital curation technologies. The platform offers language-, knowledge-and data-aware services as a flexible set of workflows and pipelines for the efficient processing of various types of digital content. The platform is intended to enable human experts (knowledge workers) to get a grasp and understand the contents of large document collections in an efficient way so that they can curate, process and further analyse the collection according to their sector-specific needs. © Springer International Publishing AG 2016.",Final,,
Almas B.; Schroeder C.T.,Applying the canonical text services model to the Coptic SCRIPTORIUM,1,,-1,#excluded,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011848778&doi=10.5334%2fdsj-2016-013&partnerID=40&md5=a4ebc8858ba4118bf5cc67be0cee118e,"Coptic SCRIPTORIUM is a platform for interdisciplinary and computational research in Coptic texts and linguistics. The purpose of this project was to research and implement a system of stable identification for the texts and linguistic data objects in Coptic SCRIPTORIUM to facilitate their citation and reuse. We began the project with a preferred solution, the Canonical Text Services URN model, which we validated for suitability for the corpus and compared it to other approaches, including HTTP URLs and Handles. The process of applying the CTS model to Coptic SCRIPTORIUM required an in-depth analysis that took into account the domain-specific scholarly research and citation practices, the structure of the textual data, and the data management workflow. © 2016 The Author(s).",Final,All Open Access; Gold Open Access; Green Open Access,
Castro L.J.G.; Berlanga R.; Rebholz-Schuhmann D.; Garcia A.,Connections across scientific publications based on semantic annotations,1,,-1,#excluded,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924715587&partnerID=40&md5=624805330bb3d7298046d5d315a45158,"The core information from scientific publications is encoded in natural language text and monolithic documents; therefore it is not well integrated with other structured and unstructured data resources. The text format requires additional processing to semantically interlink the publications and to finally reach interoperability of contained data. Data infrastructures such as the Linked Open Data initiative based on the Resource Description Framework support the connectivity of data from scientific publications once the identification of concepts and relations has been achieved, and the content has been interconnected semantically. In this manuscript we produce and analyze the semantic annotations in scientific articles to investigate on the interconnectivity across the articles. In our initial experiment based on articles from PubMed Central we demonstrate the means and the results leading to the interconnectivity using annotations of Medical Subject Headings concepts, Unified Medical Language System terms, and semantic abstractions of relations. We conclude that the different methods would contribute to different types of relatedness between articles that could be later used in recommendation systems based on semantic links across a network of scientific publications.",Final,,
Goldsack T.; Zhang Z.; Tang C.; Scarton C.; Lin C.,Enhancing Biomedical Lay Summarisation with External Knowledge Graphs,1,,-1,#excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184809727&partnerID=40&md5=ceeb43a0ece58fae8778c8bea4243038,"Previous approaches for automatic lay summarisation are exclusively reliant on the source article that, given it is written for a technical audience (e.g., researchers), is unlikely to explicitly define all technical concepts or state all of the background information that is relevant for a lay audience. We address this issue by augmenting eLife, an existing biomedical lay summarisation dataset, with article-specific knowledge graphs, each containing detailed information on relevant biomedical concepts. Using both automatic and human evaluations, we systematically investigate the effectiveness of three different approaches for incorporating knowledge graphs within lay summarisation models, with each method targeting a distinct area of the encoder-decoder model architecture. Our results confirm that integrating graph-based domain knowledge can significantly benefit lay summarisation by substantially increasing the readability of generated text and improving the explanation of technical concepts. © 2023 Association for Computational Linguistics.",Final,,
Koner R.; Li H.; Hildebrandt M.; Das D.; Tresp V.; Günnemann S.,Graphhopper: Multi-hop Scene Graph Reasoning for Visual Question Answering,1,,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116858375&doi=10.1007%2f978-3-030-88361-4_7&partnerID=40&md5=f0f5cc665dc9f46cc1c8a83af67176b1,"Visual Question Answering (VQA) is concerned with answering free-form questions about an image. Since it requires a deep semantic and linguistic understanding of the question and the ability to associate it with various objects that are present in the image, it is an ambitious task and requires multi-modal reasoning from both computer vision and natural language processing. We propose Graphhopper, a novel method that approaches the task by integrating knowledge graph reasoning; computer vision, and natural language processing techniques. Concretely, our method is based on performing context-driven, sequential reasoning based on the scene entities and their semantic and spatial relationships. As a first step, we derive a scene graph that describes the objects in the image, as well as their attributes and their mutual relationships. Subsequently, a reinforcement learning agent is trained to autonomously navigate in a multi-hop manner over the extracted scene graph to generate reasoning paths, which are the basis for deriving answers. We conduct an experimental study on the challenging dataset GQA, based on both manually curated and automatically generated scene graphs. Our results show that we keep up with human performance on manually curated scene graphs. Moreover, we find that Graphhopper outperforms another state-of-the-art scene graph reasoning model on both manually curated and automatically generated scene graphs by a significant margin. © 2021, The Author(s).",Final,All Open Access; Green Open Access; Hybrid Gold Open Access,
Kilicoglu H.; Rosemblat G.; Fiszman M.; Shin D.,Broad-coverage biomedical relation extraction with SemRep,1,,-1,#excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084825129&doi=10.1186%2fs12859-020-3517-7&partnerID=40&md5=b969041f5ce9b844abcf2a6eeade2b36,"Background: In the era of information overload, natural language processing (NLP) techniques are increasingly needed to support advanced biomedical information management and discovery applications. In this paper, we present an in-depth description of SemRep, an NLP system that extracts semantic relations from PubMed abstracts using linguistic principles and UMLS domain knowledge. We also evaluate SemRep on two datasets. In one evaluation, we use a manually annotated test collection and perform a comprehensive error analysis. In another evaluation, we assess SemRep's performance on the CDR dataset, a standard benchmark corpus annotated with causal chemical-disease relationships. Results: A strict evaluation of SemRep on our manually annotated dataset yields 0.55 precision, 0.34 recall, and 0.42 F 1 score. A relaxed evaluation, which more accurately characterizes SemRep performance, yields 0.69 precision, 0.42 recall, and 0.52 F 1 score. An error analysis reveals named entity recognition/normalization as the largest source of errors (26.9%), followed by argument identification (14%) and trigger detection errors (12.5%). The evaluation on the CDR corpus yields 0.90 precision, 0.24 recall, and 0.38 F 1 score. The recall and the F 1 score increase to 0.35 and 0.50, respectively, when the evaluation on this corpus is limited to sentence-bound relationships, which represents a fairer evaluation, as SemRep operates at the sentence level. Conclusions: SemRep is a broad-coverage, interpretable, strong baseline system for extracting semantic relations from biomedical text. It also underpins SemMedDB, a literature-scale knowledge graph based on semantic relations. Through SemMedDB, SemRep has had significant impact in the scientific community, supporting a variety of clinical and translational applications, including clinical decision making, medical diagnosis, drug repurposing, literature-based discovery and hypothesis generation, and contributing to improved health outcomes. In ongoing development, we are redesigning SemRep to increase its modularity and flexibility, and addressing weaknesses identified in the error analysis. © 2020 The Author(s).",Final,All Open Access; Gold Open Access; Green Open Access,
Ferré S.; Cellier P.,Graph-FCA: An extension of formal concept analysis to knowledge graphs,1,1,-1,#excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064636404&doi=10.1016%2fj.dam.2019.03.003&partnerID=40&md5=6162f1034398f67beb18f823e9251361,"Knowledge graphs offer a versatile knowledge representation, and have been studied under different forms, such as conceptual graphs or RDF graphs in the Semantic Web. A challenge is to discover conceptual structures in those graphs, in the same way as Formal Concept Analysis (FCA) discovers conceptual structures in tables. FCA has been successful for analysing, mining, learning, and exploring tabular data, and our aim is to help transpose those results to graph-based data. Previous several FCA approaches have already addressed relational data, hence graphs, but with various limits. We propose Graph-FCA as an extension of FCA where a dataset is a hypergraph instead of a binary table. We show that it can be formalized simply by replacing objects by tuples of objects. This leads to the notion of “n-ary concept”, whose extent is an n-ary relation of objects, and whose intent is a “projected graph pattern”. In this paper, we formally reconstruct the fundamental results of FCA for knowledge graphs. We describe in detail the representation of hypergraphs, and the operations on them, as they are much more complex than the sets of attributes that they extend. We also propose an algorithm based on a notion of “pattern basis” to generate and display n-ary concepts in a more efficient and more compact way. We explore a few use cases, in order to study the feasibility and usefulness of Graph-FCA. We consider two use cases: workflow patterns in cooking recipes and linguistic structures from parse trees. In addition, we report on experiments about quantitative aspects of the approach. © 2019",Final,All Open Access; Bronze Open Access; Green Open Access,
Ferré S,A proposal for extending formal concept analysis to knowledge graphs,1,,-1,#excluded ,,,,,,X
Xing C.; Liu X.; Du D.; Hu W.; Zhang M.,Relation extraction using language model based on knowledge graph,1,,-1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096414931&doi=10.1088%2f1742-6596%2f1624%2f2%2f022037&partnerID=40&md5=8dabd909fb79c05a8f685bdafcfba007,"Relation extraction is an important task in natural language processing (NLP). The existing methods generally pay more attention on extracting textual semantic information from text, but ignore the relation contextual information from existed relations in datasets, which is very important for the performance of relation extraction task. In this paper, we represent each individual entity as a embedding based on entities and relations knowledge graph, which encodes the relation contextual information between the given entity pairs and relations. Besides, inspired by the impressive performance of language models recently, we used the language model to leverage word semantic information, in which word semantic information can be better captured than word embedding. The experimental results on SemEval2010 Task 8 dataset showed that the F1-score of our proposed method improved nearly 3% compared with the previous methods. © 2020 Institute of Physics Publishing. All rights reserved.",Final,All Open Access; Bronze Open Access,
Kumar S.; Ramaneswaran S.; Akhtar M.S.; Chakraborty T.,From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues,1,,1,#use #commonsensekg,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184822341&partnerID=40&md5=14584b69691f8ab92b34cc16279846d8,"Understanding emotions during conversation is a fundamental aspect of human communication, driving NLP research for Emotion Recognition in Conversation (ERC). While considerable research has focused on discerning emotions of individual speakers in monolingual dialogues, understanding the emotional dynamics in code-mixed conversations has received relatively less attention. This motivates our undertaking of ERC for code-mixed conversations in this study. Recognizing that emotional intelligence encompasses a comprehension of worldly knowledge, we propose an innovative approach that integrates commonsense information with dialogue context to facilitate a deeper understanding of emotions. To achieve this, we devise an efficient pipeline that extracts relevant commonsense from existing knowledge graphs based on the code-mixed input. Subsequently, we develop an advanced fusion technique that seamlessly combines the acquired commonsense information with the dialogue representation obtained from a dedicated dialogue understanding module. Our comprehensive experimentation showcases the substantial performance improvement obtained through the systematic incorporation of commonsense in ERC. Both quantitative assessments and qualitative analyses further corroborate the validity of our hypothesis, reaffirming the pivotal role of commonsense integration in enhancing ERC. © 2023 Association for Computational Linguistics.",Final,,
Joshi M.; Sawant U.; Chakrabarti S.,Knowledge graph and corpus driven segmentation and answer inference for telegraphic entity-seeking queries,1,,-1,#excluded #entity-seeking telegraphic queries #freebase #not lingustic resources,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949831806&doi=10.3115%2fv1%2fd14-1117&partnerID=40&md5=1d7830f2fd1c75f92427a9941d39871b,"Much recent work focuses on formal interpretation of natural question utterances, with the goal of executing the resulting structured queries on knowledge graphs (KGs) such as Freebase. Here we address two limitations of this approach when applied to open-domain, entity-orientedWeb queries. First,Web queries are rarely wellformed questions. They are ""telegraphic"", with missing verbs, prepositions, clauses, case and phrase clues. Second, the KG is always incomplete, unable to directly answer many queries. We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment: A base entity e1, a relation type r, a target entity type t2, and contextual words s. The query seeks entity e2 ε t2 where r(e1, e2) holds, further evidenced by schema-agnostic words s. Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG. We do not trust the best or any specific query segmentation. Instead, evidence in favor of candidate e2s are aggregated across several segmentations. Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG, using over a thousand telegraphic queries adapted from TREC, INEX, and Web- Questions, show the efficacy of our approach. For one benchmark, MAP improves from 0.2-0.29 (competitive baselines) to 0.42 (our system). NDCG@10 improves from 0.29-0.36 to 0.54. © 2014 Association for Computational Linguistics.",Final,All Open Access; Bronze Open Access; Green Open Access,
Gu Y.; Yao S.; Gan C.; Tenenbaum J.B.; Yu M.,Revisiting the Roles of “Text” in Text Games,1,,-1,#excluded,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149869084&partnerID=40&md5=88b6ea6528904927ca47c5e3df454db6,"Text games present opportunities for natural language understanding (NLU) methods to tackle reinforcement learning (RL) challenges. However, recent work has questioned the necessity of NLU by showing random text hashes could perform decently. In this paper, we pursue a fine-grained investigation into the roles of text in the face of different RL challenges, and reconcile that semantic and non-semantic language representations could be complementary rather than contrasting. Concretely, we propose a simple scheme to extract relevant contextual information into an approximate state hash as extra input for an RNN-based text agent. Such a lightweight plug-in achieves competitive performance with state-of-the-art text agents using advanced NLU techniques such as knowledge graph and passage retrieval, suggesting non-NLU methods might suffice to tackle the challenge of partial observability. However, if we remove RNN encoders and use approximate or even ground-truth state hash alone, the model performs miserably, which confirms the importance of semantic function approximation to tackle the challenge of combinatorially large observation and action spaces. Our findings and analysis provide new insights for designing better text game task setups and agents. © 2022 Association for Computational Linguistics.",Final,,
Zhao Y.; Zhang J.; Zhou Y.; Zong C.,Knowledge graphs enhanced neural machine translation,1,,1,#use #translation,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097342403&partnerID=40&md5=0ae6d87f08dd49ee81d8742071255ca0,"Knowledge graphs (KGs) store much structured information on various entities, many of which are not covered by the parallel sentence pairs of neural machine translation (NMT). To improve the translation quality of these entities, in this paper we propose a novel KGs enhanced NMT method. Specifically, we first induce the new translation results of these entities by transforming the source and target KGs into a unified semantic space. We then generate adequate pseudo parallel sentence pairs that contain these induced entity pairs. Finally, NMT model is jointly trained by the original and pseudo sentence pairs. The extensive experiments on Chinese-to-English and English-to-Japanese translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling the induced entities. © 2020 Inst. Sci. inf., Univ. Defence in Belgrade. All rights reserved.",Final,,
Chen X.; Chen M.; Fan C.; Uppunda A.; Sun Y.; Zaniolo C.,Multilingual knowledge graph completion via ensemble knowledge transfer,1,,1,#use #completion,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098414071&partnerID=40&md5=16c05b1bd715d103286c9b4c277dc12a,"Predicting missing facts in a knowledge graph (KG) is a crucial task in knowledge base construction and reasoning; and it has been the subject of much research in recent works using KG embeddings. While existing KG embedding approaches mainly learn and predict facts within a single KG, a more plausible solution would benefit from the knowledge in multiple language-specific KGs, considering that different KGs have their own strengths and limitations on data quality and coverage. This is quite challenging, since the transfer of knowledge among multiple independently maintained KGs is often hindered by the insufficiency of alignment information and the inconsistency of described facts. In this paper, we propose KEnS, a novel framework for embedding learning and ensemble knowledge transfer across a number of language-specific KGs. KEnS embeds all KGs in a shared embedding space, where the association of entities is captured based on self-learning. Then, KEnS performs ensemble inference to combine prediction results from embeddings of multiple language-specific KGs, for which multiple ensemble techniques are investigated. Experiments on five real-world language-specific KGs show that KEnS consistently improves state-of-the-art methods on KG completion, via effectively identifying and leveraging complementary knowledge. © 2020 Association for Computational Linguistics",Final,,
Macketanz V.; Avramidis E.; Burchardt A.; Helcl J.; Srivastava A.,"Machine translation: Phrase-based, rule-based and neural approaches with linguistic evaluation",1,,-1,#excluded,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055563223&doi=10.1515%2fcait-2017-0014&partnerID=40&md5=b4fa85519fc7a94dca33c2c1b32423fd,"In this article we present a novel linguistically driven evaluation method and apply it to the main approaches of Machine Translation (Rule-based, Phrase-based, Neural) to gain insights into their strengths and weaknesses in much more detail than provided by current evaluation schemes. Translating between two languages requires substantial modelling of knowledge about the two languages, about translation, and about the world. Using English-German IT-domain translation as a case-study, we also enhance the Phrase-based system by exploiting parallel treebanks for syntax-aware phrase extraction and by interfacing with Linked Open Data (LOD) for extracting named entity translations in a post decoding framework.",Final,All Open Access; Gold Open Access; Green Open Access,
Tan Y.; Zhang X.; Chen Y.; Ali Z.; Hua Y.; Qi G.,CLRN: A reasoning network for multi-relation question answering over Cross-lingual Knowledge Graphs,1,,1,#use #cross-lingualkgs #questionanswering,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162144389&doi=10.1016%2fj.eswa.2023.120721&partnerID=40&md5=725cd1d23f8e203ad0f1238fbe3f76a0,"Cross-lingual Knowledge Graphs-based Question Answering (CLKGQA) requires the question answering (QA) system to combine the knowledge graphs (KGs) in different languages to obtain answers to input questions. In previous works, the common idea is to merge Cross-lingual Knowledge Graphs (CLKGs) into a single KG through aligned entity pairs and then treat it as a traditional KG-based QA. However, as demonstrated by Tan et al. (2023), existing Entity Alignment (EA) models cannot generate highly accurate aligned entity pairs for CLKGs. Therefore, two issues need to be addressed in the CLKGQA task: (1) Remove the dependency of the QA model on the fused KG; (2) Improve the performance of the EA model in obtaining aligned entity pairs from locally isomorphic CLKGs. To solve the above two issues, this paper presents Cross-lingual Reasoning Network (CLRN), a novel multi-hop QA model that allows switching knowledge graphs at any stage of the multi-hop reasoning. Furthermore, we establish an iterative framework that combines CLRN and EA model, in which CLRN is used for extracting potential alignment triple pairs from CLKGs during the QA process. The extracted triple pairs provide pseudo-aligned entities, and the additional aligned entity pairs are used to mine missing relations between entities in CLKGs. These pseudo-aligned entity pairs and relations improve the performance of the EA model, resulting in higher accuracy in QA. Extensive experiments demonstrate the effectiveness of the proposed model, which outperforms the baseline approaches. Through iterative enhancement, the performance of the EA model has also been improved by > 1.0 % in Hit@1 and Hit@10, and the improvement is statistically significant in the confidence interval of p<0.01. Moreover, our work discusses the correlation between QA and EA from the side of QA, which has reference value for the follow-up exploration of related communities. We have open-sourced our dataset and code, which is available at the URL https://github.com/tan92hl/Cross-lingual-Reasoning-Network-for-CLKGQA. © 2023 Elsevier Ltd",Final,,
Varshney D.; Zafar A.; Behera N.K.; Ekbal A.,Knowledge graph assisted end-to-end medical dialog generation,1,,-1,#excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151685702&doi=10.1016%2fj.artmed.2023.102535&partnerID=40&md5=5d9b3d397a451b33d684afc4c3ddb30c,"Medical dialog systems have the potential to assist e-medicine in improving access to healthcare services, improving patient treatment quality, and lowering medical expenses. In this research, we describe a knowledge-grounded conversation generation model that demonstrates how large-scale medical information in the form of knowledge graphs can aid in language comprehension and generation in medical dialog systems. Generic responses are often produced by existing generative dialog systems, resulting in monotonous and uninteresting conversations. To solve this problem, we combine various pre-trained language models with a medical knowledge base (UMLS) to generate clinically correct and human-like medical conversations using the recently released MedDialog-EN dataset. The medical-specific knowledge graph contains broadly 3 types of medical-related information, including disease, symptom and laboratory test. We perform reasoning over the retrieved knowledge graph by reading the triples in each graph using MedFact attention, which allows us to use semantic information from the graphs for better response generation. In order to preserve medical information, we employ a policy network, which effectively injects relevant entities associated with each dialog into the response. We also study how transfer learning can significantly improve the performance by utilizing a relatively small corpus, created by extending the recently released CovidDialog dataset, containing the dialogs for diseases that are symptoms of Covid-19. Empirical results on the MedDialog corpus and the extended CovidDialog dataset demonstrate that our proposed model significantly outperforms the state-of-the-art methods in terms of both automatic evaluation and human judgment. © 2023 Elsevier B.V.",Final,,
Sun W.; Li Y.; Yao J.; Wu Q.; Liu K.,Combining Structure Embedding and Text Semantics for Efficient Knowledge Graph Completion,1,,-1,#excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170076831&doi=10.18293%2fSEKE2023-100&partnerID=40&md5=e19bfda0c4056eb1fb642c35ddeff2ce,"Knowledge graph completion plays a crucial role in downstream applications. However, existing methods tend to only rely on the structure or textual information, resulting in suboptimal model performance. Moreover, recent attempts to leverage pre-trained language models to complete knowledge graphs have proved unsatisfactory. To overcome these limitations, we propose a novel model that combines structural embedding and semantic information of the knowledge graph. Compared with previous works based on pre-trained language models, our model can better use the implicit knowledge of pre-trained language models by using relation templates, entity definitions, and learnable tokens. Furthermore, our model employs a multi-head attention mechanism to transform the embedding semantic space of entities and relations obtained from the knowledge graph embedding model, thereby enhancing their expressiveness and unifying the semantic space of both types of information. Finally, we utilize convolutional neural networks to extract features from the matrices created by combining these two types of information for link prediction and triplet classification tasks. Empirical evaluations on two knowledge graph completion datasets demonstrate that our model is effective for both tasks. © 2023 Knowledge Systems Institute Graduate School. All rights reserved.",Final,All Open Access; Bronze Open Access,
Boukkouri H.E.; Ferret O.; Lavergne T.; Zweigenbaum P.,Specializing Static and Contextual Embeddings in the Medical Domain Using Knowledge Graphs: Let's Keep It Simple,1,,-1,#excluded,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154614219&partnerID=40&md5=21a721dcab51423456aa000d55e80aaf,"Domain adaptation of word embeddings has mainly been explored in the context of retraining general models on large specialized corpora. While this usually yields good results, we argue that knowledge graphs, which are used less frequently, could also be utilized to enhance existing representations with specialized knowledge. In this work, we aim to shed some light on whether such knowledge injection could be achieved using a basic set of tools: graph-level embeddings and concatenation. To that end, we adopt an incremental approach where we first demonstrate that static embeddings can indeed be improved through concatenation with in-domain node2vec representations. Then, we validate this approach on contextual models and generalize it further by proposing a variant of BERT that incorporates knowledge embeddings within its hidden states through the same process of concatenation. We show that this variant outperforms plain retraining on several specialized tasks, then discuss how this simple approach could be improved further. Both our code and pre-trained models are open-sourced for future research. In this work, we conduct experiments that target the medical domain and the English language.  © 2022 Association for Computational Linguistics.",Final,,
Goel S.; Gracia J.; Forcada M.L.,Bilingual dictionary generation and enrichment via graph exploration,1,,1,#kg apertium #use #synonyms,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140834359&doi=10.3233%2fSW-222899&partnerID=40&md5=dacaa9f99e46d8a0814518971cac32c2,"In recent years, we have witnessed a steady growth of linguistic information represented and exposed as linked data on the Web. Such linguistic linked data have stimulated the development and use of openly available linguistic knowledge graphs, as is the case with the Apertium RDF, a collection of interconnected bilingual dictionaries represented and accessible through Semantic Web standards. In this work, we explore techniques that exploit the graph nature of bilingual dictionaries to automatically infer new links (translations). We build upon a cycle density based method: partitioning the graph into biconnected components for a speed-up, and simplifying the pipeline through a careful structural analysis that reduces hyperparameter tuning requirements. We also analyse the shortcomings of traditional evaluation metrics used for translation inference and propose to complement them with new ones, both-word precision (BWP) and both-word recall (BWR), aimed at being more informative of algorithmic improvements. Over twenty-seven language pairs, our algorithm produces dictionaries about 70% the size of existing Apertium RDF dictionaries at a high BWP of 85% from scratch within a minute. Human evaluation shows that 78% of the additional translations generated for dictionary enrichment are correct as well. We further describe an interesting use-case: inferring synonyms within a single language, on which our initial human-based evaluation shows an average accuracy of 84%. We release our tool as free/open-source software which can not only be applied to RDF data and Apertium dictionaries, but is also easily usable for other formats and communities.  © 2022 - The authors. Published by IOS Press.",Final,All Open Access; Bronze Open Access; Green Open Access,
Liu Y.; Li L.; Zhang B.; Huang Q.,Think Beyond Words: Exploring Context-Relevant Visual Commonsense for Diverse Dialogue Generation,1,,-1,#excluded,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149885714&partnerID=40&md5=405a983b343341f586184d863d76d20b,"Commonsense knowledge has been widely considered for building intelligent open-domain dialogue agents, aiming to generate meaningful and diverse responses. Previous works in this field usually lack the ability to effectively obtain and utilize auxiliary commonsense from the external visual world. In this paper, we argue that exploiting logical information in images related to context can be effective to enrich and steer the generation process. In view of this, we propose VICTOR, a context-relevant VIsual Commonsense enhanced dialogue generaTOR for generating coherent and informative responses. To obtain the associated visual commonsense, we devise a novel approach that expands topic words on the knowledge graph and maps them into daily scenarios. During the generation, the model adopts multimodal fusion mechanism to integrate visual and textual information, and adaptively combine their decoding distributions for better response generation. The experimental results on two public datasets show that our proposed method outperforms the latest competitive methods in terms of coherence and diversity. © 2022 Association for Computational Linguistics.",Final,,
Liu L.; Li X.; He R.; Bing L.; Joty S.; Si L.,Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples,1,,1,#use,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143087240&partnerID=40&md5=23c56bd34f15c1847bdc19f5bba9c742,"Knowledge-enhanced language representation learning has shown promising results across various knowledge-intensive NLP tasks. However, prior methods are limited in efficient utilization of multilingual knowledge graph (KG) data for language model (LM) pretraining. They often train LMs with KGs in indirect ways, relying on extra entity/relation embeddings to facilitate knowledge injection. In this work, we explore methods to make better use of the multilingual annotation and language agnostic property of KG triples, and present novel knowledge based multilingual language models (KMLMs) trained directly on the knowledge triples. We first generate a large amount of multilingual synthetic sentences using the Wikidata KG triples. Then based on the intra- and inter-sentence structures of the generated data, we design pretraining tasks to enable the LMs to not only memorize the factual knowledge but also learn useful logical patterns. Our pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive crosslingual tasks, including named entity recognition (NER), factual knowledge retrieval, relation classification, and a newly designed logical reasoning task. © 2022 Association for Computational Linguistics.",Final,,
Tian J.; Li X.; Qiang C.,Cross-lingual Knowledge Graph Alignment via Neighborhood Reconstruction Network,1,,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125856136&doi=10.1145%2f3488933.3489028&partnerID=40&md5=d5074a6db72a845fcb4d264e1948128d,"Structural heterogeneity between knowledge graphs is an outstanding challenge for entity alignment. Most of the previous cross-lingual knowledge graph alignment studies rely on entity information only from monolingual mapping information, which may fail at matching entities that have different structure information in two KGs. In this paper, we propose the Neighborhood Reconstruction Network (NRNet), a novel entity alignment framework for tackling the structural heterogeneity challenge. NRNet first expand their one-hop neighbors based on other KGs to reduce the difference between neighbor structures. Next, it calculates the mutual information between entities to capture both the topological structure and the neighborhood difference. We further used a solution based on graph attention to obtain graph-level matching vectors. Experiments on the benchmark dataset show that the effectiveness of NRNet by detailed ablation studies and analysis.  © 2021 ACM.",Final,,
Liwicki M.; Forcher B.; Jaeger P.; Dengel A.,Koios++: A query-answering system for handwritten input,1,,-1,#excluded,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862081741&doi=10.1109%2fDAS.2012.48&partnerID=40&md5=2ad0680782e09a36ac8fabe870d374a3,"In this paper we propose KOIOS++, which automatically processes natural language queries provided by handwritten input. The system integrates several recent achievements in the area of handwriting recognition, natural language processing, information retrieval, and human computer interaction. It uses a knowledge base described by the resource description framework (RDF). Our generic approach first generates a lexicon as background information for the handwritten text recognition. After recognizing a handwritten query, several output hypotheses are sent to a natural language processing system in order to generate a structured query (SPARQL query). Subsequently, the query is applied to the given knowledge base and a result graph visualizes the retrieved information. At all stages, the user can easily adjust the intermediate results if there is any undesired outcome. The system is implemented as a web-service and therefore works for handwritten input on digital paper as well as on input on Pen-enabled interactive surfaces. Furthermore, we build on the generic RDF-representation of semantic knowledge which is also used by the linked open data (LOD) initiative. As such, our system works well in various scenarios. We have implemented prototypes for querying company knowledge bases, the DBPedia1, the DBLP computer science bibliography2, and a knowledge base of the DAS 2012. © 2012 IEEE.",Final,,
Menéndez A.A.; Gayo J.E.L.,SCOWT: Semantic competencies for worker training,1,,-1,#excluded #notavailable,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865102309&partnerID=40&md5=e68ca7aacefedfb898dc17d43287f4af,"Organizations globalization is becoming more and more usual in IT sector. Currently, teams composed by members from different countries and collaborating with other foreign teams is a growing tendency. However, this multicultural working environment carries remarkable problems due to linguistic and cultural issues. These problems arise not only in software development stages but also in information exchange between organization departments. The Human Resources (HR) department is one of those departments that suffer from ambiguous vocabulary while understanding concepts like, e.g, ""competence"" or ""skill"". We propose a Competence Management Ontology (calledSCOWT), inspired by Semantic Web technologies, Linked Data principles and the European e-Competence Framework for ICT professional competencies definition. This ontology has been designed as the key component for the development of a Competence Management System. Using ontologies will raise the current level of semantic interoperability of data exchange between HR departments and could improve efficiency of HR activities, specifically the training activities design in a global IT organization. © 2011 IADIS.",Final,,
Melnyk I.; Dognin P.; Das P.,Knowledge Graph Generation From Text,1,,-1,#excluded,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149903796&partnerID=40&md5=56beffdd18ab7b82da1a15471af9dea5,"In this work we propose a novel end-to-end multi-stage Knowledge Graph (KG) generation system from textual inputs, separating the overall process into two stages. The graph nodes are generated first using pretrained language model, followed by a simple edge construction head, enabling efficient KG extraction from the text. For each stage we consider several architectural choices that can be used depending on the available training resources. We evaluated the model on a recent WebNLG 2020 Challenge dataset, matching the state-of-the-art performance on text-to-RDF generation task, as well as on New York Times (NYT) and a large-scale TEKGEN datasets, showing strong overall performance, outperforming the existing baselines. We believe that the proposed system can serve as a viable KG construction alternative to the existing linearization or sampling-based graph generation approaches. © 2022 Association for Computational Linguistics.",Final,,
Stanković R.; Krstev C.; Todorović B.Š.; Vitas D.; Škorić M.; Nešić M.I.,Distant Reading in Digital Humanities: Case Study on the Serbian Part of the ELTeC Collection,1,,1,#kg #serbian,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144424875&partnerID=40&md5=20e877c23a939d7ac524675188fe815d,"In this paper we present the Serbian part of the ELTeC multilingual corpus of novels written in the time period 1840-1920. The corpus is being built in order to test various distant reading methods and tools with the aim of re-thinking the European literary history. We present the various steps that led to the production of the Serbian sub-collection: the novel selection and retrieval, text preparation, structural annotation, POS-tagging, lemmatization and named entity recognition. The Serbian sub-collection was published on different platforms in order to make it freely available to various users. Several use examples show that this sub-collection is useful for both close and distant reading approaches. © European Language Resources Association (ELRA), licensed under CC-BY-NC-4.0.",Final,,
Conia S.; Minhas U.F.; Li M.; Ilyas I.; Lee D.; Li Y.,Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs,1,,-1,#excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184824790&partnerID=40&md5=d35b9b6be4c24654571ed83b5f9fc1ef,"Recent work in Natural Language Processing and Computer Vision has been using textual information - e.g., entity names and descriptions - available in knowledge graphs to ground neural models to high-quality structured data. However, when it comes to non-English languages, the quantity and quality of textual information are comparatively scarce. To address this issue, we introduce the novel task of automatic Knowledge Graph Enhancement (KGE) and perform a thorough investigation on bridging the gap in both the quantity and quality of textual information between English and non-English languages. More specifically, we: i) bring to light the problem of increasing multilingual coverage and precision of entity names and descriptions in Wikidata; ii) demonstrate that state-of-the-art methods, namely, Machine Translation (MT), Web Search (WS), and Large Language Models (LLMs), struggle with this task; iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and LLMs to generate high-quality textual information; and, iv) study the impact of increasing multilingual coverage and precision of non-English textual information in Entity Linking, Knowledge Graph Completion, and Question Answering. As part of our effort towards better multilingual knowledge graphs, we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE approaches in 10 languages across 7 language families. ©2023 Association for Computational Linguistics.",Final,,
Zhu J.; Zhang M.; Yang H.; Peng S.; Wu Z.; Jiang Y.; Qiu X.; Pan W.; Zhu M.; Ma M.; Zhang W.,KG-IQES: An Interpretable Quality Estimation System for Machine Translation Based on Knowledge Graph,1,,-1,#excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185221581&partnerID=40&md5=3fa50e9d20769a445bfe42de12d15f64,"The widespread use of machine translation (MT) has driven the need for effective automatic quality estimation (AQE) methods. How to enhance the interpretability of MT output quality estimation is well worth exploring in the industry. From the perspective of the alignment of named entities (NEs) in the source and translated sentences, we construct a multilingual knowledge graph (KG) consisting of domain-specific NEs, and design a KG-based interpretable quality estimation (QE) system for machine translations (KG-IQES). KG-IQES effectively estimates the translation quality without relying on reference translations. Its effectiveness has been verified in our business scenarios. © 2023 The authors. This article is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0)",Final,,
Wang C.; Guo S.; He Z.; Xue Z.; Chen Y.; Liu K.; Zhao J.,"CogNet2: A Multi-Level Frame Organized Knowledge Base Integrating Linguistic, World and Commonsense Knowledge",1,,1,#kg,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184369774&partnerID=40&md5=2d9e69dc3544c90879a226ce0a69ddf2,"In this paper, we present CogNet2, an extension of the CogNet knowledge base, which combines the significant events form Wikidata, entities from YAGO4 and commonsense assertions from ATOMIC. It aims to unify knowledge of multiple levels of granularity. To efficiently integrate significant event and entity knowledge into CogNet, we construct significant event- and entity-centric frames, and then link them to the CogNet by automated labeling and crowd-sourced annotation. To enrich CogNet with more commonsense knowledge in social interaction, we construct frames with element restriction for fine-grained typical situations and integrate commonsense assertions about them. As a result, in comparison with CogNet1, CogNet2 increases 800+ new frames of significant events and entities, 30000+ new fine-grained frames with element restrictions, more than 204K new commonsense assertions. The scale of frame instances is up to 33.4M in total. © 2023 Copyright for this paper by its authors.",Final,,
Verlinden S.; Zaporojets K.; Deleu J.; Demeester T.; Develder C.,Injecting Knowledge Base Information into End-to-End Joint Entity and Relation Extraction and Coreference Resolution,1,,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118062880&partnerID=40&md5=431b08e936cac6de3ccf9b884a2d115d,"We consider a joint information extraction (IE) model, solving named entity recognition, coreference resolution and relation extraction jointly over the whole document. In particular, we study how to inject information from a knowledge base (KB) in such IE model, based on unsupervised entity linking. The used KB entity representations are learned from either (i) hyperlinked text documents (Wikipedia), or (ii) a knowledge graph (Wikidata), and appear complementary in raising IE performance. Representations of corresponding entity linking (EL) candidates are added to text span representations of the input document, and we experiment with (i) taking a weighted average of the EL candidate representations based on their prior (in Wikipedia), and (ii) using an attention scheme over the EL candidate list. Results demonstrate an increase of up to 5% F1-score for the evaluated IE tasks on two datasets. Despite a strong performance of the prior-based model, our quantitative and qualitative analysis reveals the advantage of using the attention-based approach. © 2021 Association for Computational Linguistics",Final,,
Zhou Y.; Geng X.; Shen T.; Zhang W.; Jiang D.,Improving Zero-Shot Cross-lingual Transfer for Multilingual Question Answering over Knowledge Graph,1,,1,#use #question-anserwing,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113823264&partnerID=40&md5=927041cb034315cb207fa63a6422b3d5,"Multilingual question answering over knowledge graph (KGQA) aims to derive answers from a knowledge graph (KG) for questions in multiple languages. To be widely applicable, we focus on its zero-shot transfer setting. That is, we can only access training data in a high-resource language, while need to answer multilingual questions without any labeled data in target languages. A straightforward approach is resorting to pre-trained multilingual models (e.g., mBERT) for cross-lingual transfer, but there is a still significant gap of KGQA performance between source and target languages. In this paper, we exploit unsupervised bilingual lexicon induction (BLI) to map training questions in source language into those in target language as augmented training data, which circumvents language inconsistency between training and inference. Furthermore, we propose an adversarial learning strategy to alleviate syntax-disorder of the augmented data, making the model incline to both language- and syntax-independence. Consequently, our model narrows the gap in zero-shot cross-lingual transfer. Experiments on two multilingual KGQA datasets with 11 zero-resource languages verify its effectiveness. © 2021 Association for Computational Linguistics.",Final,,
Recupero D.R.; Consoli S.; Gangemi A.; Nuzzolese A.G.; Presutti V.; Spampinato D.,Semantic web-based sentiment analysis,1,,1,#use sentiment analysis,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926391179&partnerID=40&md5=2348848a14158c7f8c3ec2bdd1146d5a,"The introduction of semantics in Sentiment Analysis research has proved to bring several benefits for what performances are concerned and has allowed to identify new challenging tasks to be accomplished. Semantics helps structuring the plain natural language text with formal representation. The current system we are developing performs sentiment analysis by hybridizing natural language processing techniques with Semantic Web technologies. Our system, called Sentilo, is able to recognize the holder of an opinion, to detect the topics and sub-topics in its scope, and to measure the sentiment expressed by them. This information is formally represented by means of RDF graphs according to an OWL opinion ontology, while holders and topics identity is resolved on the Linked Open Data cloud.",Final,,
Zhang M.; Liu L.; Zhao Y.; Qiao X.; Su C.; Zhao X.; Zhu J.; Zhu M.; Peng S.; Li Y.; Liu Y.; Ma W.; Piao M.; Tao S.; Yang H.; Jiang Y.,Leveraging Multilingual Knowledge Graph to Boost Domain-specific Entity Translation of ChatGPT,1,1,-1,#excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185220903&partnerID=40&md5=7a909295f31a1c5a3def5daf8b247840,"Recently, ChatGPT has shown promising results for Machine Translation (MT) in general domains and is becoming a new paradigm for translation. In this paper, we focus on how to apply ChatGPT to domain-specific translation and propose to leverage Multilingual Knowledge Graph (MKG) to help ChatGPT improve the domain entity translation quality. To achieve this, we extract the bilingual entity pairs from MKG for the domain entities that are recognized from source sentences. We then introduce these pairs into translation prompts, instructing ChatGPT to use the correct translations of the domain entities. To evaluate this novel MKG method for ChatGPT, we conduct comparative experiments on three Chinese-English (zh-en) test datasets constructed from three specific domains, of which one domain is from biomedical science, and the other two are from the Information and Communications Technology (ICT) industry - Visible Light Communication (VLC) and wireless domains. Experimental results show that both the overall translation quality of ChatGPT (+6.21, +3.13 and +11.25 in BLEU scores) and the translation accuracy of domain entities (+43.2%, +30.2% and +37.9% absolute points) are significantly improved with MKG on the three test datasets. © 2023 The authors. This article is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0)",Final,,
He Z.; Wang H.; Zhang X.,Multi-Task Learning Model Based on BERT and Knowledge Graph for Aspect-Based Sentiment Analysis,1,-1,,#excluded #noLLOD,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147807407&doi=10.3390%2felectronics12030737&partnerID=40&md5=2f299618c05df364f0255a10194383d2,"Aspect-based sentiment analysis (ABSA) aims to identify the sentiment of an aspect in a given sentence and thus can provide people with comprehensive information. However, many conventional methods need help to discover the linguistic knowledge implicit in sentences. Additionally, they are susceptible to unrelated words. To improve the performance of the model in the ABSA task, a multi-task sentiment analysis model based on Bidirectional Encoder Representation from Transformers (BERT) and a Knowledge Graph (SABKG) is proposed in this paper. Expressly, part-of-speech information is incorporated into the output representation of BERT, thereby obtaining textual semantic information through linguistic knowledge. It also enhances the textual representation to identify the aspect terms. Moreover, this paper constructs a knowledge graph of aspect and sentiment words. It uses a graph neural network to learn the embeddings in the triplet of “aspect word, sentiment polarity, sentiment word”. The constructed graph improves the contextual relationship between the text’s aspect and sentiment words. The experimental results on three open datasets show that the proposed model can achieve the most advanced performance compared with previous models. © 2023 by the authors.",Final,All Open Access; Gold Open Access,
Kumar N.; Schockaert S.,Solving Hard Analogy Questions with Relation Embedding Chains,1,1,-1,#excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184824370&partnerID=40&md5=e392314f6085d80c88e944f2001a7480,"Modelling how concepts are related is a central topic in Lexical Semantics. A common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to model the relation between two concepts as a set of paths. However, KGs are limited to a fixed set of relation types, and they are incomplete and often noisy. Another strategy is to distill relation embeddings from a fine-tuned language model. However, this is less suitable for words that are only indirectly related and it does not readily allow us to incorporate structured domain knowledge. In this paper, we aim to combine the best of both worlds. We model relations as paths but associate their edges with relation embeddings. The paths are obtained by first identifying suitable intermediate words and then selecting those words for which informative relation embeddings can be obtained. We empirically show that our proposed representations are useful for solving hard analogy questions. © 2023 Association for Computational Linguistics.",Final,,
MurugesanI K.; Atzeni M.; Kapanipathi P.; Talamadupula K.; Sachan M.; Campbell M.,Efficient Text-based Reinforcement Learning by Jointly Leveraging State and Commonsense Graph Representations,1,1,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118069672&partnerID=40&md5=8346294343e5d54db8862a4e75b05652,"Text-based games (TBGs) have emerged as useful benchmarks for evaluating progress at the intersection of grounded language understanding and reinforcement learning (RL). Recent work has proposed the use of external knowledge to improve the efficiency of RL agents for TBGs. In this paper, we posit that to act efficiently in TBGs, an agent must be able to track the state of the game while retrieving and using relevant commonsense knowledge. Thus, we propose an agent for TBGs that induces a graph representation of the game state and jointly grounds it with a graph of commonsense knowledge from ConceptNet. This combination is achieved through bidirectional knowledge graph attention between the two symbolic representations. We show that agents that incorporate commonsense into the game state graph outperform baseline agents. © 2021 Association for Computational Linguistics.",Final,,
Harrando I.; Troncy R.,Named Entity Recognition as Graph Classification,1,1,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115834933&doi=10.1007%2f978-3-030-80418-3_19&partnerID=40&md5=2f4be5d087d96ac2c7cbfa9dcf3601d6,"Injecting real-world information (typically contained in Knowledge Graphs) and human expertise into an end-to-end training pipeline for Natural Language Processing models is an open challenge. In this preliminary work, we propose to approach the task of Named Entity Recognition, which is traditionally viewed as a Sequence Labeling problem, as a Graph Classification problem, where every word is represented as a node in a graph. This allows to embed contextual information as well as other external knowledge relevant to each token, such as gazetteer mentions, morphological form, and linguistic tags. We experiment with a variety of graph modeling techniques to represent words, their contexts, and external knowledge, and we evaluate our approach on the standard CoNLL-2003 dataset. We obtained promising results when integrating external knowledge through the use of graph representation in comparison to the dominant end-to-end training paradigm. © 2021, Springer Nature Switzerland AG.",Final,,
Wang J.; Ouyang Z.; Gan J.,A method for constructing knowledge graph of ethnic cultural information resources,1,-1,-1,"#excluded potentially useful, but not resource is made available",2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092440802&doi=10.1109%2fCSEI50228.2020.9142525&partnerID=40&md5=2a9e0087a7f5396719a8e0e63e51501e,"In order to construct the knowledge graph of ethnic cultural information resources, we first use the Chinese word segmentation system and user-defined thesaurus to segment and part-of-speech tagging the data of ethnic cultural dictionary, and remove the punctuation. Then the text data is detected. If the number of continuous word segmentation is not less than the set threshold, then to perform manual word segmentation operation, and to add the result of manual word segmentation to the user-defined thesaurus of Chinese word segmentation system until there is no new word. Then we extract the attributes of the text data after the correct segmentation to build the domain knowledge graph. We detect the repeatability of the domain knowledge graph, delete the duplicate data, and store the knowledge graph. Finally, we link the stored domain knowledge graph with resources.  © 2020 IEEE.",Final,,
Ahmadnia B.,Linked data effectiveness in neural machine translation,1,1,-1,#excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123041684&doi=10.1145%2f3440084.3441214&partnerID=40&md5=8033e1fc47d630da29cdc4e58617f1f8,"Quality of data-driven Machine Translation (MT) systems depends on large volumes of data from which models can be constructed to leverage patterns and knowledge from these datasets. In corpus-based MT systems, Out-Of-Vocabulary (OOV) words and ambiguous translations are the most common sources of error. In this paper, JRC-Names and DBpedia have been employed as Linked Data (LD) to minimize the aforementioned types of errors on top of a Neural MT (NMT) model. Three strategies have been evaluated for exploiting knowledge from LD in translating named entities; 1) Dictionaries, 2) Pre-decoding, and 3) Post-editing. Based on the experimental results, these strategies optimize the benefit of the multilingual LD to NMT application. The experiments on English-Spanish translation as well as English-French translation evaluate the validity of the proposed idea. © 2020 ACM.",Final,,
Verhagen M.; Suderman K.; Wang D.; Ide N.; Shi C.; Wright J.; Pustejovsky J.,The LAPPS interchange format,1,1,1,#excluded #outofscope,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961700378&doi=10.1007%2f978-3-319-31468-6_3&partnerID=40&md5=2034b6e99f0c6bf9a1666548be35cf0f,"We describe and motivate the LAPPS Interchange Format, a JSON-LD format that is used for data transfer between language services in the Language Application Grid. The LAPPS Interchange Format enables syntactic and semantic interoperability of language services by providing a uniform syntax for common linguistic data and by using the Linked Data aspect of JSON-LD to refer to external definitions of linguistic categories. It is tightly integrated with the Web Services Exchange Vocabulary, which specifies a terminology for a core of linguistic objects and features exchanged by services. © Springer International Publishing Switzerland 2016.",Final,,
Di Buccio E.; Di Nunzio G.M.; Silvello G.,A linked open data approach for geolinguistics applications,1,1,1,#kg #ontology #domain-specific #geolinguistic #metalinguistc #linguistic atlas #geographic information,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893913189&doi=10.1504%2fIJMSO.2014.059125&partnerID=40&md5=b6c25d8bb032118d163144e76c46502a,"Geolinguistic systems explore the relationship between language and cultural adaptation and change and they can be used as instructional tools, presenting complex data and relationships in a way accessible to all educational levels. However, the heterogeneity of geolinguistic projects has been recognised as a key problem limiting the reusability of linguistic tools and data collections. We propose an approach based on LOD, which moves the focus from the systems handling the data to the data themselves with the main goal of increasing the level of interoperability of geolinguistic applications and the reuse of the data. We defined an extensible ontology for geolinguistic resources based on the common ground defined by current European linguistic projects. We provide a Geolinguistic Linked Open Dataset based on the data case study of a linguistic project named ASIt. Finally, we show a geolinguistic application, which exploits this dataset for dynamically generating linguistic maps. Copyright © 2014 Inderscience Enterprises Ltd.",Final,,
Calì A.; Capuzzi S.; Dimartino M.M.; Frosini R.,Recommendation of text tags in social applications using linked data,1,1,1,#use #sematic-similarity,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893742773&doi=10.1007%2f978-3-319-04244-2_17&partnerID=40&md5=45ad37b0010cf876305de854d418aa6f,We present a recommender system that suggests geo-located text tags by using linguistic information extracted from Linked Data sets available on the Web. The recommender system performs tag matching by measuring the semantic similarity of natural language texts. Our approach evaluates similarity using a technique that compares sentences taking into account their grammatical structure. © Springer International Publishing 2013.,Final,All Open Access; Bronze Open Access,
Guo M.; Chen Y.; Xu J.; Zhang Y.,Dynamic Knowledge Integration for Natural Language Inference,1,1,1,#use,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139414930&doi=10.1109%2fICNLP55136.2022.00066&partnerID=40&md5=0bedbd55d6413eaf9aaa331f6cc6d90c,"Natural language inference (NLI) aims to determine the entailment relationship between the premise and the hypothesis. It is a fundamental but difficult problem, since there may exists serious semantic and logistic gap between the premise and the hypothesis. Despite using strong pre-trained language model (PLM), previous work performs poorly on complicated reasoning for knowledge-sensitive cases ignoring the integration of external knowledge. We propose a dynamic knowledge integration strategy for NLI, where knowledge from multiple knowledge graphs (KGs) can be dynamically integrated. For each KG, it transforms input tokens into a graph according to the connectivity of the related entities. All the graphs are encoded by a group of parallel graph neural networks (GNNs), and after each layer the intermediate results are integrated dynamically by being conditioned on the input text. This strategy also facilitates the incorporation of PLM, simply by treating the input tokens as a fully connected graph and adapting the PLM outputs as the node embeddings. Experiments on SNLI, MNLI and SciTail show that, the dynamic integration of knowledge from WordNet and ConceptNet achieves significant improvements over the strongest baseline built upon RoBERTa.  © 2022 IEEE.",Final,,
Hakami H.; Hakami M.; Mandya A.; Bollegala D.,Learning to Borrow - Relation Representation for Without-Mention Entity-Pairs for Knowledge Graph Completion,1,1,-1,#excluded ,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138356689&partnerID=40&md5=23259f3097e7310ea1f1a2b64e45e41e,"Prior work on integrating text corpora with knowledge graphs (KGs) to improve Knowledge Graph Embedding (KGE) have obtained good performance for entities that co-occur in sentences in text corpora. Such sentences (textual mentions of entity-pairs) are represented as Lexicalised Dependency Paths (LDPs) between two entities. However, it is not possible to represent relations between entities that do not co-occur in a single sentence using LDPs. In this paper, we propose and evaluate several methods to address this problem, where we borrow LDPs from the entity pairs that co-occur in sentences in the corpus (i.e. with mention entity pairs) to represent entity pairs that do not co-occur in any sentence in the corpus (i.e. without mention entity pairs). We propose a supervised borrowing method, SuperBorrow, that learns to score the suitability of an LDP to represent a without-mention entity pair using pre-trained entity embeddings and contextualised LDP representations. Experimental results show that SuperBorrow improves the link prediction performance of multiple widely-used prior KGE methods such as TransE, DistMult, ComplEx and RotatE. © 2022 Association for Computational Linguistics.",Final,,
Sharma A.; Jain S.,Multilingual semantic representation of smart connected world data,1,1,-1,#related #ontology #multilinguism,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118413534&doi=10.1007%2f978-3-030-76387-9_7&partnerID=40&md5=25a100b762c9c451c01a5681d3518a62,"IoT devices now come in all shapes and forms. IoT is everywhere, from our mobile devices to cars. These devices help to perform various tasks from providing locations for navigation purposes, to detecting a heartbeat inside a locked car to notify parents or pet owners that they have left their child or pet inside the car and need attention. The latter example is not possible only with IoT devices; they need algorithms and systems to detect these heartbeats, and this is facilitated by Artificial Intelligence (AI). We will be working with Artificial Intelligence of Things (AIoT), a combination of Artificial Intelligence and IoT. We can't ignore the fact that our world is multilingual, with a wide variety of cultures and ethnic and racial groups. One of the AIoT system tasks is hearing a sentence from a user, interpreting what it means, and performing tasks accordingly. But this task is not so easy because of all the existing languages and cultural variations. Understanding cultural variations is crucial because it affects how a language is formed and used. This chapter covers just that, from the use and working of AIoT to how a computer can store and understand language-specific information and work with it. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2021. All rights reserved.",Final,,
Garcia-Silva A.; Denaux R.; Gomez-Perez J.M.,On the impact of knowledge-based linguistic annotations in the quality of scientific embeddings,1,1,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101797141&doi=10.1016%2fj.future.2021.02.019&partnerID=40&md5=7dd8c6f926c093f01ef4e7e542687152,"In essence, embedding algorithms work by optimizing the distance between a word and its usual context in order to generate an embedding space that encodes the distributional representation of words. In addition to single words or word pieces, other features which result from the linguistic analysis of text, including lexical, grammatical and semantic information, can be used to improve the quality of embedding spaces. However, until now we did not have a precise understanding of the impact that such individual annotations and their possible combinations may have in the quality of the embeddings. In this paper, we conduct a comprehensive study on the use of explicit linguistic annotations to generate embeddings from a scientific corpus and quantify their impact in the resulting representations. Our results show how the effect of such annotations in the embeddings varies depending on the evaluation task. In general, we observe that learning embeddings using linguistic annotations contributes to achieve better evaluation results. © 2021 Elsevier B.V.",Final,All Open Access; Green Open Access,
Pan Y.; Chen Q.; Peng W.; Wang X.; Hu B.; Liu X.; Chen J.; Zhou W.,MedWriter: Knowledge-Aware Medical Text Generation,1,1,1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139892424&partnerID=40&md5=e3ce78079138dbef5b8f502fc13e0dea,"To exploit the domain knowledge to guarantee the correctness of generated text has been a hot topic in recent years, especially for high professional domains such as medical. However, most of recent works only consider the information of unstructured text rather than structured information of the knowledge graph. In this paper, we focus on the medical topic-to-text generation task and adapt a knowledge-aware text generation model to the medical domain, named MedWriter, which not only introduces the specific knowledge from the external MKG but also is capable of learning graph-level representation. We conduct experiments on a medical literature dataset collected from medical journals, each of which has a set of topic words, an abstract of medical literature and a corresponding knowledge graph from CMeKG. Experimental results demonstrate incorporating knowledge graph into generation model can improve the quality of the generated text and has robust superiority over the competitor methods. © 2020 COLING 2020 - 28th International Conference on Computational Linguistics, Proceedings of the Conference. All rights reserved.",Final,,
Kumar S.; Jat S.; Saxena K.; Talukdar P.,Zero-shot word sense disambiguation using sense definition embeddings,1,1,1,#use #word-sense_disambiguation WordNet,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075859601&partnerID=40&md5=df7fa442077c5b74e0de03860cb4219b,"Word Sense Disambiguation (WSD) is a longstanding but open problem in Natural Language Processing (NLP). WSD corpora are typically small in size, owing to an expensive annotation process. Current supervised WSD methods treat senses as discrete labels and also resort to predicting the Most-Frequent-Sense (MFS) for words unseen during training. This leads to poor performance on rare and unseen senses. To overcome this challenge, we propose Extended WSD Incorporating Sense Embeddings (EWISE), a supervised model to perform WSD by predicting over a continuous sense embedding space as opposed to a discrete label space. This allows EWISE to generalize over both seen and unseen senses, thus achieving generalized zero-shot learning. To obtain target sense embeddings, EWISE utilizes sense definitions. EWISE learns a novel sentence encoder for sense definitions by using WordNet relations and also ConvE, a recently proposed knowledge graph embedding method. We also compare EWISE against other sentence encoders pretrained on large corpora to generate definition embeddings. EWISE achieves new state-of-the-art WSD performance. © 2019 Association for Computational Linguistics",Final,,
Zadeh P.D.H.; D. Hossein Zadeh M.; Reformat M.Z.,Feature-driven linguistic-based entity matching in linked data with application in pharmacy,1,0/1,-1,#excluded,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926221452&doi=10.1007%2fs00500-015-1632-6&partnerID=40&md5=c95fa81febc8406bc647518a8d9b384d,"The web becomes an overwhelmingly huge repository of data. At the same time, users demand access to the information on the web in a more natural way. In other words, users require interaction with the web using natural linguistic terms and expect human comprehensive answers. The introduction of Resource Description Framework (RDF) is a promising step towards significant changes how systems can utilize the web. The very nature of RDF format that ensures high interconnectivity of pieces of data creates an opportunity to process and analyse data in a different way. In this paper, we address the problem of processing web information using fuzzy-based technologies. In particular, we adopt a linguistic representation model to determining alternatives that match a given reference with the highest possible degree and satisfying some specific criteria. The process of comparing alternatives to the reference is feature-driven while an entity is described by its features. The proposed methodology is able to deal with features of different nature and utilize comparison mechanisms suitable for each type of features. The utilization of 2-tuple allows for comparing and aggregating linguistic-based descriptions of features, especially when the reference does not specify values of features explicitly. In experiments, we show the utilization of our approach in the domain of pharmacy. The obtained results show the advantage of using the feature-based comparison process and linguistic aggregation procedure over results obtained using the RDF query language SPARQL (SPARQL Protocol and RDF Query Language). © 2015, Springer-Verlag Berlin Heidelberg.",Final,,
Zhang Z.; Yuan P.; Jin H.,Exploring Word-Sememe Graph-Centric Chinese Antonym Detection,1,1,-1,#excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174442827&doi=10.1007%2f978-3-031-43418-1_35&partnerID=40&md5=e3895e1353ef188f5be4ea162c5ca49e,"Antonym detection is a vital task in NLP systems. Pattern-based methods, typical solutions for this, recognize semantic relationships between words using given patterns but have limited performance. Distributed word embeddings often struggle to distinguish antonyms from synonyms because their representations rely on local co-occurrences in similar contexts. Combining the ambiguity of Chinese and the contradictory nature of antonyms, antonym detection faces unique challenges. In this paper, we propose a word-sememe graph to integrate relationships between sememes and Chinese words, organized as a 4-partite graph. We design a heuristic sememe relevance computation as a supplementary measure and develop a relation inference scheme using related sememes as taxonomic information to leverage the relational transitivity. The 4-partite graph can be extended based on this scheme. We introduce the R elation D iscriminated L earning based on S ememe A ttention (RDLSA) model, employing three attention strategies on sememes to learn flexible entity representations. Antonym relations are detected using a Link Prediction approach with these embeddings. Our method demonstrates superior performance in Triple Classification and Chinese Antonym Detection compared to the baselines. Experimental results show reduced ambiguity and improved antonym detection using linguistic sememes. A quantitative ablation analysis further confirms our scheme’s effectiveness in capturing antonyms. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Final,,
Harrando I.; Troncy R.,Combining semantic and linguistic representations for media recommendation,1,1,-1,#excluded #noLLOD,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134315539&doi=10.1007%2fs00530-022-00968-w&partnerID=40&md5=019ec48399117519fc140f2a5a02b8b8,"Content-based recommendation systems offer the possibility of promoting media (e.g., posts, videos, podcasts) to users based solely on a representation of the content (i.e., without using any user-related data such as views or interactions between users and items). In this work, we study the potential of using different textual representations (based on the content of the media) and semantic representations (created from a knowledge graph of media metadata). We also show that using off-the-shelf automatic annotation tools from the Information Extraction literature, we can improve recommendation performance, without any extra cost of training, data collection or annotation. We first evaluate multiple textual content representations on two tasks of recommendation: user-specific, which is performed by suggesting new items to the user given a history of interactions, and item-based, which is based solely on content relatedness, and is rarely investigated in the literature of recommender systems. We compare how using automatically extracted content (via ASR) compares to using human-written summaries. We then derive a semantic content representation by combining manually created metadata and automatically extracted annotations and we show that Knowledge Graphs, through their embeddings, constitute a great modality to seamlessly integrate extracted knowledge to legacy metadata and can be used to provide good content recommendations. We finally study how combining both semantic and textual representations can lead to superior performance on both recommendation tasks. Our code is available at https://github.com/D2KLab/ka-recsys to support experiment reproducibility. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Final,All Open Access; Green Open Access,
Wen Q.; Tian Y.; Zhang X.; Hu R.; Wang J.; Hou L.; Li J.,Type-Aware Open Information Extraction via Graph Augmentation Model,1,-1,1,#kg #chinese,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111047447&doi=10.1007%2f978-981-16-1964-9_10&partnerID=40&md5=ca85f8c987b989ed0016a00b02bffc14,"Open information extraction (IE) can support knowledge graph enrichment. Open IE systems are capable of extracting relational tuples from texts without the need for a pre-specified vocabulary. There have been more researches on open IE in English than in Chinese, and most of them rely on word segmentation and syntactic analysis tools, which have a great influence on the results. Besides, the lack of annotated Chinese corpus also makes it difficult to classify triples in a supervised manner. To address the problems, we propose an unsupervised Chinese open IE model, named graph augmentation model (GAM). It first uses the knowledge graph to obtain linked entities and types of entities, where the linked entities can benefit the word segmentation accuracy and the entity types can help obtain the domain and range of relations for knowledge graph schema completion. Then it uses manually set rules to obtain candidate triples and uses a designed graph-based algorithm to iteratively calculate the importance and accuracy of triples. Experiments demonstrate that our method outperforms existing baseline methods. Specifically, GAM is proved to effectively extract domain and range of relations that other methods cannot. GAM achieves high accuracy of triples above a certain threshold, and the triples obtained show benefits in enriching a knowledge graph without the need for data annotation. © 2021, Springer Nature Singapore Pte Ltd.",Final,,
Li H.; Xu F.,Question answering with DBpedia based on the dependency parser and entity-centric index,1,1,1,#use #question-answering WordNet and conceptnet to expand synonyms,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994850866&doi=10.1109%2fICCIA.2016.10&partnerID=40&md5=d27f5e4648b945d672480870bf24972f,"The emerging Linked Open Data provides an opportunity to answer the natural language question based on knowledge bases (KB). This study proposes an approach to question answering (QA) on the DBpedia dataset. After parsing the question by a dependency parser, we locate the entity mention and property mention with predefined templates. We propose an entity-centric indexing model to help search referent entities in KB. After obtaining the referent entities, we expand the property mention with WordNet and ConceptNet to find the referent properties of the returned entities. The values of the referent property are then considered the answer to the question. Evaluations are performed on DBpedia version 2015. Results show that our approach reaches 46% precision when the top-10 entities are returned in the final QA stage. The evaluation tests show that our approach is promising in dealing with QA in Linked Data. © 2016 IEEE.",Final,,
Kato F.; Takeda H.; Koide S.; Ohmukai I.,Building DBpedia japanese and linked data cloud in japanese,1,1,1,#kg #japanese,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922053615&partnerID=40&md5=ff345c16b24e501cb208def793413b5a,"Wikipedia is one of the most valuable language and onto- logical resources covering wide domains so that DBpedia, LOD based on Wikipedia, plays the important role in the LOD cloud by connecting various resources. DBpedia Japanese is the LOD created from Wikipedia Japanese just as DBpedia data sources in other languages like English and German. We here describe how the conversion could be carried out with the efforts to fit DBpedia software and show the results of the dataset. We also describe how the created DBpedia Japanese is used by other Linked Data and show the Linked Data Cloud in Japanese.",Final,,
Perera R.; Nand P.; Klette G.,RealTextlex: A lexicalization framework for linked open data,1,1,-1,#excluded #lodlexicalization #noLLOD,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955577860&partnerID=40&md5=f707be0930aa670704c7c320f95255ad,"Linked Open Data (LOD) is growing rapidly as a source of structured knowledge used in a variety of text processing applications. However, the applications using the LOD need to be able to mediate between the front end user interfaces and LOD. This often requires a natural language interpretation of this structured, linked data. We demonstrate a middle-tier framework that can generate patterns which can be used to transform LOD triples back into natural text. The framework utilizes preprocessed free text to extract a wide range of relations which are then aligned with triples to identify possible lexicalization patterns. These lexicalization patterns can then be used to transform a given triple into natural language sentence.",Final,,
Yadav S.; Pallagani V.; Sheth A.,Medical Knowledge-enriched Textual Entailment Framework,1,1,-1,#excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149672544&partnerID=40&md5=2302894b2523d23035fce4a514c8102c,"One of the cardinal tasks in achieving robust medical question answering systems is textual entailment. The existing approaches make use of an ensemble of pre-trained language models or data augmentation, often to clock higher numbers on the validation metrics. However, two major shortcomings impede higher success in identifying entailment: (1) understanding the focus/intent of the question and (2) ability to utilize the real-world background knowledge to capture the context beyond the sentence. In this paper, we present a novel Medical Knowledge-Enriched Textual Entailment framework that allows the model to acquire a semantic and global representation of the input medical text with the help of a relevant domain-specific knowledge graph. We evaluate our framework on the benchmark MEDIQA-RQE dataset and manifest that the use of knowledge-enriched dual-encoding mechanism help in achieving an absolute improvement of 8.27% over SOTA language models. We have made the source code available here. © 2020 COLING 2020 - 28th International Conference on Computational Linguistics, Proceedings of the Conference. All rights reserved.",Final,,
Van Aggelen A.; Hollink L.; Van Ossenbruggen J.,Combining distributional semantics and structured data to study lexical change,1,1,1,#use,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016291410&partnerID=40&md5=c553b8279d1c93ff2cff33aba3370daa,"Statistical Natural Language Processing (NLP) techniques allow to quantify lexical semantic change using large text corpora. Word-level results of these methods can be hard to analyse in the context of sets of semantically or linguistically related words. On the other hand, structured knowledge sources represent such relationships explicitly, but ignore the problem of semantic change. We aim to address these limitations by combining the statistical and symbolic approach: we enrich WordNet, a structured lexical database, with quantitative lexical change scores provided by HistWords, a dataset produced by distributional NLP methods. We publish the result as Linked Open Data and demonstrate how queries on the combined dataset can provide new insights. © 2016, CEUR-WS. All rights reserved.",Final,,
Srivastava N.; Perevalov A.; Kuchelev D.; Moussallem D.; Ngonga Ngomo A.-C.; Both A.,Lingua Franca - Entity-Aware Machine Translation Approach for Question Answering over Knowledge Graphs,1,1,1,#use,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180369135&doi=10.1145%2f3587259.3627567&partnerID=40&md5=65a582eb42da55e0bc56ce9e187cbe41,"This research paper proposes an approach called Lingua Franca that improves machine translation quality by utilizing information from a knowledge graph to translate named entities accurately. The accurate entity translation is crucial when applied to entity-oriented search including Knowledge Graph Question Answering systems. In a nutshell, the approach preserves recognized named entities with an entity-replacement technique during the translation process. It replaces the entities back with their labels found in a knowledge graph for the target language to ensure that questions are translated correctly before answering them using a Knowledge Graph Question Answering system. The paper also introduces an open-source modular framework that enables researchers to design their own named entity-aware machine translation pipelines. The presented experimental results demonstrate the effectiveness of the Lingua Franca approach in comparison to baseline Machine Translation models. The approach shows a statistically significant improvement in the quality provided by several Knowledge Graph Question Answering systems using Lingua Franca on different datasets. © 2023 ACM.",Final,,
AlMousa M.; Benlamri R.; Khoury R.,A novel word sense disambiguation approach using WordNet knowledge graph,1,1,1,#use #wordsensedisambiguation,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122705398&doi=10.1016%2fj.csl.2021.101337&partnerID=40&md5=ac2d90607397c902c5eaef4ae5251b0c,"Various applications in computational linguistics and artificial intelligence rely on high-performing word sense disambiguation techniques to solve challenging tasks such as information retrieval, machine translation, question answering, and document clustering. While text comprehension is intuitive for humans, machines face tremendous challenges in processing and interpreting a human's natural language. This paper presents a novel knowledge-based word sense disambiguation algorithm, namely Sequential Contextual Similarity Matrix Multiplication (SCSMM). The SCSMM algorithm combines semantic similarity, heuristic knowledge, and document context to respectively exploit the merits of local sense-based context between consecutive terms, human knowledge about terms, and a document's main topic in disambiguating terms. Unlike other algorithms, the SCSMM algorithm guarantees the capture of the maximum sentence context while maintaining the terms’ order within the sentence. The proposed algorithm outperformed all other algorithms when disambiguating nouns on the combined gold standard datasets, while demonstrating comparable results to current state-of-the-art word sense disambiguation systems when dealing with each dataset separately. Furthermore, the paper discusses the impact of granularity level, ambiguity rate, sentence size, and part of speech distribution on the performance of the proposed algorithm. © 2022 Elsevier Ltd",Final,All Open Access; Green Open Access,
Zeng Z.; Cheng K.T.; Nanniyur S.V.; Zhou J.; Bhat S.,IEKG: A Commonsense Knowledge Graph for Idiomatic Expressions,1,1,1,#kg,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184812286&partnerID=40&md5=23c4bc18fa4642ce63ebca5720985705,"Idiomatic expression (IE) processing and comprehension have challenged pre-trained language models (PTLMs) because their meanings are non-compositional. Unlike prior works that enable IE comprehension through fine-tuning PTLMs with sentences containing IEs, in this work, we construct IEKG, a commonsense knowledge graph for figurative interpretations of IEs. This extends the established ATOMIC2020 (Hwang et al., 2021) graph, converting PTLMs into knowledge models (KMs) that encode and infer commonsense knowledge related to IE use. Experiments show that various PTLMs can be converted into KMs with IEKG. We verify the quality of IEKG and the ability of the trained KMs with automatic and human evaluation. Through applications in natural language understanding, we show that a PTLM injected with knowledge from IEKG exhibits improved IE comprehension ability and can generalize to IEs unseen during training. ©2023 Association for Computational Linguistics.",Final,,
Wang Z.; Zhang Z.; Qin J.; Iwaihara M.,"SLHCat: Mapping Wikipedia Categories and Lists to DBpedia by Leveraging Semantic, Lexical, and Hierarchical Features",1,1,-1,#excluded #noLLOD #ontologyalignment,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180152462&doi=10.1007%2f978-981-99-8085-7_12&partnerID=40&md5=ec7d49a1dfe28d59acefc0e72f577ade,"Wikipedia articles are hierarchically organized through categories and lists, providing one of the most comprehensive and universal taxonomy, but its open creation is causing redundancies and inconsistencies. Assigning DBPedia classes to Wikipedia categories and lists can alleviate the problem, realizing a large knowledge graph which is essential for categorizing digital contents through entity linking and typing. However, the existing approach of CaLiGraph is producing incomplete and non-fine grained mappings. In this paper, we tackle the problem as ontology alignment, where structural information of knowledge graphs and lexical and semantic features of ontology class names are utilized to discover confident mappings, which are in turn utilized for finetuing pretrained language models in a distant supervision fashion. Our method SLHCat consists of two main parts: 1) Automatically generating training data by leveraging knowledge graph structure, semantic similarities, and named entity typing. 2) Finetuning and prompt-tuning of the pre-trained language model BERT are carried out over the training data, to capture semantic and syntactic properties of class names. Our model SLHCat is evaluated over a benchmark dataset constructed by annotating 3000 fine-grained CaLiGraph-DBpedia mapping pairs. SLHCat is outperforming the baseline model by a large margin of 25% in accuracy, offering a practical solution for large-scale ontology mapping. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Final,All Open Access; Green Open Access,
Pedretti I.; Del Grosso A.; Giovannetti E.; Mancini L.; Piccini S.; Abrate M.; Lo Duca A.; Marchetti A.,"The clavius on the web project: Digitization, annotation and visualization of early modern manuscripts",1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958691991&doi=10.1145%2f2802612.2802636&partnerID=40&md5=62dee99529fbb39587a182d59688ec41,"This paper describes the full procedure adopted in the context of the Clavius on the Web project, which aims to help Web users to appraise the importance of specific manuscripts by going beyond their digital reproduction. The proposed approach is based on the multilayered explication of linguistic, lexical and semantic data representing the innermost nature of the analyzed manuscripts. The final purpose of the project is to gather and display the results of the three layers of analysis through interactive visualization techniques and export them as Linked Data. All the analyses rely on the XML/TEI encoding of the text, followed by a CTS-based tokenization. As a working example for this paper, the analysis of a portion of a manuscript provided by Historical Archives of the Pontifical Gregorian University will be illustrated. The text is a letter written in Latin and sent by Botvitus Nericius to Christophorus Clavius in 1598 from Madrid. © 2014 ACM.",Final,,
Li Y.; Clark P.,Answering elementary science questions by constructing coherent scenes using background knowledge,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959867303&doi=10.18653%2fv1%2fd15-1236&partnerID=40&md5=ca35d2968fd72770c7eabd9c013c4b46,"Much of what we understand from text is not explicitly stated. Rather, the reader uses his/her knowledge to fill in gaps and create a coherent, mental picture or ""scene"" depicting what text appears to convey. The scene constitutes an understanding of the text, and can be used to answer questions that go beyond the text. Our goal is to answer elementary science questions, where this requirement is pervasive; A question will often give a partial description of a scene and ask the student about implicit information. We show that by using a simple ""knowledge graph"" representation of the question, we can leverage several large-scale linguistic resources to provide missing background knowledge, somewhat alleviating the knowledge bottleneck in previous approaches. The coherence of the best resulting scene, built from a question/answer-candidate pair, reflects the confidence that the answer candidate is correct, and thus can be used to answer multiple choice questions. Our experiments show that this approach outperforms competitive algorithms on several datasets tested. The significance of this work is thus to show that a simple ""knowledge graph"" representation allows a version of ""interpretation as scene construction"" to be made viable. © 2015 Association for Computational Linguistics.",Final,All Open Access; Green Open Access; Hybrid Gold Open Access,
Wendt M.; Gerlach M.; Düwiger H.,Linguistic modeling of linked open data for question answering,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942694222&doi=10.1007%2f978-3-662-46641-4_8&partnerID=40&md5=5d225bae902853fe2896daa11926a55a,"With the evolution of linked open data sources, question answering regains importance as a way to make data accessible and explorable to the public. The triple structure of RDF-data at the same time seems to predetermine question answering for being devised in its native subject-verb-object form. The devices of natural language, however, often exceed this trFiple-centered model. But RDF does not preclude this point of view. Rather, it depends on the modeling. As part of a government funded research project named Alexandria, we implemented an approach to question answering that enables the user to ask questions in ways that may involve more than binary relations. © Springer-Verlag Berlin Heidelberg 2015.",Final,All Open Access; Green Open Access,
Franco-Salvador M.; Gupta P.; Rosso P.,Knowledge graphs as context models: Improving the detection of cross-language plagiarism with paraphrasing,1,1,1,#use #plagiarism-detection,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901341490&doi=10.1007%2f978-3-642-54798-0_12&partnerID=40&md5=38f36f4e100941b64bcdc632b5e709fd,"Cross-language plagiarism detection attempts to identify and extract automatically plagiarism among documents in different languages. Plagiarized fragments can be translated verbatim copies or may alter their structure to hide the copying, which is known as paraphrasing and is more difficult to detect. In order to improve the paraphrasing detection, we use a knowledge graph-based approach to obtain and compare context models of document fragments in different languages. Experimental results in German-English and Spanish-English cross-language plagiarism detection indicate that our knowledge graph-based approach offers a better performance compared to other state-of-the-art models. © 2014 Springer-Verlag.",Final,All Open Access; Green Open Access,
Li X.; Tur G.; Hakkani-Tur D.; Li Q.,Personal knowledge graph population from user utterances in conversational understanding,1,1,1,#use,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983195590&doi=10.1109%2fSLT.2014.7078578&partnerID=40&md5=1cdb2929b884ab85366b7f988ad9290f,"Knowledge graphs provide a powerful representation of entities and the relationships between them, but automatically constructing such graphs from spoken language utterances presents the novelty and numerous challenges. In this paper, we introduce a statistical language understanding approach to automatically construct personal (user-centric) knowledge graphs in conversational dialogs. Such information has the potential to better understand the users' requests, fulfilling them, and enabling other technologies such as developing better inferences or proactive interactions. Knowledge encoded in semantic graphs such as Freebase has been shown to benefit semantic parsing and interpretation of natural language utterances. Hence, as a first step, we exploit the personal factual relation triples from Freebase to mine natural language snippets with a search engine, and the resulting snippets containing pairs of related entities to create the training data. This data is then used to build three key language understanding components: (1) Personal Assertion Classification identifies the user utterances that are relevant with personal facts, e.g., 'my mother's name is Rosa'; (2) Relation Detection classifies the personal assertion utterance into one of the predefined relation classes, e.g., 'parents'; and (3) Slot Filling labels the attributes or arguments of relations, e.g., 'name(parents): Rosa'. Our experiments using the Microsoft conversational understanding system demonstrate the performance of this proposed approach on the population of personal knowledge graphs. © 2014 IEEE.",Final,,
Sánchez-Rada J.F.; Vulcu G.; Iglesias C.A.; Buitelaar P.,EUROSENTIMENT: Linked data sentiment analysis,1,1,1,#kg,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921887944&partnerID=40&md5=b70ae9f6fb7a8e6419d701bd746999c8,"Sentiment and Emotion Analysis strongly depend on quality language resources, especially sentiment dictionaries. These resources are usually scattered, heterogeneous and limited to specific domains of application by simple algorithms. The EUROSENTIMENT project addresses these issues by 1) developing a common language resource representation model for sentiment analysis, and APIs for sentiment analysis services based on established Linked Data formats (lemon, Marl, NIF and ONYX) 2) by creating a Language Resource Pool (a.k.a. LRP) that makes available to the community existing scattered language resources and services for sentiment analysis in an interoperable way. In this paper we describe the available language resources and services in the LRP and some sample applications that can be developed on top of the EUROSENTIMENT LRP.",Final,,
Gamba F.; Passarotti M.C.; Ruffolo P.,Linking the Dictionary of Medieval Latin in the Czech Lands to the LiLa Knowledge Base,1,1,1,#kg,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181170485&partnerID=40&md5=aff1f6c290235155b182138c4e61c2f6,"The paper presents the process of linking the Dictionary of Medieval Latin in the Czech Lands to the LiLa Knowledge Base, which adopts the Linked Data paradigm to make linguistic resources for Latin interoperable. An overview of the Dictionary and of the architecture of the LiLa Knowledge Base is first provided; then, the stages of the process of linking the Dictionary to LiLa's collection of lemmas are described. In conclusion, a query illustrates how interoperability allows for full exploitation of Latin resources. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",Final,,
Alexander R.J.; Bartocci M.; Persico O.; Vetere G.,Harnessing Il Manifesto Newspaper Archive for Knowledge Base Creation: Techniques and Findings in the MeMa Project,1,1,1,#kg,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181166938&partnerID=40&md5=afe1b1cd50fecba10a70c4af0fde3629,"The historical archive of the newspaper “il Manifesto” is a valuable asset protected by the Italian Ministry of Cultural Heritage. The MeMa project aims to create an “intelligent archive” using AI principles, fostering collaboration and transparency. The platform, built around Apache Jena and open linguistic technologies, addresses the newspaper community's specific needs. This paper presents the platform's architecture, knowledge base construction process, and future directions, emphasizing journalism enhancements through AI while respecting “Il Manifesto”'s principles. Italiano.L'archivio storico del quotidiano “il Manifesto” è tutelato dal Ministero dei Beni Culturali. Il progetto MeMa mira a creare un “archivio intelligente” basato su una intelligenza artificiale che favorisce la collaborazione e la trasparenza. La piattaforma, costruita attorno ad Apache Jena e tecnologie linguistiche aperte, risponde alle esigenze specifiche della comunità del giornale. Questo contributo presenta l'architettura della piattaforma, il processo di costruzione della base di conoscenza e le direzioni future, discutendo il potenziamento del giornalismo attraverso l'intelligenza artificiale nel rispetto dei principi de “Il Manifesto”. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",Final,,
De Felice I.; Tamponi L.; Iurescia F.; Passarotti M.,Linking the Corpus CLaSSES to the LiLa Knowledge Base of Interoperable Linguistic Resources for Latin,1,1,1,#kg,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181169574&partnerID=40&md5=cc655c13e1148a60682c1136d1905bcf,"In this paper, we describe the process of linking the corpus CLaSSES (which collects non-literary Latin texts of different periods and places) to the LiLa Knowledge Base of linguistic resources for Latin made interoperable through their publication as Linked Data. The paper details the RDF modeling of the (meta)data provided by CLaSSES and presents three queries on data from different resources that interact in LiLa. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",Final,,
Boano V.I.; Mambrini F.; Passarotti M.; Ginevra R.,Modelling and Publishing the “Lexicon der indogermanischen Verben” as Linked Open Data,1,1,1,#kg,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181173280&partnerID=40&md5=1eae48af781f7c0327c8a9776a5ad990,"This paper describes the modelling and publication of part of the etymological information in the Lexicon der indogermanischen Verben, an etymological dictionary of verbs attested in ancient Indo-European languages, as Linguistic Linked Open Data. The lexicon has been made interoperable with a set of lexical and textual linguistic resources for Latin in the Lila Knowledge Base. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",Final,,
Zugarini A.; Röthenbacher T.; Klede K.; Ernandes M.; Eskofier B.M.; Zanca D.,Die Rätselrevolution: Automated German Crossword Solving,1,1,1,#use,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181170035&partnerID=40&md5=e1a37ea17994eb346966d1a80adfd77c,"Crossword puzzles are popular word games played in various languages around the world, with diverse styles across different countries. For this reason, automated crossword solvers designed for a language, may not work well on others. In this paper, we extend Webcrow, an automatic crossword solver, to German, making it the first program for crossword solving in the German language. To address the lack of large clue-answer crossword pairs data, Webcrow combines multiple modules, known as experts, which retrieve potential answers from various resources, including the web, knowledge graphs, and linguistic rules. The system is evaluated on a collection of crosswords from variegate sources, where it is able to solve perfectly 67% of them. Additional analysis reveals that while our solver achieved commendable results, puzzles with poorly constrained schemas and original clues still presented significant hurdles. These findings shed light on the complexity of the crossword-solving problem and emphasize the need for future research to address and overcome these particular challenges effectively. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",Final,,
Cirillo N.; Vellutino D.,Towards a Multi-Level Annotation Format for the Interoperability of Automatic Term Extraction Corpora,1,,-1,#excluded #outofscope no resource is made available,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181171880&partnerID=40&md5=228c822d822adb2b51f3dce7eaa952fb,"The main corpora used as benchmarks in Automatic Term Extraction are represented in different formats. Unfortunately, none of these formats covers the wide range of linguistic phenomena related to terminology. To address this issue, we propose to encode Automatic Term Extraction corpora in RDF using the OntoLex-Lemon and the NLP Interchange Format ontologies. Furthermore, we developed a small Italian corpus on waste management legislation to provide an example of the proposed formalization. Italiano. I corpora principali impiegati nella valutazione degli algoritmi di Estrazione Automatica di Termini sono codificati in formati diversi. Purtroppo, nessuno di questi formati permette di rappresentare l'ampia gamma di fenomeni linguistici legati alla terminologia. Per affrontare la questione, proponiamo di codificare i corpora di Estrazione Automatica di Termini in RDF usando le ontologie OntoLex-Lemon e NLP Interchange Format. Inoltre, abbiamo sviluppato un piccolo corpus italiano riguardante la legislazione della gestione dei rifiuti per fornire un esempio della formalizzazione proposta. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",Final,,
Pavithra C.P.; Mandal S.,An Overview of Relevant Literature on Different Approaches to Word Sense Disambiguation,1,1,1,#use #wordsensedisambiguation,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123448034&doi=10.1109%2fICECCT52121.2021.9616677&partnerID=40&md5=362966991d677e643d37bcc41ddb99cc,"WSD (Word Sense Disambiguation) is a common issue in Natural Language Processing (NLP) and Machine Learning technology. In NLP, word sense disambiguation is described as the capacity to detect which meaning of a word is activated by its use in a specific context. WSD is a solution to the uncertainty that occurs when words have different meanings in different contexts. Contextual word meaning plays an important role in various applications such as sentiment analysis, search engine, information extraction, machine translation etc. It is a challenge for these systems to detect and overcome the uncertainty that emerges from the lexical ambiguity. Many studies have been conducted over the decades to propose various approaches to the WSD problem. In this manuscript, a comparative study of three approaches namely LESK algorithm, embedding techniques, and Neural Network techniques based on the text collected from children's story books is performed. We explored an approach that combines Bi-LSTM neural network with Knowledge Graph to predict contextual word meaning. Our study shows that the combined approach accuracy is 80.34approaches  © 2021 IEEE.",Final,,
Klimek B.; Ackermann M.; Brümmer M.; Hellmann S.,MMoOn Core - The Multilingual Morpheme Ontology,1,1,1,#kg,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113951709&doi=10.3233%2fSW-200412&partnerID=40&md5=61b6d9164fe4de93cdd02d74fa6a8a54,"In the last years a rapid emergence of lexical resources has evolved in the Semantic Web. Whereas most of the linguistic information is already machine-readable, we found that morphological information is mostly absent or only contained in semi-structured strings. An integration of morphemic data has not yet been undertaken due to the lack of existing domain-specific ontologies and explicit morphemic data. In this paper, we present the Multilingual Morpheme Ontology called MMoOn Core which can be regarded as the first comprehensive ontology for the linguistic domain of morphological language data. It will be described how crucial concepts like morphs, morphemes, word forms and meanings are represented and interrelated and how language-specific morpheme inventories can be created as a new possibility of morphological datasets. The aim of the MMoOn Core ontology is to serve as a shared semantic model for linguists and NLP researchers alike to enable the creation, conversion, exchange, reuse and enrichment of morphological language data across different data-dependent language sciences. Therefore, various use cases are illustrated to draw attention to the cross-disciplinary potential which can be realized with the MMoOn Core ontology in the context of the existing Linguistic Linked Data research landscape. © 2021 - The authors. Published by IOS Press.",Final,All Open Access; Bronze Open Access; Green Open Access,
Xia P.; Qin G.; Vashishtha S.; Chen Y.; Chen T.; May C.; Harman C.; Rawlins K.; White A.S.; van Durme B.,Lome: Large ontology multilingual extraction,1,1,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107282328&partnerID=40&md5=0d2242fdaff90b4f240f8baae701e87a,"We present Lome, a system for performing multilingual information extraction. Given a text document as input, our core system identifies spans of textual entity and event mentions with a FrameNet (Baker et al., 1998) parser. It subsequently performs coreference resolution, fine-grained entity typing, and temporal relation prediction between events. By doing so, the system constructs an event and entity focused knowledge graph. We can further apply third-party modules for other types of annotation, like relation extraction. Our (multilingual) first-party modules either outperform or are competitive with the (monolingual) state-of-the-art. We achieve this through the use of multilingual encoders like XLM-R (Conneau et al., 2020) and leveraging multilingual training data. Lome is available as a Docker container on Docker Hub. In addition, a lightweight version of the system is accessible as a web demo. © 2021 Association for Computational Linguistics",Final,,
Becker M.; Korfhage K.; Frank A.,COCO-EX: A tool for linking concepts from texts to conceptnet,1,1,1,#use,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107282443&partnerID=40&md5=bf9c125f24319e7264ffc336123d4791,"In this paper we present COCO-EX, a tool for Extracting Concepts from texts and linking them to the ConceptNet knowledge graph. COCO-EX extracts meaningful concepts from natural language texts and maps them to conjunct concept nodes in ConceptNet, utilizing the maximum of relational information stored in the ConceptNet knowledge graph. COCO-EX takes into account the challenging characteristics of ConceptNet, namely that - unlike conventional knowledge graphs - nodes are represented as non-canonicalized, free-form text. This means that i) concepts are not normalized; ii) they often consist of several different, nested phrase types; and iii) many of them are uninformative, over-specific, or misspelled. A commonly used shortcut to circumvent these problems is to apply string matching. We compare COCO-EX to this method and show that COCO-EX enables the extraction of meaningful, important rather than overspecific or uninformative concepts, and allows to assess more relational information stored in the knowledge graph. © 2021 Association for Computational Linguistics",Final,,
Chen M.; Shi W.; Zhou B.; Roth D.,Cross-lingual entity alignment with incidental supervision,1,1,1,#use #embeddings,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106367685&partnerID=40&md5=6ce87ce9d8dab67a1d80ecba9685f668,"Much research effort has been put to multilingual knowledge graph (KG) embedding methods to address the entity alignment task, which seeks to match entities in different language-specific KGs that refer to the same real-world object. Such methods are often hindered by the insufficiency of seed alignment provided between KGs. Therefore, we propose an incidentally supervised model, JEANS, which jointly represents multilingual KGs and text corpora in a shared embedding scheme, and seeks to improve entity alignment with incidental supervision signals from text. JEANS first deploys an entity grounding process to combine each KG with the monolingual text corpus. Then, two learning processes are conducted: (i) an embedding learning process to encode the KG and text of each language in one embedding space, and (ii) a self-learning based alignment learning process to iteratively induce the matching of entities and that of lexemes between embeddings. Experiments on benchmark datasets show that JEANS leads to promising improvement on entity alignment with incidental supervision, and significantly outperforms state-of-the-art methods that solely rely on internal information of KGs. © 2021 Association for Computational Linguistics",Final,,
Zhou P.; Gopalakrishnan K.; Hedayatnia B.; Kim S.; Pujara J.; Ren X.; Liu Y.; Hakkani-Tu D.,Commonsense-Focused Dialogues for Response Generation: An Empirical Study,1,1,1,#use #commonsense,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117308108&partnerID=40&md5=303ec9ab3fd4f438ba19544878e2e34c,"Smooth and effective communication requires the ability to perform latent or explicit commonsense inference. Prior commonsense reasoning benchmarks (such as SocialIQA and CommonsenseQA) mainly focus on the discriminative task of choosing the right answer from a set of candidates, and do not involve interactive language generation as in dialogue. Moreover, existing dialogue datasets do not explicitly focus on exhibiting commonsense as a facet. In this paper, we present an empirical study of commonsense in dialogue response generation. We first auto-extract commonsensical dialogues from existing dialogue datasets by leveraging ConceptNet, a commonsense knowledge graph. Furthermore, building on social contexts/situations in SocialIQA, we collect a new dialogue dataset with 25K dialogues aimed at exhibiting social commonsense in an interactive setting. We evaluate response generation models trained using these datasets and find that models trained on both extracted and our collected data produce responses that consistently exhibit more commonsense than baselines. Finally we propose an approach for automatic evaluation of commonsense that relies on features derived from ConceptNet and pretrained language and dialog models, and show reasonable correlation with human evaluation of responses' commonsense quality.  ©2021 Association for Computational Linguistics.",Final,,
Bauer L.; Bansal M.,"Identify, align, and integrate: Matching knowledge graphs to commonsense reasoning tasks",1,1,1,#use #commonsense,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107268857&partnerID=40&md5=1a3d30a6d333c1d466373512665e1c17,"Integrating external knowledge into commonsense reasoning tasks has shown progress in resolving some, but not all, knowledge gaps in these tasks. For knowledge integration to yield peak performance, it is critical to select a knowledge graph (KG) that is well-aligned with the given task's objective. We present an approach to assess how well a candidate KG can correctly identify and accurately fill in gaps of reasoning for a task, which we call KG-to-task match. We show this KG-to-task match in 3 phases: knowledge-task identification, knowledge-task alignment, and knowledge-task integration. We also analyze our transformer-based KG-to-task models via commonsense probes to measure how much knowledge is captured in these models before and after KG integration. Empirically, we investigate KG matches for the SocialIQA (SIQA) (Sap et al., 2019b), Physical IQA (PIQA) (Bisk et al., 2020), and MCScript2.0 (Ostermann et al., 2019) datasets with 3 diverse KGs: ATOMIC (Sap et al., 2019a), ConceptNet (Speer et al., 2017), and an automatically constructed instructional KG based on WikiHow (Koupaee and Wang, 2018). With our methods we are able to demonstrate that ATOMIC, an event-inference focused KG, is the best match for SIQA and MCScript2.0, and that the taxonomic ConceptNet and WikiHow-based KGs are the best matches for PIQA across all 3 analysis phases. We verify our methods and findings with human evaluation. © 2021 Association for Computational Linguistics",Final,,
Szakács B.B.; Mészáros T.,"Hybrid distance-based, CNN and Bi-LSTM system for dictionary expansion",1,1,-1,#excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101382998&doi=10.36244%2fICJ.2020.4.2&partnerID=40&md5=20a966a3a01dc875d6c918a94b90d59f,"Dictionaries like WordNet can help in a variety of Natural Language Processing applications by providing additional morphological data. They can be used in Digital Humanities research, building knowledge graphs and other applications. Creating dictionaries from large corpora of texts written in a natural language is a task that has not been a primary focus of research, as other tasks have dominated the field (such as chat-bots), but it can be a very useful tool in analysing texts. Even in the case of contemporary texts, categorizing the words according to their dictionary entry is a complex task, and for less conventional texts (in old or less researched languages) it is even harder to solve this problem automatically. Our task was to create a software that helps in expanding a dictionary containing word forms and tagging unprocessed text. We used a manually created corpus for training and testing the model. We created a combination of Bidirectional Long-Short Term Memory networks, convolutional networks and a distance-based solution that outperformed other existing solutions. While manual post-processing for the tagged text is still needed, it significantly reduces the amount of it. © 2020 Scientific Association for Infocommunications. All rights reserved.",Final,All Open Access; Bronze Open Access; Green Open Access,
Bouziane A.; Bouchiha D.; Doumi N.,Annotating Arabic Texts with Linked Data,1,1,1,#use #arabic,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106857950&doi=10.1109%2fISIA51297.2020.9416543&partnerID=40&md5=ffe754a542a65be0b0f0ada6d5e53302,"The evolution of the traditional Web towards the semantic Web allows the machine to be a first-order citizen on the Web and increases discoverability of and accessibility to the unstructured data on the Web. This evolution enables the Linked Data technology to be used as background knowledge bases for unstructured data, notably the texts, available nowadays on the Web. For the Arabic language, the current situation is less brightness; the content of the Arabic language on the Web doesn't reflect the importance of this language. Given the fact that Arabic is one of the most important languages in the Web, and unfortunately it is under-resourced, so creating linguistic resources for it now is a necessity. Thus, we developed a linguistic approach for annotating Arabic textual corpus with Linked Data, especially DBpedia, which is Linked Open Data (LOD) extracted from Wikipedia. This approach uses natural language techniques to shedding light on Arabic text with Linked Open Data. The evaluation results of this approach are encouraging, despite the high complexity of our independent-domain knowledge base and the reduced resources in Arabic natural language processing.  © 2020 IEEE.",Final,,
Mehler A.; Hemati W.; Welke P.; Konca M.; Uslu T.,Multiple Texts as a Limiting Factor in Online Learning: Quantifying (Dis-)similarities of Knowledge Networks,1,-1?,-1,#excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095556372&doi=10.3389%2ffeduc.2020.562670&partnerID=40&md5=b555be9c0f7d792286387c3c944df84a,"We test the hypothesis that the extent to which one obtains information on a given topic through Wikipedia depends on the language in which it is consulted. Controlling the size factor, we investigate this hypothesis for a number of 25 subject areas. Since Wikipedia is a central part of the web-based information landscape, this indicates a language-related, linguistic bias. The article therefore deals with the question of whether Wikipedia exhibits this kind of linguistic relativity or not. From the perspective of educational science, the article develops a computational model of the information landscape from which multiple texts are drawn as typical input of web-based reading. For this purpose, it develops a hybrid model of intra- and intertextual similarity of different parts of the information landscape and tests this model on the example of 35 languages and corresponding Wikipedias. In the way it measures the similarities of hypertexts, the article goes beyond existing approaches by examining their structural and semantic aspects intra- and intertextually. In this way it builds a bridge between reading research, educational science, Wikipedia research and computational linguistics. © Copyright © 2020 Mehler, Hemati, Welke, Konca and Uslu.",Final,All Open Access; Gold Open Access; Green Open Access,
Kancheva Z.; Radev I.,Linguistic vs Encyclopaedic Knowledge. Classification of MWEs from Wikipedia Articles,1,1,1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098103687&doi=10.2478%2fcait-2020-0051&partnerID=40&md5=a78ab248628da4eb3d98a025ca7a4b6d,"This paper reports on the first steps in the creation of linked data through the mapping between the synsets of BTB-WordNet and the articles in Bulgarian Wikipedia. The task of expanding the BTB-WordNet with encyclopaedic knowledge is done by mapping its synsets to Wikipedia articles with many MWEs found in the articles and subjected to further analysis. We look for a way to filter the Wikipedia MWEs in the effort of selecting the ones most beneficial to the enrichment of BTB-WN. © 2020 Z. Kancheva et al., published by Sciendo 2020.",Final,All Open Access; Gold Open Access,
Navigli R.,BabelNet 3.0: A core for linguistic linked data and NLP,1,1,1,#kg,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964048415&partnerID=40&md5=1196f8a7acc89a7ad22c70dc11844dbb,"This tutorial will introduce BabelNet 3.0, the largest multilingual encyclopedic dictionary and semantic network, which covers 271 languages. BabelNet is a core component of the Linked Open Data cloud and a powerful engine for virtually any Natural Language Processing task in desperate need of wide-coverage lexical semantics in arbitrary languages. © Springer International Publishing Switzerland 2016.",Final,,
Sánchez-Rada J.F.; Iglesias C.A.,Onyx: A Linked Data approach to emotion representation,1,1,1,#kg,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928027547&doi=10.1016%2fj.ipm.2015.03.007&partnerID=40&md5=107c96b0046793fd47e9000173a7f6d1,"Extracting opinions and emotions from text is becoming increasingly important, especially since the advent of micro-blogging and social networking. Opinion mining is particularly popular and now gathers many public services, datasets and lexical resources. Unfortunately, there are few available lexical and semantic resources for emotion recognition that could foster the development of new emotion aware services and applications. The diversity of theories of emotion and the absence of a common vocabulary are two of the main barriers to the development of such resources. This situation motivated the creation of Onyx, a semantic vocabulary of emotions with a focus on lexical resources and emotion analysis services. It follows a linguistic Linked Data approach, it is aligned with the Provenance Ontology, and it has been integrated with the Lexicon Model for Ontologies (lemon), a popular RDF model for representing lexical entries. This approach also means a new and interesting way to work with different theories of emotion. As part of this work, Onyx has been aligned with EmotionML and WordNet-Affect. © 2015 Elsevier Ltd. All rights reserved.",Final,All Open Access; Green Open Access,
Chiarcos C.,Annotation interoperability (Invited tutorial),1,1,-1,#notavailable #excluded,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964065872&partnerID=40&md5=4ebc8818a0899557b1124e4425712525,"The tutorial addressed the problem of heterogeneous representations for language resources, in particular annotated corpora. Heterogeneity exists on multiple levels, the most important being the levels of physical representation, and annotation vocabulary. Establishing interoperability between and among corpora, dictionaries and NLP tools thus involves two dimensions, as well, namely structural interoperability (defining how to access the data, i.e., common formats and protocols) and conceptual interoperability (defining how to interpret the data, i.e., by reference to a common vocabulary). In this tutorial, both aspects were addressed with a focus on linguistic annotations and the Ontologies of Linguistic Annotation (OLiA). As a means to solve the problem of conceptual interoperability for annotations, the OLiA ontologies represent a major hub of linguistic terminology in the Linguistic Linked Open Data (LLOD) cloud. Finally, use cases from natural language processing and corpus querying were described. © Springer International Publishing Switzerland 2016.",Final,,
Strobin L.; Niewiadomski A.,Integration of multiple graph datasets and their linguistic summaries: An application to linked data,1,1,-1,#excluded #outofscope #noLLOD linguistic summary of LOD,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976632384&doi=10.1007%2f978-3-319-39378-0_29&partnerID=40&md5=ebfbd84b45f6a78f961d2d2942a121dd,"This paper presents a novel method of generating and evaluating linguistic summaries of content stored in distributed graph datasets, like Linked Data. Linguistic summarization is a well known data mining technique, aimed to discover patterns in data and present them in natural language. So far, this method has been researched only for relational databases. In our recent paper we have presented how to adapt this method for graph datasets. We have solved the problems of subject definition (further extended in this paper), retrieval of the attributes for summarization, generalization of summarizers and qualifiers. In this paper we extend that research by adapting proposed method to distributed interlinked graph datasets, which results in obtaining new summaries, and therefore new knowledge. We discuss how to follow different types of equivalence links that may exists between graph datasets. In order to measure characteristics specific for summaries of distributed graph data we propose new truth values (degree of subject appropriateness, degree of summarizer order and degree of linkage), and adapt existing ones (degree of covering). We run several experiments on Linked Data and discuss the results. © Springer International Publishing Switzerland 2016.",Final,,
Nagvenkar A.; Pawar J.; Bhattacharyya P.,IndoWordNet conversion to web ontology language (OWL),1,1,1,#kg,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962799579&partnerID=40&md5=5551ce812d53615f1172cff07c66db34,"WordNet plays a significant role in Linked Open Data (LOD) cloud. It has numerous application ranging from ontology annotation to ontology mapping. IndoWord-Net is a linked WordNet connecting 18 Indian language WordNets with Hindi as a source WordNet. The Hindi WordNet was initially developed by linking it to English WordNet. In this paper, we present a data representation of IndoWordNet in Web Ontology Language (OWL). The schema of Princeton WordNet has been enhanced to support the representation of IndoWordNet. This IndoWordNet representation in OWL format is now available to link other web resources. This representation is implemented for eight Indian languages.",Final,,
Chiarcos C.; Sukhareva M.,OLiA - Ontologies of Linguistic Annotation,1,1,1,#kg #ontology,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929329936&doi=10.3233%2fSW-140167&partnerID=40&md5=f770d89e6ffb85c9b28c8ddb2dea89d3,"This paper describes the Ontologies of Linguistic Annotation (OLiA) as one of the data sets currently available as part of Linguistic Linked Open Data (LLOD) cloud. Within the LLOD cloud, the OLiA ontologies serve as a reference hub for annotation terminology for linguistic phenomena on a great band-width of languages, they have been used to facilitate interoperability and information integration of linguistic annotations in corpora, NLP pipelines, and lexical-semantic resources and mediate their linking with multiple community-maintained terminology repositories. © 2015 - IOS Press and the authors. All rights reserved.",Final,,
Çağdaş V.; Stubkjær E.,A SKOS vocabulary for Linked Land Administration: Cadastre and Land Administration Thesaurus,1,,-1,#excluded #outofscope,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948985186&doi=10.1016%2fj.landusepol.2014.12.017&partnerID=40&md5=fc7d3a37a9ebc9d1f9b18c1e74b1db38,"Intensification of international communication calls for multilingual terminology databases and other linguistic tools. The World Wide Web Consortium framed the further specification of the Simple Knowledge Organization System (SKOS), which is a formal language designed for standardized representation of structured vocabularies, as well as the Linked Data methodology of publishing structured data in a machine-readable and interlinked way, that they become more useful. Applying the Linked Data approach, the European Commission recently established e-Government Core Vocabularies. This paper describes the development of a Knowledge Organization System (KOS) in terms of a thesaurus for the domain of cadastre and land administration. The main purpose is to contribute towards the development of 'Linked Land Administration' that adopts Linked Data technologies for semantic management of datasets kept in public registries, and scholarly and legislative resources kept in libraries. The proposed controlled vocabulary in SKOS format may be used for specifying metadata records of scholarly and legislative resources, as well as enrichment of these resources through semantic annotations. It also provides a base for further ontology development initiatives. The thesaurus is mainly derived from terms of the standard ISO 19152:2012 Land Administration Domain Model, which represents the static aspect of the domain. Reports of the project Property Formation in the Nordic countries (Kort og Matrikelstyrelsen, 2006) provide the basis for the temporal or activity aspect of the domain, while the ANSI/NISO, 2005 Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies provided methodological advice. The thesaurus consists of 143 terms, the relations of which are recorded according to the mentioned SKOS standard. About one fourth of the terms are adopted from existing thesauri, including the AGROVOC multilingual agricultural vocabulary, the GEMET Thesaurus with INSPIRE Spatial Data Themes, and the STW Thesaurus for Economics. © 2015 Elsevier Ltd.",Final,,
Winiwarter W.,JAMRED - A Japanese abstract meaning representation EDitor,1,1,1,#use WordNet,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967225539&doi=10.1145%2f2837185.2837246&partnerID=40&md5=eae858b0144bbc20b9a769a5da793aa3,"Meaning Representation (AMR) has emerged recently as one of the most influential formalisms for the semantic representation of natural language. In this paper, we present a Web-based environment for the annotation of Japanese Web pages using augmented browsing and logic programming as the two key enabling technologies. We have developed a visual AMR editor strongly integrated into our Web-based Japanese language learning environment, which hides most of the complexities of AMR syntax from the user and extends it by grounding concepts through links to WordNet synsets and DBpedia resources. The resulting tool can be used meaningfully for several language engineering tasks, in particular, language learning, corpus annotation, and translation. © 2015 ACM.",Final,,
Wang C.; Gao M.; He X.; Zhang R.,Challenges in Chinese knowledge graph construction,1,1,-1,#excluded #outofscope no resource is made available,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944313968&doi=10.1109%2fICDEW.2015.7129545&partnerID=40&md5=00015070afabe392d68409a6f4b2bd71,"The automatic construction of large-scale knowledge graphs has received much attention from both academia and industry in the past few years. Notable knowledge graph systems include Google Knowledge Graph, DBPedia, YAGO, NELL, Probase and many others. Knowledge graph organizes the information in a structured way by explicitly describing the relations among entities. Since entity identification and relation extraction are highly depending on language itself, data sources largely determine the way the data are processed, relations are extracted, and ultimately how knowledge graphs are formed, which deeply involves the analysis of lexicon, syntax and semantics of the content. Currently, much progress has been made for knowledge graphs in English language. In this paper, we discuss the challenges facing Chinese knowledge graph construction because Chinese is significantly different from English in various linguistic perspectives. Specifically, we analyze the challenges from three aspects: data sources, taxonomy derivation and knowledge extraction. We also present our insights in addressing these challenges. © 2015 IEEE.",Final,,
Winiwarter W.,JILL - Japanese incidental language learning,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967145274&doi=10.1145%2f2837185.2837191&partnerID=40&md5=57f44e22da2b1e839b9e93bfae6dced2,"We present aWeb-based environment for incidental learning of Japanese. As the two key enabling technologies we use augmented browsing at the client and logic programming at the server. This way we can create a learning experience which blends foreign language acquisition with the user's everyday browsing activities. Main features presented in this paper are an intuitive visual presentation of sentence structures, comprehensive support at the word level by integrating several lexical resources including DBpedia, and multiple customization options. © 2015 ACM.",Final,,
Krause S.; Hennig L.; Gabryszak A.; Xu F.; Uszkoreit H.,Sar-graphs: A Linked Linguistic Knowledge Resource Connecting Facts with Language,1,1,-1,#excluded #old superato da articolo più recente del 2016,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120063260&partnerID=40&md5=a16d1a7609391717447bfb96366186ec,"We present sar-graphs, a knowledge resource that links semantic relations from factual knowledge graphs to the linguistic patterns with which a language can express instances of these relations. Sar-graphs expand upon existing lexicosemantic resources by modeling syntactic and semantic information at the level of relations, and are hence useful for tasks such as knowledge base population and relation extraction. We present a languageindependent method to automatically construct sar-graph instances that is based on distantly supervised relation extraction. We link sar-graphs at the lexical level to BabelNet, WordNet and UBY, and present our ongoing work on pattern- and relationlevel linking to FrameNet. An initial dataset of English sar-graphs for 25 relations is made publicly available, together with a Java-based API. © 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing.",Final,,
Presutti V.; Nuzzolese A.G.; Consoli S.; Gangemi A.; Recupero D.R.,From hyperlinks to Semantic Web properties using Open Knowledge Extraction,1,1,1,#use,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971554079&doi=10.3233%2fSW-160221&partnerID=40&md5=ac1d14b2b6bd082d9df1734161891fb7,"Open information extraction approaches are useful but insufficient alone for populating the Web with machine readable information as their results are not directly linkable to, and immediately reusable from, other Linked Data sources. This work proposes a novel paradigm, named Open Knowledge Extraction, and its implementation (Legalo) that performs unsupervised, open domain, and abstractive knowledge extraction from text for producing machine readable information. The implemented method is based on the hypothesis that hyperlinks (either created by humans or knowledge extraction tools) provide a pragmatic trace of semantic relations between two entities, and that such semantic relations, their subjects and objects, can be revealed by processing their linguistic traces (i.e. the sentences that embed the hyperlinks) and formalised as Semantic Web triples and ontology axioms. Experimental evaluations conducted on validated text extracted from Wikipedia pages, with the help of crowdsourcing, confirm this hypothesis showing high performances. © 2016 - IOS Press and the authors.",Final,All Open Access; Green Open Access,
Aspillaga C.; Mendoza M.; Soto A.,Inspecting the concept knowledge graph encoded by modern language models,1,1,-1,#excluded #noLLOD ,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115697837&partnerID=40&md5=b3da3ae1bde188fa9f14b21fb13d8bec,"The field of natural language understanding has experienced exponential progress in the last few years, with impressive results in several tasks. This success has motivated researchers to study the underlying knowledge encoded by these models. Despite this, attempts to understand their semantic capabilities have not been successful, often leading to non-conclusive, or contradictory conclusions among different works. Via a probing classifier, we extract the underlying knowledge graph of nine of the most influential language models of the last years, including word embeddings, text generators, and context encoders. This probe is based on concept relatedness, grounded on WordNet. Our results reveal that all the models encode this knowledge, but suffer from several inaccuracies. Furthermore, we show that the different architectures and training strategies lead to different model biases. We conduct a systematic evaluation to discover specific factors that explain why some concepts are challenging. We hope our insights will motivate the development of models that capture concepts more precisely. © 2021 Association for Computational Linguistics",Final,,
Carvalho S.; Roche C.; Costa R.,Ontologies for terminological purposes: The EndoTerm project,1,1,-1,#excluded no resource is made available,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955246750&partnerID=40&md5=22484d2fa7c52b510c2786ba83f281b8,"In today's digital society, characterized by the Semantic Web and by Linked Data, ontologies, in the sense of Knowledge Engineering, have paved the way for new perspectives for Terminology, namely in what concerns the operationalization of terminological products. The collaborative work involving Terminology and ontologies has led to the emergence of new theoretical perspectives, one of which being Ontoterminology. This approach aims to reconcile Terminology's linguistic and conceptual dimensions whilst maintaining their fundamental differences and, in addition, enables the construction of a computer-readable representation of a given conceptualization. Bearing this in mind, this paper presents the EndoTerm project, a multilingual resource within the medical domain - with <Endometriosis> as the core concept - that comprises both verbal and nonverbal representations and that can be computationally represented and manipulated. The presentation of micro-concept systems based on these verbal and non-verbal representations will support a reflection upon the role of the latter in terminology work.",Final,,
McCrae J.P.; Cimiano P.; Matteis L.; Navigli R.; Doncel V.R.; Vila-Suero D.; Gracia J.; Abele A.; Vulcu G.; Buitelaar P.,Reconciling Heterogeneous Descriptions of Language Resources,1,1,1,#use #reconciliation,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037062941&partnerID=40&md5=ed7fb332868691d899ab1410be3ce03d,"Language resources are a cornerstone of linguistic research and for the development of natural language processing tools, but the discovery of relevant resources remains a challenging task. This is due to the fact that relevant metadata records are spread among different repositories and it is currently impossible to query all these repositories in an integrated fashion, as they use different data models and vocabularies. In this paper we present a first attempt to collect and harmonize the metadata of different repositories, thus making them queriable and browsable in an integrated way. We make use of RDF and linked data technologies for this and provide a first level of harmonization of the vocabularies used in the different resources by mapping them to standard RDF vocabularies including Dublin Core and DCAT. Further, we present an approach that relies on NLP and in particular word sense disambiguation techniques to harmonize resources by mapping values of attributes - such as the type, license or intended use of a resource - into normalized values. Finally, as there are duplicate entries within the same repository as well as across different repositories, we also report results of detection of these duplicates. © 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing.",Final,,
Lee C.-Y.; Hsieh S.-K.,Linguistic Linked Data in Chinese: The Case of ChineseWordNet,1,1,1,#kg #chinese #WordNet,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031283009&partnerID=40&md5=c25ab3bedbba36373c64117925801a98,"The present study describes recent developments of Chinese WordNet, which has been reformatted using the lemon model and published as part of the Linguistic Linked Open Data Cloud. While lemon suffices for modeling most of the structures in Chinese WordNet at the lexical level, the model does not allow for finergrained distinction of a word sense, or meaning facets, a linguistic feature also attended to in Chinese WordNet. As for the representation of synsets, we use the WordNet RDF ontology for integration's sake. Also, we use another ontology proposed by the Global WordNet Association to show how Chinese WordNet as Linked Data can be integrated into the Global WordNet Grid. © 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing.",Final,,
Villegas M.; Bel N.,PAROLE/SIMPLE 'lemon' ontology and lexicons,1,1,1,#kg #ontology,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929303692&doi=10.3233%2fSW-140148&partnerID=40&md5=6f9894bba1bf28153bb538a863672ff5,"The PAROLE/SIMPLE 'lemon' Ontology and Lexicon are the OWL/RDF version of the PAROLE/SIMPLE lexicons (defined during the PAROLE (LE2-4017) and SIMPLE (LE4-8346) IV FP EU projects) once mapped onto lemon model and LexInfo ontology. Original PAROLE/SIMPLE lexicons contain morphological, syntactic and semantic information, organized according to a common model and to common linguistic specifications for 12 European languages. The data set we describe includes the PAROLE/SIMPLE model mapped to lemon and LexInfo ontology and the Spanish & Catalan lexicons. All data are published in the Data Hub and are distributed under CC Attribution 3.0 Unported license. The Spanish lexicon contains 199466 triples and 7572 lexical entries fully annotated with syntactic and semantic information. The Catalan lexicon contains 343714 triples and 20545 lexical entries annotated with syntactic information half of which are also annotated with semantic information. In this paper we describe the resulting data, the mapping process and the benefits obtained. We demonstrate that the Linked Open Data principles prove essential for datasets such as original PAROLE/SIMPLE lexicons where harmonization and interoperability were crucial. The resulting data is lighter and better suited for exploitation. In addition, it facilitates further extensions and linking to external resources such as WordNet, lemonUby, DBpedia etc. © 2015 - IOS Press and the authors. All rights reserved.",Final,All Open Access; Green Open Access,
Wang C.; Song Y.; El-Kishky A.; Roth D.; Zhang M.; Han J.,Incorporating world knowledge to document clustering via heterogeneous information networks,1,1,-1,#excluded #noLLOD,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954104008&doi=10.1145%2f2783258.2783374&partnerID=40&md5=5c28b32026aa7202d6b17bfb9cb64535,"One of the key obstacles in making learning protocols realistic in applications is the need to supervise them, a costly process that often requires hiring domain experts. We consider the framework to use the world knowledge as indirect supervision. World knowledge is general-purpose knowledge, which is not designed for any specific domain. Then the key challenges are how to adapt the world knowledge to domains and how to represent it for learning. In this paper, we provide an example of using world knowledge for domain dependent document clustering. We provide three ways to specify the world knowledge to domains by resolving the ambiguity of the entities and their types, and represent the data with world knowledge as a heterogeneous information network. Then we propose a clustering algorithm that can cluster multiple types and incorporate the sub-type information as constraints. In the experiments, we use two existing knowledge bases as our sources of world knowledge. One is Freebase, which is collaboratively collected knowledge about entities and their organizations. The other is YAGO2, a knowledge base automatically extracted from Wikipedia and maps knowledge to the linguistic knowledge base, WordNet. Experimental results on two text benchmark datasets (20news-groups and RCV1) show that incorporating world knowledge as indirect supervision can significantly outperform the state-of-the-art clustering algorithms as well as clustering algorithms enhanced with world knowledge features.",Final,All Open Access; Green Open Access,
Cheniki N.; Belkhir A.; Atif Y.,Supporting multilingual semantic Web services discovery by consuming data from DBpedia knowledge base,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959933874&doi=10.1145%2f2816839.2816862&partnerID=40&md5=a79701a17e143e5939ac809da64d1d98,"The Web is becoming a truly multilingual hub in which speakers from different languages and cultures are producing, interacting and consuming information. Web services are an essential part of the Web which nowadays attract more and more people around the world. So, supporting multilingual Web services discovery is a crucial goal to achieve, so that providers and users publish or consume Web services independently from their culture and native language. In this paper, we propose to overcome language barrier by supporting multilingual Web services discovery using DB-pedia, which is a cross-domain multilingual knowledge base. DBpedia is used to annotate services with semantic entities called resources as well as their categories and types. We take advantage of semantic and multilingual information provided by DBpedia to enable cross-language semantic Web services discovery. Implementation shows that DBpedia offers a valuable information source to achieve our goal. © 2015 ACM.",Final,,
Mohamed R.; El-Makky N.M.; Nagi K.,ArabRelat: Arabic relation extraction using distant supervision,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961176131&doi=10.5220%2f0005636604100417&partnerID=40&md5=2dd482bce31d0abcd1362f835dce4eb3,"Relation Extraction is an important preprocessing task for a number of text mining applications, including: Information Retrieval, Question Answering, Ontology building, among others. In this paper, we propose a novel Arabic relation extraction method that leverages linguistic features of the Arabic language in Web data to infer relations between entities. Due to the lack of labeled Arabic corpora, we adopt the idea of distant supervision, where DBpedia, a large database of semantic relations extracted from Wikipedia, is used along with a large unlabeled text corpus to build the training data. We extract the sentences from the unlabeled text corpus, and tag them using the corresponding DBpedia relations. Finally, we build a relation classifier using this data which predicts the relation type of new instances. Our experimental results show that the system reaches 70% for the F-measure in detecting relations. Copyright © 2015 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",Final,,
Westphal P.; Stadler C.; Pool J.,Countering language attrition with PanLex and the Web of Data,1,1,1,#kg,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929329731&doi=10.3233%2fSW-140138&partnerID=40&md5=57e748dd4fa77890501020cad958f43b,"The world is losing some of its 7,000 languages. Hypothesizing that language attrition might subside if all languages were intertranslatable, the PanLex project supports panlingual lexical translation by integrating all known lexical translations. Semantic Web technologies can flexibly represent and reason with the content of its database and interlink it with linguistic and other resources and annotations. Conversely, PanLex, with its collection of translation links between more than a billion pairs of lexemes from more than 9,000 language varieties, can improve the coverage of the Linguistic Web of Data. We detail how we transformed the content of the PanLex database to RDF, established conformance with the lemon and GOLD data models, interlinked it with Lexvo and DBpedia, and published it as Linked Data and via SPARQL. © 2015 - IOS Press and the authors. All rights reserved.",Final,,
Declerck T.; Wandl-Vogt E.; Krek S.; Tiberius C.,Towards multilingual elexicography by means of linked (Open) Data,1,1,1,#ontology #kg,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962264489&partnerID=40&md5=88d3e39003b509bd4dc9ad338b93ef2b,"In this short paper, we document the current state of work consisting in mapping various lexicographic resources onto the OntoLex model, which is an OWL and RDF(s) based representation format. This model has been designed in the context of a W3C Community Group effort for supporting the publication of linguistic data in the Linked (Open) Data cloud. The deployment of OntoLex is currently being tested within the ISCH COST Action IS1305 European Network of e-Lexicography (ENeL), which is adapting to the field of digital lexicography guidelines that have been suggested by the LIDER FP7 Support Action.",Final,,
Dragoni M.; Tettamanzi A.G.B.; da Costa Pereira C.,Propagating and Aggregating Fuzzy Polarities for Concept-Level Sentiment Analysis,1,1,1,#use #sentimentanalysis  WordNet and senticnet,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926322258&doi=10.1007%2fs12559-014-9308-6&partnerID=40&md5=76b58e9064aa0df3998923e453fa0fa6,"An emerging field within sentiment analysis concerns the investigation about how sentiment polarities associated with concepts have to be adapted with respect to the different domains in which they are used. In this paper, we explore the use of fuzzy logic for modeling concept polarities, and the uncertainty associated with them, with respect to different domains. The approach is based on the use of a knowledge graph built by combining two linguistic resources, namely WordNet and SenticNet. Such a knowledge graph is then exploited by a graph-propagation algorithm that propagates sentiment information learned from labeled datasets. The system implementing the proposed approach has been evaluated on the Blitzer dataset. The results demonstrate its viability in real-world cases. © 2014, Springer Science+Business Media New York.",Final,All Open Access; Green Open Access,
Álvarez A.A.,"Enriching Digitized Medieval Manuscripts: Linking Image, Text and Lexical Knowledge",1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122503695&partnerID=40&md5=a7452c56d9465f76100490ec9185d749,"This paper describes an on-going project of transcribing and annotating digitized manuscripts of medieval Spanish with paleographic and lexical information. We link lexical units from the manuscripts with the Multilingual Central Repository (MCR), making terms retrievable by any of the languages that integrate MCR. The goal of the project is twofold: creating a paleographic knowledge base from digitized medieval facsimiles, that will allow paleographers, philologist, historical linguist, and humanities scholars in general, to analyze and retrieve graphemic, lexical and textual information from historical documents; and on the other hand, developing machine readable documents that will link image representations of graphemic and lexical units in a facsimile with Linked Open Data resources. This paper concentrates on the encoding and cross-linking procedures. c 2015 Association for Computational Linguistics and The Asian Federation of Natural Language Processing. © 2015 Proceedings of the Annual Meeting of the Association for Computational Linguistics.",Final,,
Celikyilmaz A.; Hakkani-Tiir D.; Pasupat P.; Sarikaya R.,Enriching word embeddings using knowledge graph for semantic tagging in conversational dialog systems,1,1,-1,#excluded,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987629985&partnerID=40&md5=deb001215138bdab456b70f41603852e,"Unsupervised word embeddings provide rich linguistic and conceptual information about words. However, they may provide weak information about domain specific semantic relations for certain tasks such as semantic parsing of natural language queries, where such information about words can be valuable. To encode the prior know ledge about the semantic word relations, we present new method as follows: we extend the neural network based lexical word embedding objective function (Mikolov et al. 2013) by incorporating the information about relationship between entities that we extract from knowledge bases. Our model can jointly learn lexical word representations from free text enriched by the relational word embeddings from relational data (e.g. Freebase) for each type of entity relations. We empirically show on the task of semantic tagging of natural language queries that our enriched embeddings can provide information about not only short-range syntactic dependencies but also long-range semantic dependencies between words. Using the enriched embeddings, we obtain an average of 2% improvement in F-score compared to the previous baselines. Copyright © 2015. Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",Final,,
De Melo G.,Lexvo.org: Language-related information for the Linguistic Linked Data cloud,1,1,1,#kg,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929309290&doi=10.3233%2fSW-150171&partnerID=40&md5=f37c88e076c88f5831122a3041ca8ed0,"Lexvo.org brings information about languages, words, and other linguistic entities to the Web of Linked Data. It defines URIs for terms, languages, scripts, and characters, which are not only highly interconnected but also linked to a variety of resources on the Web. Additionally, new datasets are being published to contribute to the emerging Linked Data Cloud of Language-Related information. © 2015 - IOS Press and the authors. All rights reserved.",Final,,
Siemoneit B.; McCrae J.P.; Cimiano P.,Linking four heterogeneous language resources as linked data,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009269613&partnerID=40&md5=dfbe34f7414893e5c07f0c81c0ec3ccb,"The interest in publishing language resources as linked data is increasing, as clearly corroborated by the recent growth of the Linguistic Linked Data cloud. However, the actual value of data published as linked data is the fact that it is linked across datasets, supporting integration and discovery of data. As the manual creation of links between datasets is costly and therefore does not scale well, automatic linking approaches are of great importance to increase the quality and degree of linking of the Linguistic Linked Data cloud. In this paper we examine an automatic approach to link four different datasets to each other: Two terminologies, the European Migration Network (EMN) glossary as well as the Interactive Terminology for Europe (IATE), BabelNet, and the Manually Annotated Subcorpus (MASC) of the American National Corpus. We describe our methodology, present some results on the quality of the links and summarize our experiences with this small linking exercise We will make sure that the resources are added to the linguistic linked data cloud. © 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing.",Final,,
Sasaki F.; Gornostay T.; Dojchinovski M.; Osella M.; Mannens E.; Stoitsis G.; Ritchie P.; Koidl K.,Introducing FREME: Deploying linguistic linked data,1,1,-1,#excluded #outofscope,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962281732&partnerID=40&md5=5e4040102e2f6f4b16843b81622ecc55,"This paper introduces the FREME project, a new Horizon 2020 innovation action. It aims at building an open framework of e-Services for multilingual and semantic enrichment of digital content, based on a reusable set of open Application Programme Interfaces and Graphical User Interfaces to FREME enrichment services. In addition, the paper discusses how the project deploys Linguistic Linked Data (LLD), especially existing LLD resources, LLD best practices and the LLD reference architecture.",Final,,
Chen Y.-N.; Wang W.Y.; Rudnicky A.I.,Jointly modeling inter-slot relations by random walk on knowledge graphs for unsupervised spoken language understanding,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959178594&doi=10.3115%2fv1%2fn15-1064&partnerID=40&md5=063c7e0df5060b318024b813d9264f66,"A key challenge of designing coherent semantic ontology for spoken language understanding is to consider inter-slot relations. In practice, however, it is difficult for domain experts and professional annotators to define a coherent slot set, while considering various lexical, syntactic, and semantic dependencies. In this paper, we exploit the typed syntactic dependency theory for unsupervised induction and filling of semantics slots in spoken dialogue systems. More specifically, we build two knowledge graphs: a slot-based semantic graph, and a word-based lexical graph. To jointly consider word-to-word, word-toslot, and slot-to-slot relations, we use a random walk inference algorithm to combine the two knowledge graphs, guided by dependency grammars. The experiments show that considering inter-slot relations is crucial for generating a more coherent and compete slot set, resulting in a better spoken language understanding model, while enhancing the interpretability of semantic slots. © 2015 Association for Computational Linguistics.",Final,All Open Access; Green Open Access; Hybrid Gold Open Access,
Eckle-Kohler J.; McCrae J.P.; Chiarcos C.,"LemonUby - A large, interlinked, syntactically-rich lexical resource for ontologies",1,1,1,#kg,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929333600&doi=10.3233%2fSW-140159&partnerID=40&md5=9810242a0d59db7a8d89f50f37462b9f,"We introduce lemonUby, a new lexical resource integrated in the Semantic Web which is the result of converting data extracted from the existing large-scale linked lexical resource UBY to the lemon lexicon model. The following data from UBY were converted: WordNet, FrameNet, VerbNet, English and German Wiktionary, the English and German entries of OmegaWiki, as well as links between pairs of these lexicons at the word sense level (links between VerbNet and FrameNet, VerbNet and WordNet, WordNet and FrameNet, WordNet and Wiktionary, WordNet and German OmegaWiki). We linked lemonUby to other lexical resources and linguistic terminology repositories in the Linguistic Linked Open Data cloud and outline possible applications of this new dataset. © 2015 - IOS Press and the authors. All rights reserved.",Final,,
Sérasset G.,DBnary: Wiktionary as a Lemon-based multilingual lexical resource in RDF,1,1,1,#kg,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929331267&doi=10.3233%2fSW-140147&partnerID=40&md5=88747477c7725eef8b8640fb49224648,"Contributive resources, such as Wikipedia, have proved to be valuable to Natural Language Processing or multilingual Information Retrieval applications. This work focusses on Wiktionary, the dictionary part of the resources sponsored by the Wikimedia foundation. In this article, we present our extraction of multilingual lexical data from Wiktionary data and to provide it to the community as a Multilingual Lexical Linked Open Data (MLLOD). This lexical resource is structured using the LEMON Model. This data, called DBnary, is registered at http://thedatahub.org/dataset/dbnary. © 2015 - IOS Press and the authors. All rights reserved.",Final,,
Li J.; Wang C.; He X.; Zhang R.; Gao M.,User generated content oriented Chinese taxonomy construction,1,1,-1,#excluded #outofscope,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950238015&doi=10.1007%2f978-3-319-25255-1_51&partnerID=40&md5=810c7d0fdc664a6192eb1e6b6993dd03,"The taxonomy is one of the basic components in knowledge graphs as it establishes types of classes and semantic relations among the classes. Taxonomies are normally constructed either manually, or by language-dependent rules or patterns for type and relation extraction or inference. Existing work on building taxonomies for knowledge graphs is mostly in English language environment. In this paper, we propose a novel approach for large-scale Chinese taxonomy construction based on user generated content. We take Chinese Wikipedia as the data source, develop methods to extract classes and their relations mined from user tagged categories, and build up the taxonomy using a bottom-up strategy. The algorithms can be easily applied to other Wiki-style data sources. The experiments show that the constructed Chinese taxonomy achieves better results in both quality and quantity. © Springer International Publishing Switzerland 2015.",Final,,
Rouces J.; De Melo G.; Hose K.,Framebase: Representing N-ary relations using semantic frames,1,1,1,#kg,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937485067&doi=10.1007%2f978-3-319-18818-8_31&partnerID=40&md5=91c42249bc74eb30360db366db046087,"Large-scale knowledge graphs such as those in the Linked Data cloud are typically represented as subject-predicate-object triples. However, many facts about the world involve more than two entities. While n-ary relations can be converted to triples in a number of ways, unfortunately, the structurally different choices made in different knowledge sources significantly impede our ability to connect them. They also make it impossible to query the data concisely and without prior knowledge of each individual source. We present FrameBase, a wide-coverage knowledge-base schema that uses linguistic frames to seamlessly represent and query n-ary relations from other knowledge bases, at different levels of granularity connected by logical entailment. It also opens possibilities to draw on natural language processing techniques for querying and data mining. © Springer International Publishing Switzerland 2015.",Final,All Open Access; Bronze Open Access,
Arcan M.; Turchi M.; Buitelaar P.,Knowledge portability with semantic expansion of ontology labels,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943746735&doi=10.3115%2fv1%2fp15-1069&partnerID=40&md5=cb59323a8b2b76877cf4445da028a55e,"Our research focuses on the multilingual enhancement of ontologies that, often represented only in English, need to be translated in different languages to enable knowledge access across languages. Ontology translation is a rather different task then the classic document translation, because ontologies contain highly specific vocabulary and they lack contextual information. For these reasons, to improve automatic ontology translations, we first focus on identifying relevant unambiguous and domain-specific sentences from a large set of generic parallel corpora. Then, we leverage Linked Open Data resources, such as DBPedia, to isolate ontologyspecific bilingual lexical knowledge. In both cases, we take advantage of the semantic information of the labels to select relevant bilingual data with the aim of building an ontology-specific statistical machine translation system. We evaluate our approach on the translation of a medical ontology, translating from English into German. Our experiment shows a significant improvement of around 3 BLEU points compared to a generic as well as a domain-specific translation approach. © 2015 Association for Computationl Linguisticss.",Final,All Open Access; Green Open Access; Hybrid Gold Open Access,
Hauck P.; Braga R.; Campos F.; Torrent T.; Matos E.; David J.M.N.,Supporting FrameNet Project with Semantic Web technologies,1,1,1,#kg,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944338891&partnerID=40&md5=55195632f450d4c300e56691024dd0c5,"FrameNet Project is being developed by ICSI at Berkeley, with the goal of documenting the English language lexicon based on Frame Semantics. For Brazilian Portuguese, the FrameNet-Br Project, hosted at UFJF, follows the same theoretical and methodological perspective. This work presents a service-based infrastructure that combines Semantic Web technologies with FrameNet-like databases, by considering the hypothesis that the application of technologies such as ontologies, linked data, and web services can contribute to build and reuse lexical resources based on Frame Semantics. The contributions are related to enriched semantics, data reliability and natural language processing.",Final,,
Perera R.; Nand P.,A multi-strategy approach for lexicalizing linked open data,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942510700&doi=10.1007%2f978-3-319-18117-2_26&partnerID=40&md5=377178b92a49433985cfee371f267073,"This paper aims at exploiting Linked Data for generating natural text, often referred to as lexicalization. We propose a framework that can generate patterns which can be used to lexicalize Linked Data triples. Linked Data is structured knowledge organized in the form of triples consisting of a subject, a predicate and an object. We use DBpedia as the Linked Data source which is not only free but is currently the fastest growing data source organized as Linked Data. The proposed framework utilizes the Open Information Extraction (OpenIE) to extract relations from natural text and these relations are then aligned with triples to identify lexicalization patterns. We also exploit lexical semantic resources which encode knowledge on lexical, semantic and syntactic information about entities. Our framework uses VerbNet and WordNet as semantic resources. The extracted patterns are ranked and categorized based on the DBpedia ontology class hierarchy. The pattern collection is then sorted based on the score assigned and stored in an index embedded database for use in the framework as well as for future lexical resource. The framework was evaluated for syntactic accuracy and validity by measuring the Mean Reciprocal Rank (MRR) of the first correct pattern. The results indicated that framework can achieve 70.36% accuracy and a MRR value of 0.72 for five DBpedia ontology classes generating 101 accurate lexicalization patterns. © Springer International Publishing Switzerland 2015.",Final,,
Li Q.; Liu S.; Lin R.; Li M.; Zhou M.,Entity translation with collective inference in knowledge graph,1,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951275676&doi=10.1007%2f978-3-319-25207-0_5&partnerID=40&md5=5981a3d872977177e02b88848fe0a77c,"Nowadays knowledge base (KB) has been viewed as one of the important infrastructures for many web search applications and NLP tasks. However, in practice the availability of KB data varies from language to language, which greatly limits potential usage of knowledge base. In this paper, we propose a novel method to construct or enrich a knowledge base by entity translation with help of another KB but compiled in a different language. In our work, we concentrate on two key tasks: 1) collecting translation candidates with as good coverage as possible from various sources such as web or lexicon; 2) building an effective disambiguation algorithm based on collective inference approach over knowledge graph to find correct translation for entities in the source knowledge base. We conduct experiments on movie domain of our inhouse knowledge base from English to Chinese, and the results show the proposed method can achieve very high translation precision compared with classical translation methods, and significantly increase the volume of Chinese knowledge base in this domain. © Springer International Publishing Switzerland 2015.",Final,,
Gracia J.; Vila-Suero D.; McCrae J.P.; Flati T.; Baron C.; Dojchinovski M.,Language resources and linked data: A practical perspective,1,,-1,#related,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928557666&doi=10.1007%2f978-3-319-17966-7_1&partnerID=40&md5=090f916d8a33c0466d970b75c8caa41d,"Recently, experts and practitioners in language resources have started recognizing the benefits of the linked data (LD) paradigm for the representation and exploitation of linguistic data on the Web. The adoption of the LD principles is leading to an emerging ecosystem of multilingual open resources that conform to the Linguistic Linked Open Data Cloud, in which datasets of linguistic data are interconnected and represented following common vocabularies, which facilitates linguistic information discovery, integration and access. In order to contribute to this initiative, this paper summarizes several key aspects of the representation of linguistic information as linked data from a practical perspective. The main goal of this document is to provide the basic ideas and tools for migrating language resources (lexicons, corpora, etc.) as LD on the Web and to develop some useful NLP tasks with them (e.g., word sense disambiguation). Such material was the basis of a tutorial imparted at the EKAW’14 conference, which is also reported in the paper. © Springer International Publishing Switzerland 2015.",Final,All Open Access; Green Open Access,
Ustalov D.A.,Russian thesauri as linked open data,1,1,1,#kg #russian,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952764996&partnerID=40&md5=96d23c7a151c13f0d982a031ae398084,"Open linguistic data is a good recently established trend allowing both researchers and developers in the field of natural language processing to create their own applications using high-quality dictionaries, thesauri, corpora, etc. At the same time, the published open data are stored in different formats making them difficult to be used in an efficient way without falling within vendor lock-in. This paper is devoted to the problem of representing popular lexical resources of the Russian language in the form of Linked Open Data. It summarizes the recent work in the field of thesauri representation formats and approaches to converting such formats to those of Linked Data. It also proposes an approach to converting popular Russian thesauri to the vocabularies that are the essential parts of the Linguistic Linked Open Data Cloud. The proposed approach has been implemented in open source software and the resulted dataset has been made publicly available on NLPub in the Turtle format under the terms of a Creative Commons license.",Final,,
Ehrmann M.; Cecconi F.; Vannella D.; McCrae J.; Cimiano P.; Navigli R.,Representing multilingual data as linked data: The case of BabelNet 2.0,1,1,-1,#excluded outdated contribution,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988951977&partnerID=40&md5=34d7ad7dfd105e42f06a0441c1f7051a,"Recent years have witnessed a surge in the amount of semantic information published on the Web. Indeed, the Web of Data, a subset of the Semantic Web, has been increasing steadily in both volume and variety, transforming the Web into a 'global database' in which resources are linked across sites. Linguistic fields - in a broad sense - have not been left behind, and we observe a similar trend with the growth of linguistic data collections on the so-called 'Linguistic Linked Open Data (LLOD) cloud'. While both Semantic Web and Natural Language Processing communities can obviously take advantage of this growing and distributed linguistic knowledge base, they are today faced with a new challenge, i.e., that of facilitating multilingual access to the Web of data. In this paper we present the publication of BabelNet 2.0, a wide-coverage multilingual encyclopedic dictionary and ontology, as Linked Data. The conversion made use of lemon, a lexicon model for ontologies particularly well-suited for this enterprise. The result is an interlinked multilingual (lexical) resource which can not only be accessed on the LOD, but also be used to enrich existing datasets with linguistic information, or to support the process of mapping datasets across languages.",Final,,
Chiarcos C.; Sukhareva M.; Mittmann R.; Price T.; Chobotsky J.; Detmold G.,New technologies for old Germanic. Resources and research on parallel bibles in older continental western Germanic,1,1,1,#kg,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964029453&partnerID=40&md5=1b0b40cff472a8438b40b1e0b5bb83c7,"We provide an overview of on-going efforts to facilitate the study of older Germanic languages currently pursued at the Goethe-University Frankfurt, Germany. We describe created resources, such as a parallel corpus of Germanic Bibles and a morphosyntactically annotated corpus of Old High German (OHG) and Old Saxon, a lexicon of OHG in XML and a multilingual etymological database. We discuss NLP algorithms operating on this data, and their relevance for research in the Humanities. RDF and Linked Data represent new and promising aspects in our research, currently applied to establish cross-references between etymological dictionaries, infer new information from their symmetric closure and to formalize linguistic annotations in a corpus and grammatical categories in a lexicon in an interoperable way. © 2014 Association for Computational Linguistics.",Final,,
Chen Y.; Skiena S.,Building sentiment lexicons for all major languages,1,1,1,#use,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906927782&doi=10.3115%2fv1%2fp14-2063&partnerID=40&md5=21feca95c076bbfb9e61213e4383cbf7,"Sentiment analysis in a multilingual world remains a challenging problem, because developing language-specific sentiment lexicons is an extremely resourceintensive process. Such lexicons remain a scarce resource for most languages. In this paper, we address this lexicon gap by building high-quality sentiment lexicons for 136 major languages. We integrate a variety of linguistic resources to produce an immense knowledge graph. By appropriately propagating from seed words, we construct sentiment lexicons for each component language of our graph. Our lexicons have a polarity agreement of 95.7% with published lexicons, while achieving an overall coverage of 45.2%. We demonstrate the performance of our lexicons in an extrinsic analysis of 2,000 distinct historical figures' Wikipedia articles on 30 languages. Despite cultural difference and the intended neutrality of Wikipedia articles, our lexicons show an average sentiment correlation of 0.28 across all language pairs. © 2014 Association for Computational Linguistics.",Final,All Open Access; Bronze Open Access; Green Open Access,
Kríž V.; Hladká B.; Nečaský M.; Knap T.,Data extraction using NLP techniques and its transformation to linked data,1,1,-1,#excluded #noLLOD,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944543658&doi=10.1007%2f978-3-319-13647-9_13&partnerID=40&md5=e5639512518db37019fb671eb487f863,"We present a system that extracts a knowledge base from raw unstructured texts that is designed as a set of entities and their relations and represented in an ontological framework. The extraction pipeline processes input texts by linguistically-aware tools and extracts entities and relations from their syntactic representation. Consequently, the extracted data is represented according to the Linked Data principles. The system is designed both domain and language independent and provides users with data for more intelligent search than full-text search. We present our first case study on processing Czech legal texts. © Springer International Publishing Switzerland 2014.",Final,,
Borin L.; Dannélls D.; Forsberg M.; McCrae J.P.,Representing Swedish lexical resources in RDF with lemon,1,1,1,#kg #swedish,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921893669&partnerID=40&md5=5561a53363080f9e2f6a3b75923329c3,"The paper presents an ongoing project which aims to publish Swedish lexical-semantic resources using Semantic Web and Linked Data technologies. In this article, we highlight the practical conversion methods and challenges of converting three of the Swedish language resources in RDF with lemon.",Final,,
Huang H.-H.; Yu C.-S.; Chen H.-Y.; Chen H.-H.; Lee P.-C.; Chen C.-H.,Integrating linguistic and world knowledge for domain-adaptable natural language interfaces,1,1,-1,#excluded #noLLOD,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958524555&doi=10.1007%2f978-3-319-07983-7_33&partnerID=40&md5=a63bfc3e7b5f63e273fa104310821b3b,"Nowadays, natural language interfaces (NLIs) show strong demands on various smart devices from wearable devices, cell phones, televisions, to vehicles. Domain adaptation becomes one of the major challenging issues to support the applications on different domains. In this paper, we propose a framework of domain-adaptable NLIs to integrate linguistic knowledge and world knowledge. Given a knowledge base of a target domain and the function definition of a target smart device, the corresponding NLI system is developed under the framework. In the experiments, we demonstrate a Chinese NLI system for a video on demand (VOD) service. © Springer International Publishing Switzerland 2014.",Final,,
Walter S.; Unger C.; Cimiano P.,ATOLL - A framework for the automatic induction of ontology lexica,1,1,0/1,#excluded,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84915732154&doi=10.1016%2fj.datak.2014.09.003&partnerID=40&md5=2c0fe477f12c272f5d6455236f1ac341,"There is a range of large knowledge bases, such as Freebase and DBpedia, as well as linked data sets available on the web, but they typically lack lexical information stating how the properties and classes they comprise are realized lexically. Often only one label is attached, if at all, thus lacking rich linguistic information, e.g. about morphological forms, syntactic arguments or possible lexical variants and paraphrases. While ontology lexicon models like lemon allow for defining such linguistic information with respect to a given ontology, the cost involved in creating and maintaining such lexica is substantial, requiring a high manual effort. Towards lowering this effort we present ATOLL, a framework for the automatic induction of ontology lexica, based both on existing labels and on dependency paths extracted from a text corpus. We instantiate ATOLL with respect to DBpedia as dataset and Wikipedia as corresponding corpus, and evaluate it by comparing the automatically generated lexicon with a manually constructed one. Our results clearly corroborate that our approach shows a high potential to be applied in a semi-automatic fashion in which a lexicon engineer can validate, reject or refine the automatically generated lexical entries, thus having a clear potential to contributing to the reduction of the overall cost of creating ontology lexica. © 2014 Elsevier B.V. All rights reserved.",Final,All Open Access; Green Open Access,
Declerck T.; Krieger H.-U.,Harmonization of German lexical resources for opinion mining,1,1,1,#kg,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037104633&partnerID=40&md5=abd2c1bb4f96fefe866d0c647843b35d,"We present on-going work on the harmonization of existing German lexical resources in the field of opinion and sentiment mining. The input of our harmonization effort consisted in four distinct lexicons of German word forms, encoded either as lemmas or as full forms, marked up with polarity features, at distinct granularity levels. We describe how the lexical resources have been mapped onto each other, generating a unique list of entries, with unified Part-of-Speech information and basic polarity features. Future work will be dedicated to the comparison of the harmonized lexicon with German corpora annotated with polarity information. We are further aiming at both linking the harmonized German lexical resources with similar resources in other languages and publishing the resulting set of lexical data in the context of the Linguistic Linked Open Data cloud.",Final,,
Di Buono M.P.; Monteleone M.; Elia A.,Terminology and Knowledge Representation Italian Linguistic Resources for the Archaeological Domain,1,1,1,#use #domainspecific,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978822095&partnerID=40&md5=ef7bef62c8826d6dc79abd7bef319b80,"Knowledge representation is heavily based on using terminology, due to the fact that many terms have precise meanings in a specific domain but not in others. As a consequence, terms becomes unambiguous and clear, and at last, being useful for conceptualizations, are used as a starting point for formalizations. Starting from an analysis of problems in existing dictionaries, in this paper we present formalized Italian Linguistic Resources (LRs) for the Archaeological domain, in which we integrate/couple formal ontology classes and properties into/to electronic dictionary entries, using a standardized conceptual reference model. We also add Linguistic Linked Open Data (LLOD) references in order to guarantee the interoperability between linguistic and language resources, and therefore to represent knowledge. © COLING 2014. All rights reserved.",Final,,
Walter S.; Unger C.; Cimiano P.,A corpus-based approach for the induction of ontology lexica,1,1,1,#use,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884933747&doi=10.1007%2f978-3-642-38824-8_9&partnerID=40&md5=6d5dbc4fbc39c798ec928d0de77de5d1,"While there are many large knowledge bases (e.g. Freebase, Yago, DBpedia) as well as linked data sets available on the web, they typically lack lexical information stating how the properties and classes are realized lexically. If at all, typically only one label is attached to these properties, thus lacking any deeper syntactic information, e.g. about syntactic arguments and how these map to the semantic arguments of the property as well as about possible lexical variants or paraphrases. While there are lexicon models such as lemon allowing to define a lexicon for a given ontology, the cost involved in creating and maintaining such lexica is substantial, requiring a high manual effort. Towards lowering this effort, in this paper we present a semi-automatic approach that exploits a corpus to find occurrences in which a given property is expressed, and generalizing over these occurrences by extracting dependency paths that can be used as a basis to create lemon lexicon entries. We evaluate the resulting automatically generated lexica with respect to DBpedia as dataset and Wikipedia as corresponding corpus, both in an automatic mode, by comparing to a manually created lexicon, and in a semi-automatic mode in which a lexicon engineer inspected the results of the corpus-based approach, adding them to the existing lexicon if appropriate. © 2013 Springer-Verlag Berlin Heidelberg.",Final,All Open Access; Green Open Access,
O'Riain S.; Coughlan B.; Buitelaar P.; Declerk T.; Krieger U.; Marie-Thomas S.,Cross-lingual querying and comparison of linked financial and business data,1,1,-1,#excluded #noLLOD,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893767862&doi=10.1007%2f978-3-642-41242-4_32&partnerID=40&md5=5a2d88e4cf1487605ff1a9af578f3dd9,"Cross lingual querying of financial and business data from multi-lingual sources requires that inherent challenges posed by the diversity of financial concepts and languages used in different jurisdictions be addressed. Ontologies can be used to semantically align financial concepts and integrate financial facts with other company information from multilingual, semi-structured and unstructured Open Data sources. Availability as Linked Data then allows cross-lingual interrogation of the interlinked multi-lingual data set. This paper presents how the use of semantics and Linked Data enables the alignment and integration of business and financial facts provided by the different European Business Registers. The demonstrator allows business users to query multilingual data, perform comparisons, and review generated financial metrics. © Springer-Verlag 2013.",Final,All Open Access; Bronze Open Access,
Schoene A.M.; Dethlefs N.; Ananiadou S.,RELATE: Generating a linguistically inspired Knowledge Graph for fine-grained emotion classification,1,1,1,#kg #emotions ,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143651259&partnerID=40&md5=6572091fcd5d64ecee27f933bf8ed3e5,"Several existing resources are available for sentiment analysis (SA) tasks that are used for learning sentiment specific embedding (SSE) representations. These resources are either large, common-sense knowledge graphs (KG) that cover a limited amount of polarities/emotions or they are smaller in size (e.g.: lexicons), which require costly human annotation and cover fine-grained emotions. Therefore using knowledge resources to learn SSE representations is either limited by the low coverage of polarities/emotions or the overall size of a resource. In this paper, we first introduce a new directed KG called 'RELATE', which is built to overcome both the issue of low coverage of emotions and the issue of scalability. RELATE is the first KG of its size to cover Ekman's six basic emotions that are directed towards entities. It is based on linguistic rules to incorporate the benefit of semantics without relying on costly human annotation. The performance of 'RELATE' is evaluated by learning SSE representations using a Graph Convolutional Neural Network (GCN). © European Language Resources Association (ELRA), licensed under CC-BY-NC-4.0.",Final,,
Lewandowska-Tomaszczyk B.; Baczkowska A.; Dontcheva-Navrátilová O.; Liebeskind C.; Valūnaitė Oleškevičienė G.; Zitnik S.; Trojszczak M.; Povolná R.; Selmistraitis L.; Utka A.; Gudelis D.,LLOD schema for Simplified Offensive Language Taxonomy in multilingual detection and applications,1,1,-1,#excluded #outofscope,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180448082&doi=10.1515%2flpp-2023-0016&partnerID=40&md5=2f971e23337d5b3cbf581098de640309,"The goal of the paper is to present a Simplified Offensive Language (SOL) Taxonomy, its application and testing in the Second Annotation Campaign conducted between March-May 2023 on four languages: English, Czech, Lithuanian, and Polish to be verified and located in LLOD. Making reference to the previous Offensive Language taxonomic models proposed mostly by the same COST Action Nexus Linguarum WG 4.1.1 team, the number and variety of the categories underwent the definitional revision, and the present typology was tested in the annotation on the publicly available offensive language datasets of each of the four languages. The results of the annotation are presented and as they are contained within the accepted statistical values on the inter-annotator agreement in the SOL categories and their aspects, we propose this taxonomy as a core ontology which represents the encoding of the supported offensive languages and justify its use on new data in terms of a more universal Linguistic Linked Open Data (LLOD) schema.  © 2023 Walter de Gruyter GmbH, Berlin/Boston.",Final,All Open Access; Bronze Open Access,
Bellandi A.; Piccini S.,Creating specialised dictionaries using LexO,1,,-1,#excluded #outofscope,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178137448&doi=10.1515%2flex-2023-0012&partnerID=40&md5=00f7a48155843f344c3dba39b211f452,"Although the contribution of linguistics to terminology is widely recognized today, there remains a lack of tools allowing the construction of specialized dictionaries that deal with both the conceptual and the linguistic dimension of the terms. In this paper we present LexO, a collaborative web editor that was built with the aim of filling this gap. The guiding principles of LexO are the following: i) to allow terminologists to define the morphological, syntactic and semantic features of a term, w.r.t. the traditional onomasiological perspective; ii) to adhere to the open science philosophy and to the FAIR principles, so to create specialized dictionaries that can be shared and reused within the scientific community. © 2023 Walter de Gruyter GmbH, Berlin/Boston.",Final,,
Costa R.; Salgado A.; Ramos M.; Almeida B.; Silva R.; Carvalho S.; Khan F.; Tasovac T.; Khemakhem M.; Romary L.,A crossroad between lexicography and terminology work: Knowledge organization and domain labelling,1,1,1,#kg #portuguese,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163993265&doi=10.1093%2fllc%2ffqad022&partnerID=40&md5=cc9504aff414986d41021c39fa0ed7b7,"MORDigital project aims to encode the selected editions of Diccionario de Lingua Portugueza by António de Morais Silva, first published in 1789. Our ultimate goals are, on the one hand, to promote accessibility to cultural heritage while fostering reusability and, on the other hand, to contribute towards a more significant presence of lexicographic digital content in Portuguese through open tools and standards. The Morais dictionary represents a significant legacy, since it marks the beginning of Portuguese dictionaries, having served as a model for all subsequent lexicographic production. The team follows a new paradigm in lexicography, which results from the convergence between lexicography, terminology, computational linguistics, and ontologies as an integral part of digital humanities and linked (open) data. In the Portuguese context, this research fills a gap concerning searchable online retrodigitized dictionaries, built on current standards and methodologies which promote data sharing and harmonization, namely TEI Lex-0. The team will further ensure the connection to other existing systems and lexical resources, particularly in the Portuguese-speaking world.  © 2023 The Author(s).",Final,,
Valūnaitė-Oleškevičienė G.; Selmistraitis L.; Utka A.; Gudelis D.,Offensive language in user-generated comments in Lithuanian,1,1,-1,#excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180496544&doi=10.1515%2flpp-2023-0013&partnerID=40&md5=554a84b8d150bb2c5a8e633d1c765d97,"The aim of the current research is to investigate the feasibility of identifying offensive language in Lithuanian by utilising the Simplified Offensive Language Taxonomy (SOLT). The key principle behind this taxonomy is its ability to complement existing offensive language ontologies and tagset systems, with the ultimate goal of integrating it into publicly accessible Linguistic Linked Open Data (LLOD) resources. The dataset used in the current study is a publicly available corpus of user-generated comments collected from a Lithuanian portal (Amilevičius et al. 2016). The study identified that offensive language predominantly focuses on collective derogatory language rather than individuals. The most common category of offensive language is related to physical and mental disabilities, followed by ideological offenses, xenophobic and sexist remarks, and less frequent categories like ageism, classism, homophobia, and religious discrimination. These results highlight the diverse range of offensive language online and underscore the need to combat discrimination and promote respectful discourse, particularly concerning marginalised groups.  © 2023 Walter de Gruyter GmbH, Berlin/Boston.",Final,,
Wu Y.; Wu X.; Li J.; Zhang Y.; Wang H.; Du W.; He Z.; Liu J.; Ruan T.,MMpedia: A Large-Scale Multi-modal Knowledge Graph,1,1,1,#kg #multimodal,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177439074&doi=10.1007%2f978-3-031-47243-5_2&partnerID=40&md5=d97d33de6b765a990acb315b64c495f3,"Knowledge graphs serve as crucial resources for various applications. However, most existing knowledge graphs present symbolic knowledge in the form of natural language, lacking other modal information, e.g., images. Previous multi-modal knowledge graphs have encountered challenges with scaling and image quality. Therefore, this paper proposes a highly-scalable and high-quality multi-modal knowledge graph using a novel pipeline method. Summarily, we first retrieve images from a search engine and build a new Recurrent Gate Multi-modal model to filter out the non-visual entities. Then, we utilize entities’ textual and type information to remove noisy images of the remaining entities. Through this method, we construct a large-scale multi-modal knowledge graph named MMpedia, containing 2,661,941 entity nodes and 19,489,074 images. As we know, MMpedia has the largest collection of images among existing multi-modal knowledge graphs. Furthermore, we employ human evaluation and downstream tasks to verify the usefulness of images in MMpedia. The experimental result shows that both the state-of-the-art method and multi-modal large language model (e.g., VisualChatGPT) achieve about a 4% improvement on Hit@1 in the entity prediction task by incorporating our collected images. We also find that the multi-modal large language model is hard to ground entities to images. The dataset (https://zenodo.org/record/7816711 ) and source code of this paper are available at https://github.com/Delicate2000/MMpedia. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.",Final,,
Mambrini F.; Passarotti M.C.,The LiLa Lemma Bank: A Knowledge Base of Latin Canonical Forms,1,1,1,#kg ,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179106255&doi=10.5334%2fjohd.145&partnerID=40&md5=c7bca225d7d3e40141a1883995b62a53,"The dataset contains a list of 215,102 Latin dictionary forms (known as canonical forms or lemmas). The dataset is a set of 1,699,687 Resource Description Framework (RDF) triples that describe, using a series of Web Ontology Language (OWL) ontologies for Linguistic Linked Data, the morphological properties of these forms. The dataset is used to link together a series of corpora and dictionaries in the interoperable network of language resources published by the LiLa: Linking Latin project. © 2023 The Author(s). This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International License (CC-BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. See http://creativecommons.org/licenses/by/4.0/.",Final,All Open Access; Gold Open Access,
Iurescia F.; Litta E.; Passarotti M.; Pellegrini M.; Moretti G.; Ruffolo P.,Linking the Neulateinische Wortliste to the LiLa Knowledge Base of Interoperable Resources for Latin,1,1,1,#kg,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175477239&partnerID=40&md5=68f42614eab37b7bd59f7101634a6985,"This paper describes the process of interlinking a lexical resource consisting of a list of more than 20,000 Neo-Latin words with other resources for Latin. The resources are made interoperable thanks to their linking to the LiLa Knowledge Base, which applies Linguistic Linked Open Data practices and data categories to describe and publish on the Web both textual and lexical resources for the Latin language. © 2023 Association for Computational Linguistics.",Final,,
Huaman E.; Huaman J.L.; Huaman W.,Getting Quechua Closer to Final Users Through Knowledge Graphs,1,1,1,#kg,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164270030&doi=10.1007%2f978-3-031-35445-8_5&partnerID=40&md5=4e17f9c08ab107992111f7a599bbcff2,"Quechua language and Quechua knowledge gather millions of people around the world, especially in several countries in South America. Unfortunately, there are only a few resources available to Quechua communities, and they are mainly stored in unstructured format. In this paper, the Quechua Knowledge Graph is envisioned and generated as an effort to get Quechua language and knowledge closer to the Quechua communities, researchers, and technology developers. The process model for building the Quechua Knowledge Graph involved its creation, hosting, curation, and deployment phases. Currently, there are 553,636 triples stored in the Quechua Knowledge Graph, which is accessible on the Web, retrievable by machines, and curated by users. To showcase the deployment of the Quechua Knowledge Graph, use cases and future work are described. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Final,All Open Access; Green Open Access,
Tittel S.,Ceci n’est pas un dictionnaire. Adding and Extending Lexicographical Data of Medieval Romance Languages to and through a Multilingual Lexico-Ontological Project,1,1,,#kg,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171336702&partnerID=40&md5=8f67bb4534bf2f7dd8fc87f8fc942f23,"Historical lexicography of the Romance languages currently finds itself in a difficult place since the funding of some important dictionaries ended. The newly launched project ALMA will contribute to the future of these dictionaries’ content. ALMA combines methods of historical lexicography, text philology, corpus linguistics, and the history of sciences with a Linked Data approach and ontology development. It adopts a Pan-Romance perspective focusing on medieval Italian, French, and Occitan / Gascon within two knowledge domains, ‘medicine’ and ‘law’. ALMA’s goals include re-using, extending, further processing, and disseminating lexicographical data by integrating it into its work pipeline. This makes for benefits on both sides: Pivotal for the ALMA project is the anchoring of its philological and lexicological work within the framework of the entire languages examined by the dictionaries. The dictionaries, most notably those whose funding ended, profit by seeing their linguistic, textual, and historico-cultural knowledge put into new formats—e.g., Linked Data—, contexts—e.g., Pan-Romance—, and correlations—e.g., through linking to the historicized domain ontologies ALMA will develop. This introduces the valuable dictionary contents to a knowledge circulation that goes beyond their original scope and ensures its long-term re-use in a somewhat concealed way. © 2023 Lexical Computing CZ s.r.o.. All rights reserved.",Final,,
Jia Z.,Multi-Dialectal Representation Learning of Sinitic Phonology,1,1,-1,#excluded #noMaterializedResource,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173499956&partnerID=40&md5=be64a07e43f9a2d7c48b238346d6db2f,"Machine learning techniques have shown their competence for representing and reasoning in symbolic systems such as language and phonology. In Sinitic Historical Phonology, notable tasks that could benefit from machine learning include the comparison of dialects and reconstruction of proto-languages systems. Motivated by this, this paper provides an approach for obtaining multi-dialectal representations of Sinitic syllables, by constructing a knowledge graph from structured phonological data, then applying the BoxE technique from knowledge base learning. We applied unsupervised clustering techniques to the obtained representations to observe that the representations capture phonemic contrast from the input dialects. Furthermore, we trained classifiers to perform inference of unobserved Middle Chinese labels, showing the representations' potential for indicating archaic, proto-language features. The representations can be used for performing completion of fragmented Sinitic phonological knowledge bases, estimating divergences between different characters, or aiding the exploration and reconstruction of archaic features. © 2023 Association for Computational Linguistics.",Final,,
Vasilevich A.; Wetzel M.,Multilingual Knowledge Systems as Linguistic Linked Open Data,1,1,1,#kg,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141866894&doi=10.1007%2f978-3-031-17258-8_23&partnerID=40&md5=c5ad1b639c47006e962ade964c538329,"Creation and re-usability of language resources in accordance with Linked Data principles is a valuable asset in the modern data world. We describe the contributions made to extend the Linguistic Linked Open Data (LLOD) stack with a new resource, Coreon MKS, bringing together concept-oriented, language-agnostic terminology management and graph-based knowledge organisation. We dwell on our approach to mirroring of Coreon’s original data structure to RDF and supplying it with a SPARQL endpoint. We integrate MKS into the existing ELG infrastructure, using it as a platform for making the published MKS discoverable and retrievable via a industry-standard interface. While we apply this approach to LLOD-ify Coreon MKS, it can also provide relevant input for standardisation bodies and interoperability communities, acting as a blueprint for similar integration activities. © 2023, The Author(s).",Final,All Open Access; Hybrid Gold Open Access,
Liu W.; Tuo J.; Xue J.,A Study on the Connotation Remodeling of Science and Technology Vocabulary Based on the Application of Knowledge Graphs,1,-1,-1,#excluded #outofscope no reference to LLOD resources,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132024163&doi=10.1088%2f1742-6596%2f2278%2f1%2f012015&partnerID=40&md5=07779c19cb45e5ed16f352dff78ac2fa,"Disruptive Technology is also often translated as disruptive technology or breakthrough technology, which was first proposed by Professor Christensen of Harvard Business School in 1990s. Nowadays, the competition among countries is increasingly fierce, especially in the field of science and technology, which often leads to the backwardness and impoverishment of the whole industry due to the technological gap. Today, with the rapid consumption of social resources, social and economic development urgently needs disruptive technologies to promote industrial transformation, optimize economic structure and improve people's quality of life. Countries that master disruptive technologies can open up new industrial development paths and lay a foundation for unique technological advantages, thus bringing huge technological dividends to national economic development. Visuwords is a knowledge graph application platform, through which the meaning and semantic association of words can be visualized. This study takes the word ""famished""as an example to analyze the relationship between its new connotation and original meaning to verify its feasibility at the communication level. Today's social science knowledge in the general public needs to be disseminated from the original relatively obscure and academic to an easily acceptable form. This study applies the idea of scientific popularization to a knowledge graph visualization tool, while using the connotation of linguistic ideas to analyze and deconstruct specific wordsThe study combines the connotation concept of linguistics with the disciplinary tools in the field of knowledge graph, and specifically analyzes the role of scientific terms, especially some abbreviations, in the mass science communication.  © Published under licence by IOP Publishing Ltd.",Final,All Open Access; Bronze Open Access,
Giovannetti E.; Albanesi D.; Bellandi A.; Dattilo D.; Del Grosso A.M.; Marchi S.,An ontology of masters of the Babylonian Talmud,1,1,1,#kg,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141278876&doi=10.1093%2fllc%2ffqab043&partnerID=40&md5=24971102523111e6ca94b38cde2f25ec,"The purpose of this research is to build an ontology of the masters appearing in the Babylonian Talmud (BT). The ontology built so far has been shared as a Linked Open Data and it will be linked to existing vocabularies. This work has been developed in the context of the Babylonian Talmud Translation Project, where more than eighty Talmudists are working together, since 2012, at the translation (comprehensive of explicative notes and glossaries) of the Talmud into Italian. The construction of the resource has involved the application of tools leveraging on computational linguistics approaches. The ontology, already describing more than 500 masters, constitutes the first portion of a more comprehensive Talmudic Knowledge Base where the text itself, the terminology, the entities, and the concepts constituting the BT will be formalized and linked to each other.  © 2022 The Author(s) 2021. Published by Oxford University Press on behalf of EADH. All rights reserved.",Final,,
Armaselu F.; Apostol E.-S.; Khan A.F.; Liebeskind C.; Mcgillivray B.; Truicǎ C.-O.; Utka A.; Oleškevičiene G.V.; Van Erp M.,LL(O)D and NLP perspectives on semantic change for humanities research,1,,-1,#related,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136245558&doi=10.3233%2fSW-222848&partnerID=40&md5=415e770a0e93459f7177187911a839f3,"This paper presents an overview of the LL(O)D and NLP methods, tools and data for detecting and representing semantic change, with its main application in humanities research. The paper's aim is to provide the starting point for the construction of a workflow and set of multilingual diachronic ontologies within the humanities use case of the COST Action Nexus Linguarum, European network for Web-centred linguistic data science, CA18209. The survey focuses on the essential aspects needed to understand the current trends and to build applications in this area of study.  © 2022 - The authors. Published by IOS Press.",Final,All Open Access; Bronze Open Access; Green Open Access,
Martín-Chozas P.; Vázquez-Flores K.; Calleja P.; Montiel-Ponsoda E.; Rodríguez-Doncel V.,TermitUp: Generation and enrichment of linked terminologies,1,1,-1,#related,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140849135&doi=10.3233%2fSW-222885&partnerID=40&md5=508d8e226d64fbe8278dee83c0ce83bb,"Domain-specific terminologies play a central role in many language technology solutions. Substantial manual effort is still involved in the creation of such resources, and many of them are published in proprietary formats that cannot be easily reused in other applications. Automatic term extraction tools help alleviate this cumbersome task. However, their results are usually in the form of plain lists of terms or as unstructured data with limited linguistic information. Initiatives such as the Linguistic Linked Open Data cloud (LLOD) foster the publication of language resources in open structured formats, specifically RDF, and their linking to other resources on the Web of Data. In order to leverage the wealth of linguistic data in the LLOD and speed up the creation of linked terminological resources, we propose TermitUp, a service that generates enriched domain specific terminologies directly from corpora, and publishes them in open and structured formats. TermitUp is composed of five modules performing terminology extraction, terminology post-processing, terminology enrichment, term relation validation and RDF publication. As part of the pipeline implemented by this service, existing resources in the LLOD are linked with the resulting terminologies, contributing in this way to the population of the LLOD cloud. TermitUp has been used in the framework of European projects tackling different fields, such as the legal domain, with promising results. Different alternatives on how to model enriched terminologies are considered and good practices illustrated with examples are proposed.  © 2022 - The authors. Published by IOS Press.",Final,All Open Access; Bronze Open Access,
Zhu Y.; Hu L.; Ning N.; Zhang W.; Wu B.,A lexical psycholinguistic knowledge-guided graph neural network for interpretable personality detection,1,1,-1,#excluded,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130979428&doi=10.1016%2fj.knosys.2022.108952&partnerID=40&md5=7f0c12a4f159aa56486489a050c7c5cf,"With the blossoming of online social media, personality detection based on user-generated content has a significant impact on information scientific and industrial applications. Most existing approaches rely heavily on semantic features or superficial psycholinguistic statistical features calculated by existing tools and fail to effectively exploit psycholinguistic knowledge that can help determine and interpret peoples personality traits. In this paper, we propose a novel lexical psycholinguistic knowledge-guided graph neural model for interpretable personality detection, which leverages the personality lexicons as a bridge for injecting relevant external knowledge to enrich the semantics of a document. Specifically, we learn a kind of personality-aware word embedding, that encodes psycholinguistic information in the continuous representations of words. Then, a Heterogeneous Personality word graph is constructed by aligning the personality lexicons with the personality knowledge graph, which is fed into a Message-passing graph Network (HPMN) to extract explicit lexicon and knowledge relations through the interactions among heterogeneous graph nodes. Finally, through a carefully designed readout function, all heterogeneous nodes are selectively incorporated as knowledge-guided document embeddings for user-generated text personality understanding and interpretation. Experiments show that our model effectively detects personality traits. Moreover, it provides a certain level of support for lexical hypotheses in psycholinguistic research from a computational linguistics perspective. © 2022",Final,,
Forkel R.; Hammarström H.,"Glottocodes: Identifiers linking families, languages and dialects to comprehensive reference information",1,1,1,#kg,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140853767&doi=10.3233%2fSW-212843&partnerID=40&md5=558639b9e88deeea1855087cfe435372,"Glottocodes constitute the backbone identification system for the language, dialect and family inventory Glottolog (https://glottolog.org). In this paper, we summarize the motivation and history behind the system of glottocodes and describe the principles and practices of data curation, technical infrastructure and update/version-tracking systematics. Since our understanding of the target domain - the dialects, languages and language families of the entire world - is continually evolving, changes and updates are relatively common. The resulting data is assessed in terms of the FAIR (Findable, Accessible, Interoperable, Reusable) Guiding Principles for scientific data management and stewardship. As such the glottocode-system responds to an important challenge in the realm of Linguistic Linked Data with numerous NLP applications.  © 2022 - The authors. Published by IOS Press.",Final,All Open Access; Bronze Open Access,
Schneidermann N.S.; Pedersen B.S.,"Evaluating a New Danish Sentiment Resource: the Danish Sentiment Lexicon, DSL",1,1,1,#use,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146225959&partnerID=40&md5=f56055c8d4bfb8efa5e69f27e218c159,"In this paper, we evaluate a new sentiment lexicon for Danish, the Danish Sentiment Lexicon (DSL), to gain input regarding how to carry out the final adjustments of the lexicon. A feature of the lexicon that differentiates it from other sentiment resources for Danish is that it is linked to a large number of other Danish lexical resources via the DDO lemma and sense inventory and the LLOD via the Danish WordNet, DanNet. We perform our evaluation on four datasets labeled with sentiments. In addition, we compare the lexicon against two existing benchmarks for Danish: the Afinn and the Sentida resources. We observe that DSL performs mostly comparably to the existing resources, but that more fine-grained explorations need to be done in order to fully exploit its possibilities given its linking properties. © European Language Resources Association (ELRA), licensed under CC-BY-NC-4.0.",Final,,
Stranisci M.A.; Frenda S.; Lai M.; Araque O.; Cignarella A.T.; Basile V.; Patti V.; Bosco C.,O-Dang! The Ontology of Dangerous Speech Messages,1,1,1,#kg,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146216572&partnerID=40&md5=fef0ade8584f337d1191ead5ebacfbc8,"Inside the NLP community there is a considerable amount of language resources created, annotated and released every day with the aim of studying specific linguistic phenomena. Despite a variety of attempts in order to organize such resources has been carried on, a lack of systematic methods and of possible interoperability between resources are still present. Furthermore, when storing linguistic information, still nowadays, the most common practice is the concept of “gold standard”, which is in contrast with recent trends in NLP that aim at stressing the importance of different subjectivities and points of view when training machine learning and deep learning methods. In this paper we present O-Dang!: The Ontology of Dangerous Speech Messages, a systematic and interoperable Knowledge Graph (KG) for the collection of linguistic annotated data. O-Dang! is designed to gather and organize Italian datasets into a structured KG, according to the principles shared within the Linguistic Linked Open Data community. The ontology has also been designed to account a perspectivist approach, since it provides a model for encoding both gold standard and single-annotator labels in the KG. The paper is structured as follows. In Section 1. the motivations of our work are outlined. Section 2. describes the O-Dang! Ontology, that provides a common semantic model for the integration of datasets in the KG. The Ontology Population stage with information about corpora, users, and annotations is presented in Section 3.. Finally, in Section 4. an analysis of offensiveness across corpora is provided as a first case study for the resource. © European Language Resources Association (ELRA), licensed under CC-BY-NC-4.0.",Final,,
Sun Y.; Shi Q.; Qi L.; Zhang Y.,JointLK: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering,1,1,1,#use,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138372259&partnerID=40&md5=5e07a3b2313a072bb3f369edb054dac8,"Existing KG-augmented models for commonsense question answering primarily focus on designing elaborate Graph Neural Networks (GNNs) to model knowledge graphs (KGs). However, they ignore (i) the effectively fusing and reasoning over question context representations and the KG representations, and (ii) automatically selecting relevant nodes from the noisy KGs during reasoning. In this paper, we propose a novel model, JointLK, which solves the above limitations through the joint reasoning of LM and GNN and the dynamic KGs pruning mechanism. Specifically, JointLK performs joint reasoning between LM and GNN through a novel dense bidirectional attention module, in which each question token attends on KG nodes and each KG node attends on question tokens, and the two modal representations fuse and update mutually by multi-step interactions. Then, the dynamic pruning module uses the attention weights generated by joint reasoning to prune irrelevant KG nodes recursively. We evaluate JointLK on the CommonsenseQA and OpenBookQA datasets, and demonstrate its improvements to the existing LM and LM+KG models, as well as its capability to perform interpretable reasoning. © 2022 Association for Computational Linguistics.",Final,,
Chiarcos C.; Fäth C.; Ionov M.,Querying a Dozen Corpora and a Thousand Years with Fintan,1,1,-1,#excluded,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144449882&partnerID=40&md5=796cde76de97360b5cf4c16f50ccbf12,"Large-scale diachronic corpus studies covering longer time periods are difficult if more than one corpus are to be consulted and, as a result, different formats and annotation schemas need to be processed and queried in a uniform, comparable and replicable manner. We describes the application of the Flexible Integrated Transformation and Annotation eNgineering (Fintan) platform for studying word order in German using syntactically annotated corpora that represent its entire written history. Focusing on nominal dative and accusative arguments, this study hints at two major phases in the development of scrambling in modern German. Against more recent assumptions, it supports the traditional view that word order flexibility decreased over time, but it also indicates that this was a relatively sharp transition in Early New High German. The successful case study demonstrates the potential of Fintan and the underlying LLOD technology for historical linguistics, linguistic typology and corpus linguistics. The technological contribution of this paper is to demonstrate the applicability of Fintan for querying across heterogeneously annotated corpora, as previously, it had only been applied for transformation tasks. With its focus on quantitative analysis, Fintan is a natural complement for existing multi-layer technologies that focus on query and exploration. © European Language Resources Association (ELRA), licensed under CC-BY-NC-4.0.",Final,,
Kirillovich A.; Nikolaev K.,Adapting the LodView RDF Browser for Navigation over the Multilingual Linguistic Linked Open Data Cloud,1,1,1,#use,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138990992&doi=10.1109%2fSETIT54465.2022.9875628&partnerID=40&md5=6ad70bd7390e781408048147b270f432,"This paper is dedicated to using of LodView RDF browser for navigation over the multilingual Linguistic Linked Open Data cloud. We reveal several limitations of LodView that impede its use for this purpose, and propose improvements to be made for fixing these limitations. These improvements are: 1) resolution of Cyrillic URIs; 2) decoding Cyrillic URIs in Turtle representations of resources; 3) support of Cyrillic literals; 4) user-friendly URLs for RDF representations of resources; 5) support of hash URIs; 6) expanding nested resources; 7) support of RDF collections; 8) support of LATEX math notation; and 9) pagination of resource property values. We implement several of the proposed improvements.  © 2022 IEEE.",Final,All Open Access; Green Open Access,
Porzel R.; Pomarlan M.; Spillner L.; Bateman J.; Mildner T.; Santagiustina C.,Narrativizing Knowledge Graphs,1,1,1,#use,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142493609&partnerID=40&md5=97e9b0544dd52d6554863b5fbd4e8177,"Any natural language expression of a set of facts - that can be represented as a knowledge graph - will more or less overtly assume a specific perspective on these facts. In this paper we see the conversion of a given knowledge graph into natural language as the construction of a narrative about the assertions made by the knowledge graph. We, therefore, propose a specific pipeline that can be applied to produce linguistic narratives from knowledge graphs using an ontological layer and corresponding rules that turn a knowledge graph into a semantic specification for natural language generation. Critically, narratives are seen as necessarily committing to specific perspectives taken on the facts presented. We show how this most commonly neglected facet of producing summaries of facts can be brought under control. © 2021 Copyright for this paper by its authors.",Final,,
Robin C.; Suresh G.V.; Doncel V.R.; McCrae J.; Buitelaar P.,Linghub2: Language Resource Discovery Tool for Language Technologies,1,1,-1,#excluded #outofscope no specific LLOD resources are used. the tool seems to be customizable to query LLOD via a working SPARQL endpoint,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144395636&partnerID=40&md5=95b25678ee51855168f23c5b460f39cf,"Language resources are an essential component of natural language processing, as well as related research and applications. Users of language resources have different needs in terms of format, language, topics, etc. for the data they need to use. Linghub (McCrae and Cimiano, 2015) was first developed for this purpose, using the capabilities of linked data to represent metadata, and tackling the heterogeneous metadata issue. Linghub is aimed at helping language resources and technology users to easily find and retrieve relevant data, and identify important information on access, topics, etc. This work describes a rejuvenation and modernisation of the 2015 platform into using a popular open source data management system, DSpace, as foundation. The new platform, Linghub2, contains updated and extended resources, more languages, and continues the work towards the homogenisation of metadata through conversions, through linkage to standardisation strategies and community groups, such as the Open Digital Rights Language (ODRL) community group. © European Language Resources Association (ELRA), licensed under CC-BY-NC-4.0.",Final,,
Krause L.; Sommerauer P.; Vossen P.,Towards More Informative List Verbalisations,1,1,-1,#excluded #noLLOD,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142540592&partnerID=40&md5=4754d7f294b8fb873fdcde3c9cbc6b46,"In this paper we propose the task of list verbalisation within a Knowledge Graph Question Answering system. Inspired by the Gricean Maxims of Quantity, Relation, and Manner we show a proof of concept ranking answer candidates through graph-based and language model-based measurements for on the one hand popularity and on the other hand a more pragmatically informed context. Our finding show that in our current set-up graph-based measures work best, while language model-based systems need further refinement and may benefit from approaches such as fine-tuning or prompting. We evaluate our approach with a user study and give insights into promising future directions of the task. © 2022 Copyright for this paper by its authors.",Final,,
Vasilogamvrakis N.,The Ontological Approach of Modern Greek Morphology (short paper),1,1,-1,#outofscope #excluded,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140894096&partnerID=40&md5=b4f7f5075f57b7a58b0c5fb0c4dce941,"This article comprises a brief overview of my PhD research proposal investigating the ontological approach of Modern Greek (MG) morphology. Its main objective is to study contemporary onto-linguistic models in order to form an onto-morphological tool for MG morphological analysis. The research was motivated by the lack of an ontologically holistic approach based on the Semantic Web (SW) paradigm to represent MG morphology. After a brief review on the current ontological setting within the Semantic Web, the respective morphological framework is determined and placed into the Strong Lexicalist theory justified by MG morpheme-based nature. Following this, main research questions are defined and the methodology of the research is presented as an itinerary process between ontological development, theory and lexical data testing. Finally, the article concludes with some preliminary research results based on a morpheme-based analysis of indicative MG lexical data in the MMoOn ontological model. © 2020 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0)",Final,,
Basile P.; Cassotti P.; Ferilli S.; McGillivray B.,A New Time-sensitive Model of Linguistic Knowledge for Graph Databases,1,1,-1,#outofscope #excluded #noLLOD,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143434825&partnerID=40&md5=784409e837cc93283825123be1e091e6,"Graph databases are a straightforward technology for storing knowledge graphs. However, they are schema-less. We apply the GraphBRAIN Schema (GBS) format to describe Time-sensitive Linguistic Knowledge in a graph database (Neo4j). Our schema can model relations between concepts and words, information about word occurrences, and diachronic information about concepts and words. This paper introduces GraphBRAIN technology and describes our model for time-sensitive linguistic data. Moreover, we provide an example of usage and show the potential of this model for humanities and cultural heritage research. © 2022 Copyright for this paper by its authors.",Final,,
Chiarcos C.; Fäth C.; Ionov M.,Unifying Morphology Resources with OntoLex-Morph. A Case Study in German,1,,1,#kg #ontology mapping,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144442932&partnerID=40&md5=b2279530bc4eb58858921bff9b066447,"The OntoLex vocabulary has become a widely used community standard for machine-readable lexical resources on the web. The primary motivation to use OntoLex in favor of tool- or application-specific formalisms is to facilitate interoperability and information integration across different resources. One of its extension that is currently being developed is a module for representing morphology, OntoLex-Morph. In this paper, we show how OntoLex-Morph can be used for the encoding and integration of different types of morphological resources on a unified basis. With German as the example, we demonstrate it for (a) a full-form dictionary with inflection information (Unimorph), (b) a dictionary of base forms and their derivations (UDer), (c) a dictionary of compounds (from GermaNet), and (d) lexicon and inflection rules of a finite-state parser/generator (SMOR/Morphisto). These data are converted to OntoLex-Morph, their linguistic information is consolidated and corresponding lexical entries are linked with each other. The main contribution of this paper is the discussion of the current state of OntoLex-Morph and its validation on different types of real-world resources for a single language. In the longer term, the successful application of OntoLex-Morph to such diverse data, along with the adjustments to the vocabulary observed in the process, will be a means to establish interoperability among morphological resources as well as between them and classical lexical data such as dictionaries, WordNets, or thesauri. © European Language Resources Association (ELRA), licensed under CC-BY-NC-4.0.",Final,,
Leenoi D.; Alongkornchai A.; Takhom A.; Boonkwan P.; Sunnithi T.,A Construction of Thai WordNet through Translation Equivalence,1,1,1,#kg,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143991392&doi=10.1109%2fiSAI-NLP56921.2022.9960263&partnerID=40&md5=e6162d7666ef392af33ee0d71a3e3858,"WordNet is a crucial language resource associated with artificial intelligence activities, for instance, constructing building models for advancement of computational linguistics and natural language processing, or representing statistical insights through knowledge graphs that emulate cognition and human understanding. Thai WordNet has been developed in many approaches, e.g., a merge approach in gold standard, and semi-auto construction with a bilingual dictionary. However, existing Thai WordNet is not easy to find words fit with the definition of synsets; and cover cultural gaps between the different languages of which needed to be aware. This paper presents a methodology of Translation Equivalence in order to construct Thai language resource, called LST22 Thai WordNet.  © 2022 IEEE.",Final,,
Mambrini F.; Passarotti M.; Moretti G.; Pellegrini M.,The Index Thomisticus Treebank as Linked Data in the LiLa Knowledge Base,1,1,1,#kg,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144353382&partnerID=40&md5=c82f76ce1252ae5ed16ed2cd7a990f7f,"Although the Universal Dependencies initiative today allows for cross-linguistically consistent annotation of morphology and syntax in treebanks for several languages, syntactically annotated corpora are not yet interoperable with many lexical resources that describe properties of the words that occur therein. In order to cope with such limitation, we propose to adopt the principles of the Linguistic Linked Open Data community, to describe and publish dependency treebanks as LLOD. In particular, this paper illustrates the approach pursued in the LiLa Knowledge Base, which enables interoperability between corpora and lexical resources for Latin, to publish as Linguistic Linked Open Data the annotation layers of two versions of a Medieval Latin treebank (the Index Thomisticus Treebank). © European Language Resources Association (ELRA), licensed under CC-BY-NC-4.0.",Final,,
Yu S.; Zhang S.; Zhang J.; Zhou J.; Sun Y.; Li B.; Xuan Q.,SubGraph Networks Based Entity Alignment for Cross-Lingual Knowledge Graph,1,,-1,#excluded #noLLOD,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145259786&doi=10.1007%2f978-981-19-7532-5_7&partnerID=40&md5=a0c5880675341497b46dd740c0c3054c,"Entity alignment is the task of discovering entities representing the equal real-world object in two knowledge graphs (KGs). Cross-lingual knowledge graph entity alignment aims to discover the cross-lingual links in the multi-language KGs, which is of wonderful value to solve the NLP problems and integrate multi-language KGs. In the task of aligning cross-language knowledge graphs, the structures of the two graphs are very similar, and the equivalent entities often have the same subgraph structure characteristics. The traditional GCN method neglects to obtain structural features through representative parts of the original graph and the use of adjacency matrix is not enough to effectively represent the structural features of the graph. In this paper, we introduce the subgraph network (SGN) method into the GCN-based cross-lingual KG entity alignment method. In the method, we extracted the first-order subgraphs of the KGs to expand the structural features of the original graph to enhance the representation ability of the entity embedding and improve the alignment accuracy. Experiments show that the proposed method is advanced in the task of entity alignment. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Final,All Open Access; Green Open Access,
Vasilogamvrakis N.; Sfakakis M.,A Morpheme-Based Paradigm for the Ontological Analysis of Modern Greek Derivational Morphology,1,1,1,#kg,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128491291&doi=10.1007%2f978-3-030-98876-0_34&partnerID=40&md5=6620014278811c1edae91d13371b3856,"Morphology is the linguistic field that investigates the minimal meaningful units within words and their interactive processes. In coping with the ontological representation of Modern Greek (MG) derivational morphology the morpheme-based or lexicalist paradigm was tested due to the highly productive concatenative nature of the language. Following this, a specific domain ontological model, the MMoOn was chosen to assess MG morpheme-based morphological representation while being prepared to incorporate other formation approaches when required by the lexical data. Among others, MMoOn was chosen because of its targeted morphological character, its conceptual granularity, the covering of derivational aspects of morphology, its elasticity of embedding different inflectional language data models and its reference to previous frameworks. Accordingly, the model was appropriately extended for the MG language schema and tested towards a very productive MG derivational pattern revealing its high dynamics of representation and usability as a computed lexical inventory that semantically interlinks its entries. © 2022, Springer Nature Switzerland AG.",Final,,
Linares-Sánchez J.J.; Sánchez-Cuadrado S.; Morato J.,Linked data for the Greek and Latin literatura analysis; [Datos enlazados para el análisis de la literatura grecolatina],1,1,1,#kg,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124721799&doi=10.3989%2fredc.2022.1.1842&partnerID=40&md5=eb0ae79c3e1f596e81ddf6e2275563fe,"The development of a domain ontology for the Greek and Latin literature representation in the form of linked data is described. The principles of the Semantic Web and the semantic diffusion of contents applied to classical Greek-Latin literature are analyzed. The essence of the Methontology methodology has been adapted for the construction of ontologies and a formalized linguistic resource has been implemented. The result of this research has been the development of a pilot project of linked data based on the principles and technologies of Linked Open Data (LOD) in the field of comparative literature, in which the Litcomp ontology has been developed to improve the study of the influence and preservation of Greek and Latin literature. © 2022 CSIC. All Rights Reserved.",Final,All Open Access; Gold Open Access; Green Open Access,
Alcina A.,Using a Linguistic Approach to Represent Terminology in an Ontology,1,1,-1,#outofscope #excluded,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134236644&partnerID=40&md5=53f41bec8389b984b9092912a8c42c77,"Ontology models of lexical and terminological resources need to include the linguistic dimension in a more coherent and adequate way. In general, ontologies describe objects, but the linguistic approach consist of describe terms as words, not as codes or labels. In this work, we will present our ontology model, where terms are the 'individuals' that are the object of classification and linguistic concepts (whether grammatical, as a noun or verb, or morphological, as a full or derived form) constitute the 'classes' into which the terms are classified. This model of lexical representation can enhance the use of ontologies to make dictionaries and contribute to the Web Semantic and Linked Data Linguistics.  © 2022 Copyright for this paper by its authors.",Final,,
Montiel-Ponsoda E.,Terminology and ontologies,1,,-1,#related,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132126845&doi=10.1075%2ftlrp.23.07mon&partnerID=40&md5=11fc6f6ec22c63556237819fdc71f354,"This chapter revisits the relation between terminologies and ontologies, focusing on those aspects that lie at the intersection of the two worlds. Terminological resources, methods and tools that follow the conceptual paradigm are analyzed to explain how close they are to ontologies. Then, current methodologies in Ontology Engineering are reviewed from a terminological perspective to highlight those aspects of terminological practice that are becoming increasingly relevant in an ontology development process. Finally, current trends and models in Ontology Engineering whose aim is to enrich ontologies with linguistic (terminological) descriptions are examined in the light of their use in Artificial Intelligence and Natural Language Processing tools, pointing to a renewed convergence of these two worlds. © 2022 Terminology and Lexicography Research and Practice. All rights reserved.",Final,,
Racioppa S.; Declerck T.,Porting the Latin WordNet onto OntoLex-Lemon,1,1,1,#kg,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137094300&partnerID=40&md5=60cb170f433be37564195d5135fe8da8,"In this paper we describe the porting of the Latin WordNet data available at the University of Exeter onto the OntoLex-Lemon model, focusing on the representation of both morphological and conceptual information. In the longer term, we aim at integrating the resulting data set in the Linguistic Linked Open Data (LLOD) infrastructure, linking (or even merging) it to the Latin data sets already published in the LOD framework by the ERC “Linking Latin” (LILA) project. We discuss some lessons learned, as it turned out that such a transformation and linking exercise can lead to an improved consistency and accuracy of the original data. © 2021 Lexical Computing CZ s.r.o.. All rights reserved.",Final,,
Rackevičienė S.; Utka A.; Mockienė L.; Rokas A.,Methodological Framework for the Development of an English-Lithuanian Cybersecurity Termbase; [Anglų-lietuvių kalbų kibernetinio saugumo terminų bazės kūrimo metodikos modelis],1,1,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121922199&doi=10.5755%2fj01.sal.1.39.29156&partnerID=40&md5=6f901adc6b6422983b5c7aa9b9c25531,"The aim of the paper is to present a methodological framework for the development of an Abstract English-Lithuanian bilingual termbase in the cybersecurity domain, which can be applied as a model for other language pairs and other specialised domains. It is argued that the presented methodological approach can ensure creation of high-quality bilingual termbases even with limited available resources. The paper touches upon the methods and problems of dataset (corpora) compilation, terminology annotation, automatic bilingual term extraction (BiTE) and alignment, knowledge-rich context extraction, and linguistic linked open data (LLOD) technologies. The paper presents theoretical considerations as well as the arguments on the effectiveness of the described methods. The theoretical analysis and a pilot study allow arguing that: 1) a combination of parallel and comparable corpora enable to considerably expand the amount and variety of data sources that can be used for terminology extraction; this methodology is especially important for less-resourced languages which often lack parallel data; 2) deep learning systems trained by using gold standard corpora (manually annotated data) allow effective automatization of extraction of terminological data and metadata, which enables to regularly update termbases with minimised manual input; 3) LLOD technologies enable to integrate the terminological data into the global linguistic data ecosystem and make it reusable, searchable and discoverable across the Web. © 2021 Kaunas University of Technology. All rights reserved.",Final,All Open Access; Gold Open Access; Green Open Access,
Wang L.; Li Y.; Aslan O.; Vinyals O.,WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset,1,1,-1,#excluded #outofscope no linguistic features,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130644669&partnerID=40&md5=4895d13adc9319199642b638967d2d1e,"We present a new dataset ofWikipedia articles each paired with a knowledge graph, to facilitate the research in conditional text generation, graph generation and graph representation learning. Existing graph-text paired datasets typically contain small graphs and short text (1 or few sentences), thus limiting the capabilities of the models that can be learned on the data. Our new datasetWikiGraphs is collected by pairing each Wikipedia article from the established WikiText-103 benchmark (Merity et al., 2016) with a subgraph from the Freebase knowledge graph (Bollacker et al., 2008). This makes it easy to benchmark against other state-of-the-art text generative models that are capable of generating long paragraphs of coherent text. Both the graphs and the text data are of significantly larger scale compared to prior graph-text paired datasets. We present baseline graph neural network and transformer model results on our dataset for 3 tasks: graph → text generation, graph → text retrieval and text → graph retrieval. We show that better conditioning on the graph provides gains in generation and retrieval quality but there is still large room for improvement.  © 2021 Association for Computational Linguistics.",Final,,
Huang H.; Lei M.; Feng C.,Graph-based reasoning model for multiple relation extraction,1,1,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092253997&doi=10.1016%2fj.neucom.2020.09.025&partnerID=40&md5=9f9d541afc98a60e2a8c0c34dd11f21d,"Linguistic knowledge is useful for various NLP tasks, but the difficulty lies in the representation and application. We consider that linguistic knowledge is implied in a large-scale corpus, while classification knowledge, the knowledge related to the definitions of entity and relation types, is implied in the labeled training data. Therefore, a corpus subgraph is proposed to mine more linguistic knowledge from the easily accessible unlabeled data, and sentence subgraphs are used to acquire classification knowledge. They jointly constitute a relation knowledge graph (RKG) to extract relations from sentences in this paper. On RKG, entity recognition can be regarded as a property value filling problem and relation classification can be regarded as a link prediction problem. Thus, the multiple relation extraction can be treated as a reasoning process for knowledge completion. We combine statistical reasoning and neural network reasoning to segment sentences into entity chunks and non-entity chunks, then propose a novel Chunk Graph LSTM network to learn the representations of entity chunks and infer the relations among them. The experiments on two standard datasets demonstrate our model outperforms the previous models for multiple relation extraction. © 2020 Elsevier B.V.",Final,,
AlMousa M.; Benlamri R.; Khoury R.,Exploiting non-taxonomic relations for measuring semantic similarity and relatedness in WordNet,1,1,1,#use,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095852763&doi=10.1016%2fj.knosys.2020.106565&partnerID=40&md5=5573a5a128dd93ea1e4d6a4101770633,"Various applications in computational linguistics and artificial intelligence employ semantic similarity to solve challenging tasks, such as word sense disambiguation, text classification, information retrieval, machine translation, and document clustering. To our knowledge, research to date rely solely on the taxonomic relation “ISA” to evaluate semantic similarity and relatedness between terms. This paper explores the benefits of using all types of non-taxonomic relations in large linked data, such as WordNet knowledge graph, to enhance existing semantic similarity and relatedness measures. We propose a holistic poly-relational approach based on a new relation-based information content and non-taxonomic-based weighted paths to devise a comprehensive semantic similarity and relatedness measure. To demonstrate the benefits of exploiting non-taxonomic relations in a knowledge graph, we used three strategies to deploy non-taxonomic relations at different granularity levels. We conduct experiments on four well-known gold standard datasets. The results of our proposed method demonstrate an improvement over the benchmark semantic similarity methods, including the state-of-the-art knowledge graph embedding techniques, that ranged from 3.8%–23.8%, 1.3%–18.3%, 31.8%–117.2%, and 19.1%–111.1%, on all gold standard datasets MC, RG, WordSim, and Mturk, respectively. These results demonstrate the robustness and scalability of the proposed semantic similarity and relatedness measure, significantly improving existing similarity measures. © 2020 Elsevier B.V.",Final,All Open Access; Green Open Access,
Wood I.D.; Wan S.; Johnson M.,Integrating Lexical Information into Entity Neighbourhood Representations for Relation Prediction,1,1,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137727793&partnerID=40&md5=9aa09388b06ff0f26271296d2d15a2ea,"Relation prediction informed from a combination of text corpora and curated knowledge bases, combining knowledge graph completion with relation extraction, is a relatively little studied task. A system that can perform this task has the ability to extend an arbitrary set of relational database tables with information extracted from a document corpus. OpenKi (Zhang et al., 2019) addresses this task through extraction of named entities and predicates via OpenIE tools then learning relation embeddings from the resulting entity-relation graph for relation prediction, outperforming previous approaches. We present an extension of OpenKi that incorporates embeddings of text-based representations of the entities and the relations. We demonstrate that this results in a substantial performance increase over a system without this information. https://github.com/drevicko/OpenKI. © 2021 Association for Computational Linguistics.",Final,,
Armaselu F.; Apostol E.-S.; Khan A.F.; Liebeskind C.; McGillivray B.; Truică C.-O.; Oleškevičiene G.V.,"HISTORIAE, history of socio-cultural transformation as linguistic data science. A humanities use case",1,1,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115084095&doi=10.4230%2fOASIcs.LDK.2021.34&partnerID=40&md5=890dcdc8d00c64cac8be48579dfddd0d,"The paper proposes an interdisciplinary approach including methods from disciplines such as history of concepts, linguistics, natural language processing (NLP) and Semantic Web, to create a comparative framework for detecting semantic change in multilingual historical corpora and generating diachronic ontologies as linguistic linked open data (LLOD). Initiated as a use case (UC4.2.1) within the COST Action Nexus Linguarum, European network for Web-centred linguistic data science, the study will explore emerging trends in knowledge extraction, analysis and representation from linguistic data science, and apply the devised methodology to datasets in the humanities to trace the evolution of concepts from the domain of socio-cultural transformation. The paper will describe the main elements of the methodological framework and preliminary planning of the intended workflow. © Florentina Armaselu, Elena-Simona Apostol, Anas Fahad Khan, Chaya Liebeskind, Barbara McGillivray, Ciprian-Octavian Truică, and Giedre Valūnaite Oleškevičien e; licensed under Creative Commons License CC-BY 4.0",Final,,
Alberts H.; Huang N.; Deshpande Y.R.; Liu Y.; Cho K.; Vania C.; Calixto I.,VisualSem: A High-quality Knowledge Graph for Vision & Language,1,1,1,#kg,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133780792&partnerID=40&md5=6ba1f09f115e5471cdc85c0285f0b3e2,"An exciting frontier in natural language understanding (NLU) and generation (NLG) calls for (vision-and-) language models that can efficiently access external structured knowledge repositories. However, many existing knowledge bases only cover limited domains, or suffer from noisy data, and most of all are typically hard to integrate into neural language pipelines. To fill this gap, we release VisualSem: a high-quality knowledge graph (KG) which includes nodes with multilingual glosses, multiple illustrative images, and visually relevant relations. We also release a neural multi-modal retrieval model that can use images or sentences as inputs and retrieves entities in the KG. This multi-modal retrieval model can be integrated into any (neural network) model pipeline. We encourage the research community to use VisualSem for data augmentation and/or as a source of grounding, among other possible uses. VisualSem as well as the multi-modal retrieval models are publicly available and can be downloaded in this URL: https://github.com/iacercalixto/visualsem. © 2021 Association for Computational Linguistics.",Final,,
Kirillovich A.; Shaekhov M.; Galieva A.; Nevzorova O.; Ilvovsky D.; Loukachevitch N.,TatWordNet: A linguistic linked open data-integrated WordNet resource for tatar,1,1,1,#kg,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115081049&doi=10.4230%2fOASIcs.LDK.2021.16&partnerID=40&md5=2eed9fc4b40da810c25e9844c161869c,"We present the first release of TatWordNet (http://WordNet.tatar), a WordNet resource for Tatar. TatWordNet has been constructed by the combination of the expand and the merge approaches. The synsets of TatWordNet have been compiled by: (i) the automatic conversion of concepts of TatThes, a socio-political Tatar; (ii) semi-automatic translation of synsets of RuWordNet, a WordNet resource for Russian with the followed manual verification and correction; (iii) manual translation of base RuWordNet synsets; (iv) and manual translation of the all hypernyms of the previously translated RuWordNet synsets. The currents version of TatWordNet contains 18,583 synsets, 36,540 lexical entries and 49,525 senses. The resource has been published to the Linguistic Linked Open Data cloud and interlinked with the Global WordNet Grid. © Alexander Kirillovich, Marat Shaekhov, Alfiya Galieva, Olga Nevzorova, Dmitry Ilvovsky, and Natalia Loukachevitch; licensed under Creative Commons License CC-BY 4.0",Final,,
Pellegrini M.; Litta E.; Passarotti M.; Sprugnoli R.; Mambrini F.; Moretti G.,LiLa Linking Latin Tutorial,1,1,-1,#excluded #tutorial,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126045333&partnerID=40&md5=4631fe94637a8a089afa3248a70ac2cb,"By applying Linked Data and FAIR principles, the LiLa: Linking Latin project makes linguistic resources (e.g. textual corpora, lexica, dictionaries) for Latin interact on the web via a lexical basis made of a collection of lemmas known as the LiLa Lemma Bank. In this hands-on tutorial, participants learned how to link a Latin text to the LiLa Knowledge Base of linguistic resources. By the end of the tutorial participants should have a better understanding of the benefits of linking a Latin text to the LiLa Knowledge Base, and of the work required to help machines process linguistic data and produce quality resources. © 2021 Copyright for this paper by its authors",Final,,
Sprugnoli R.; Passarotti M.; Testori M.; Moretti G.,Extending and Using a Sentiment Lexicon for Latin in a Linked Data Framework,1,1,1,#use,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126062402&partnerID=40&md5=9b41dc7547f84161c3b88dc7d0c48d59,"In this paper we present the methodology followed to extend a Latin sentiment lexicon (called LatinAffectus), the process of inclusion of the lexicon in a knowledge base of interoperable linguistic resources for Latin and one use case performed on the treebank of Dante Alighieri's Latin works annotated following the Universal Dependencies guidelines. In addition, we report on our first attempt at linking the polarity scores of SentiWordNet 3.0 to a manually revised version of Latin WordNet. © 2021 Copyright for this paper by its authors",Final,,
Vasilevich A.; Wetzel M.,Multilingual Knowledge Systems as Linguistic Linked Open Data for European Language Grid,1,1,1,#kg,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125997399&partnerID=40&md5=a5ab353907b44cacfc1c709258748574,"Creation and re-usability of language resources in accordance with Linked Data principles is a valuable asset in the modern data world. In this paper we describe the contributions made to extend the LLOD stack with a new resource, Coreon MKS, bringing together concept-oriented, language-agnostic terminology management and graph-based knowledge organization. We dwell on our approach to mirroring of Coreon's original data structure to RDF and supplying it with a real-time SPARQL endpoint. We integrate MKS into the existing ELG infrastructure, using it as a platform for making the published MKS discoverable and retrievable via a industry-standard interface. While we apply this approach to LLOD-ify Coreon MKS, it can provide a relevant input for standardisation bodies and interoperability communities, acting as a blueprint for similar integration activities. © 2021 Copyright for this paper by its authors",Final,,
Bajčetić L.; Declerck T.,Interlinking slovene language datasets,1,1,1,#kg,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115011533&partnerID=40&md5=2309a0384246dbb077d806ffb819c97c,"We present the current implementation state of our work consisting in interlinking language data and linguistic information included in different types of Slovenian language resources. The types of resources we currently deal with are a lexical database (which also contains collocations and example sentences), a morphological lexicon, and the Slovene WordNet. We first transform the encoding of the original data into the OntoLex-Lemon model and map the different descriptors used in the original sources onto the LexInfo vocabulary. This harmonization step is enabling the interlinking of the various types of information included in the different resources, by using relations defined in OntoLex-Lemon. As a result, we obtain a partial merging of the information that was originally distributed over different resources, which is leading to a cross-enrichment of those original data sources. A final goal of the presented work is to publish the linked and merged Slovene linguistic datasets in the Linguistic Linked Open Data cloud. © 2020, European Association for Lexicography. All rights reserved.",Final,,
Kanojia D.; Dabre R.; Dewangan S.; Bhattacharyya P.; Haffari G.; Kulkarni M.,Harnessing Cross-lingual Features to Improve Cognate Detection for Low-resource Languages,1,1,-1,#outofscope #excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107052021&partnerID=40&md5=d2309ba9f3986911ac6f28964561d9e0,"Cognates are variants of the same lexical form across different languages; for example “fonema” in Spanish and “phoneme” in English are cognates, both of which mean “a unit of sound”. The task of automatic detection of cognates among any two languages can help downstream NLP tasks such as Cross-lingual Information Retrieval, Computational Phylogenetics, and Machine Translation. In this paper, we demonstrate the use of cross-lingual word embeddings for detecting cognates among fourteen Indian Languages. Our approach introduces the use of context from a knowledge graph to generate improved feature representations for cognate detection. We then evaluate the impact of our cognate detection mechanism on neural machine translation (NMT), as a downstream task. We evaluate our methods to detect cognates on a challenging dataset of twelve Indian languages, namely, Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu, Punjabi, Bengali, Marathi, and Malayalam. Additionally, we create evaluation datasets for two more Indian languages, Konkani and Nepali1. We observe an improvement of up to 18% points, in terms of F-score, for cognate detection. Furthermore, we observe that cognates extracted using our method help improve NMT quality by up to 2.76 BLEU. We also release2 our code, newly constructed datasets and cross-lingual models publicly. © 2020 COLING 2020 - 28th International Conference on Computational Linguistics, Proceedings of the Conference. All rights reserved.",Final,,
Gantar P.,Dictionary of modern slovene: From slovene lexical database to digital dictionary database; [Rječnik suvremenoga slovenskog jezika: Od slovenske leksičke baze do digitalne rječničke baze],1,1,-1,#excluded #noLLOD,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096796818&doi=10.20344%2fAMP.12408&partnerID=40&md5=6a2ef1bb8151b1d2558fbfa750c41c20,"The ability to process language data has become fundamental to the development of technologies in various areas of human life in the digital world. The development of digitally readable linguistic resources, methods, and tools is, therefore, also a key challenge for the contemporary Slovene language. This challenge has been recognized in the Slovene language community both at the professional and state level and has been the subject of many activities over the past ten years, which will be presented in this paper. The idea of a comprehensive dictionary database covering all levels of linguistic description in modern Slovene, from the morphological and lexical levels to the syntactic level, has already formulated within the framework of the European Social Fund's Communication in Slovene (2008-2013) project; the Slovene Lexical Database was also created within the framework of this project. Two goals were pursued in designing the Slovene Lexical Database (SLD): creating linguistic descriptions of Slovene intended for human users that would also be useful for the machine processing of Slovene. Ever since the construction of the first Slovene corpus, it has become evident that there is a need for a description of modern Slovene based on real language data, and that it is necessary to understand the needs of language users to create useful language reference works. It also became apparent that only the digital medium enables the comprehensiveness of language description and that the design of the database must be adapted to it from the start. Also, the description must follow best practices as closely as possible in terms of formats and international standards, as this enables the inclusion of Slovene into a wider network of resources, such as Open Linked Data, babelNet and ELExIS. Due to time pressures and trends in lexicography, procedures to automate the extraction of linguistic data from corpora and the inclusion of crowdsourcing into the lexicographic process were taken into consideration. Following the essential idea of creating an all-inclusive digital dictionary database for Slovene, a few independent databases have been created over the past two years: the Collocations Dictionary of Modern Slovene, and the automatically generated Thesaurus of Modern Slovene, both of which also exist as independent online dictionary portals. One of the novelties that we put forward together with both dictionaries is the 'responsive dictionary' concept, which includes crowdsourcing methods. Ultimately, the Digital Dictionary Database provides all (other) levels of linguistic description: the morphological level with the Sloleks database upgrade, the phraseological level with the construction of a multi-word expressions lexicon, and the syntactic level with the formalization of Slovene verb valency patterns. Each of these databases contains its specific language data that will ultimately be included in the comprehensive Slovene Digital Dictionary Database, which will represent basic linguistic descriptions of Slovene both for the human and machine user. © 2020 Institute of Croatian Language and Linguistics. All rights reserved.",Final,All Open Access; Gold Open Access; Green Open Access,
Gatiatullin A.; Kirillovich A.; Nevzorova O.,On developing of the FrameNet-like resource for Tatar,1,1,1,#kg,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098702581&partnerID=40&md5=ae7032e93ab3b0db787a71d2d3c67b9f,"In this paper, we present TatVerbBank, the first FrameNet-like resource for Tatar language. TatVerbBank is organized as a collection of semantic and syntactic frames. A semantic frame contains semantic roles associated with a concept (for example, for the concept of gift, the roles are giver, recipient, gift, time, etc.). A syntactic frame contains a subcategorization model for a particular Tatar lexical entry and its mapping to semantic roles. The developed resource is represented in terms of Lemon, LexInfo and PREMON ontologies and will we published at Linguistic Linked Open Data cloud. © 2020 CEUR-WS. All rights reserved.",Final,,
Martín-Chozas P.,Creation and enrichment of a terminological knowledge graph in the legal domain,1,1,1,#kg,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084616018&partnerID=40&md5=bcb915526ea38ea83a305eab069c6d96,"Domain-specific terminologies are of great use in a number of contexts, such as information retrieval from text documents or supporting humans in translation tasks. However, automated terminology extraction tools usually render plain lists with no additional information (hierarchical relations, definitions or examples of use, amongst others). The output of these tools is very often offered in non-open formats, hampering their reuse and interoperability. Moreover, terminology management tools demand a lot of manual work to curate and enrich the resources and they do not support the representation of terminological relations beyond broader/narrower. The contributions of this Thesis mitigate these problems by automating the creation of rich terminologies from plain text documents, by establishing links to external resources, and by adopting the W3C standards for the Semantic Web. The proposed method comprises six tasks: refinement, disambiguation, enrichment, relation validation, relation extraction and RDF conversion. We have applied this methodology to two different legal corpora, i.e., contracts and collective agreements. The result of this methodology will be a Terminological Knowledge Graph that can be exploited by different Natural Language Processing applications. Copyright © 2020 held by the author.",Final,,
Fiorelli M.; Stellato A.; Lorenzetti T.; Turbati A.; Schmitz P.; Francesconi E.; Hajlaoui N.; Batouche B.,Editing OntoLex-Lemon in VocBench 3,1,-1,-1,#excluded #outofscope tools to make the onotlogy editable,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095127398&partnerID=40&md5=c433d34f15e50fa72bc033afcdab3696,"OntoLex-Lemon is a collection of RDF vocabularies for specifying the verbalization of ontologies in natural language. Beyond its original scope, OntoLex-Lemon, as well as its predecessor Monnet lemon, found application in the Linguistic Linked Open Data cloud to represent and interlink language resources on the Semantic Web. Unfortunately, generic ontology and RDF editors were considered inconvenient to use with OntoLex-Lemon because of its complex design patterns and other peculiarities, including indirection, reification and subtle integrity constraints. This perception led to the development of dedicated editors, trading the flexibility of RDF in combining different models (and the features already available in existing RDF editors) for a more direct and streamlined editing of OntoLex-Lemon patterns. In this paper, we investigate on the benefits gained by extending an already existing RDF editor, VocBench 3, with capabilities closely tailored to OntoLex-Lemon and on the challenges that such extension implies. The outcome of such investigation is twofold: a vertical assessment of a new editor for OntoLex-Lemon and, in the broader scope of RDF editor design, a new perspective on which flexibility and extensibility characteristics an editor should meet in order to cover new core modeling vocabularies, for which OntoLex-Lemon represents a use case. © European Language Resources Association (ELRA), licensed under CC-BY-NC",Final,,
Martín-Chozas P.; Ahmadi S.; Montiel-Ponsoda E.,Defying wikidata: Validation of terminological relations in the web of data,1,1,1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096530716&partnerID=40&md5=8658e3c7154fd8ea391e1f3a2b1933ab,"In this paper we present an approach to validate terminological data retrieved from open encyclopaedic knowledge bases. This need arises from the enrichment of automatically extracted terms with information from existing resources in the Linguistic Linked Open Data cloud. Specifically, the resource employed for this enrichment is WIKIDATA, since it is one of the biggest knowledge bases freely available within the Semantic Web. During the experiment, we noticed that certain RDF properties in the Knowledge Base did not contain the data they are intended to represent, but a different type of information. In this paper we propose an approach to validate the retrieved data based on four axioms that rely on two linguistic theories: the x-bar theory and the multidimensional theory of terminology. The validation process is supported by a second knowledge base specialised in linguistic data; in this case, CONCEPTNET. In our experiment, we validate terms from the legal domain in four languages: Dutch, English, German and Spanish. The final aim is to generate a set of sound and reliable terminological resources in RDF to contribute to the population of the Linguistic Linked Open Data cloud. © European Language Resources Association (ELRA), licensed under CC-BY-NC",Final,,
Sprugnoli R.; Mambrini F.; Moretti G.; Passarotti M.,Towards the modeling of polarity in a Latin knowledge base,1,1,1,#kg,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095966361&partnerID=40&md5=0401ffa5ec60fa70a40954c232432bc9,"In this paper, we describe the process of inclusion of a prior polarity lexicon of Latin lemmas, called LatinAffectus, in a knowledge base of interoperable linguistic resources developed within the LiLa: Linking Latin project. More specifically, a manually-curated list of lemma-sentiment pairs is linked to a comprehensive collection of Latin lemmas by using Semantic Web and Linked Data standards and practices. LatinAffectus is modeled relying on three formal representation frameworks: Lemon and Ontolex to describe the lexicon, and the Marl ontology to describe the sentiment properties of each of its lexical entries. We present the lexicon, the methodology and the results of the linking process, as well as a use case and the planned future work. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",Final,,
Kirillovich A.; Galieva A.; Nevzorova O.; Shaekhov M.; Loukachevitch N.; Ilvovsky D.,Tatar WordNet: The sources and the component parts,1,1,1,#kg,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088754152&doi=10.1007%2f978-3-030-51913-1_13&partnerID=40&md5=4b63210b6ee20e97513a84a03c4f02da,"We describe an ongoing project of construction of the Tatar WordNet. The Tatar WordNet is being constructed on the base of three source resources, developed by us. The first source is TatThes, a bilingual Russian-Tatar Social-Political Thesaurus. TatThes, in turn, has been constructed by manual translation and extension of RuThes, a linguistic ontology for Russian. The second source is a Tatar translation of RuWordNet, a WordNet for Russian. This translation was carried out automatically on the base of a Russian-Tatar dictionary, and then was manually verified. The third source is a semantic classification of Tatar verbs, developed from scratch. We discuss the structure, methodology of compilation and the current state these source resources, and justify the choice of them as the initial resources for building the Tatar WordNet. Our ultimate goal is to publish Tatar WordNet on the Linguistic Linked Open Data cloud and integrate it to the Global WordNet Grid. © Springer Nature Switzerland AG 2020.",Final,,
Nikolaev K.; Kirillovich A.,Adapting the LodView RDF browser for navigation over the linguistic linked open data cloud in Russian and the languages of Russia,1,1,1,#excluded #outofscope LLOD visualization via LodView,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098700630&partnerID=40&md5=57fda6920d57dcc3dd48117335d3dd44,"This paper is dedicated to using of LodView RDF browser for navigation on the Linguistic Linked Open Data cloud in Russian and languages of Russia. We reveal several limitations of LodView, that prevents its using for this purpose. These limitations are: 1) resolution of Cyrillic URIs; 2) Cyrillic URIs in Turtle representations of resources; 3) support for Cyrillic literals; 4) support for URIs with IDs of fragments; 5) human-readable URLs for RDF representations of resources; 6) deployment of embedded resources. We updated the LodView for fix the recovered limitations. © 2020 CEUR-WS. All rights reserved.",Final,,
Gracia J.; Fäth C.; Hartung M.; Ionov M.; Bosque-Gil J.; Veríssimo S.; Chiarcos C.; Orlikowski M.,Leveraging Linguistic Linked Data for Cross-Lingual Model Transfer in the Pharmaceutical Domain,1,1,1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096625207&doi=10.1007%2f978-3-030-62466-8_31&partnerID=40&md5=4760f3754c42b51d2fd3ed570ec01f69,"We describe the use of linguistic linked data to support a cross-lingual transfer framework for sentiment analysis in the pharmaceutical domain. The proposed system dynamically gathers translations from the Linked Open Data (LOD) cloud, particularly from Apertium RDF, in order to project a deep learning-based sentiment classifier from one language to another, thus enabling scalability and avoiding the need of model re-training when transferred across languages. We describe the whole pipeline traversed by the multilingual data, from their conversion into RDF based on a new dynamic and flexible transformation framework, through their linking and publication as linked data, and finally their exploitation in the particular use case. Based on experiments on projecting a sentiment classifier from English to Spanish, we demonstrate how linked data techniques are able to enhance the multilingual capabilities of a deep learning-based approach in a dynamic and scalable way, in a real application scenario from the pharmaceutical domain. © 2020, Springer Nature Switzerland AG.",Final,All Open Access; Green Open Access,
Roche C.; Costa R.; Carvalho S.; Almeida B.,Knowledge-based terminological e-dictionaries: The EndoTerm and al-Andalus Pottery projects,1,-1,-1,#excluded #notavailable,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091466533&doi=10.1075%2fterm.00038.roc&partnerID=40&md5=fdd3c9bf5cdbbd3bce75a974250c6e5c,"The advent of the Semantic Web and of the Linked Data initiative have contributed to new perspectives and opportunities regarding terminology work. Among them are the double dimension approach and the theoretical perspective of ontoterminology anchored therein, which explore the synergies resulting from the systematic organisation of both term systems and concept systems. By doing so, they provide a theoretical and methodological foundation underlying the creation of knowledge-based terminological products that can support the conception and development of different types of e-dictionaries. Within that scope, and based on examples pertaining to two different subject fields, namely endometriosis and Islamic archaeology, this article aims to propose a framework for the creation of a terminological e-dictionary, defined as a reference resource in a specific domain that gathers, structures and describes linguistic data in a systematic way in one, two or more languages, in order to define concepts that are denoted by terms. © John Benjamins Publishing Company",Final,,
León-Araúz P.; Reimerink A.; Faber P.,EcoLexicon and by-products: Integrating and reusing terminological resources,1,1,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091434930&doi=10.1075%2fterm.00037.leo&partnerID=40&md5=79f916658f2e83de90b487fbd84dd647,"Reutilization and interoperability are major issues in the fields of knowledge representation and extraction, as reflected in initiatives such as the Semantic Web and the Linked Open Data Cloud. This paper shows how terminological resources can be integrated and reused within different types of application. EcoLexicon is a multilingual terminological knowledge base (TKB) on environmental science that integrates conceptual, linguistic and visual information. It has led to the following by-products: (i) the EcoLexicon English Corpus; (ii) EcoLexiCAT, a terminology-enhanced translation tool; and (iii) Manzanilla, an image annotation tool. This paper explains EcoLexicon and its by-products, and shows how the latter exploit and enhance the data in the TKB. © John Benjamins Publishing Company",Final,,
Simov K.,Integrated Language and Knowledge Resources for a Bulgarian-Centric Knowledge Graph,1,1,1,#use #integration,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084240830&partnerID=40&md5=f16110765eac6784e8dd66b825ae97ed,"This paper reports on the integration of language and knowledge resources within CLaDA-BG infrastructure. The idea is to encode linguistic knowledge on all levels of language starting from text, grammatical annotation and lemmatization to semantic and conceptual annotation. Our goal is to support conceptual annotation of various research objects (mainly texts). One of the main applications will be the management of a Bulgaria-centric Knowledge Graph. © 2019 Digital Presentation and Preservation of Cultural and Scientific Heritage. All rights reserved.",Final,,
Rospocher M.; Corcoglioniti F.; Palmero Aprosio A.,PreMOn: LODifing linguistic predicate models,1,1,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057966885&doi=10.1007%2fs10579-018-9437-8&partnerID=40&md5=b84ee695965b207c1f9d9ef98c61ddd9,"PreMOn is a freely available linguistic resource for exposing predicate models (PropBank, NomBank, VerbNet, and FrameNet) and mappings between them (e.g., SemLink and the predicate matrix) as linguistic linked open data (LOD). It consists of two components: (1) the PreMOn Ontology, that builds on the OntoLex-Lemon model by the W3C ontology-Lexica community group to enable an homogeneous representation of data from various predicate models and their linking to ontological resources; and, (2) the PreMOn Dataset, a LOD dataset integrating various versions of the aforementioned predicate models and mappings, linked to other LOD ontologies and resources (e.g., FrameBase, ESO, WordNet RDF). PreMOn is accessible online in different ways (e.g., SPARQL endpoint), and extensively documented. © 2018, Springer Nature B.V.",Final,,
Martín-Chozas P.; Montiel-Ponsoda E.; Rodríguez-Doncel V.,Language resources as linked data for the legal domain,1,1,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071015850&doi=10.3233%2fFAIA190019&partnerID=40&md5=cae02938b5a5871653f9f781cb72e542,"This Chapter describes a four-stage methodology to generate Linguistic Linked Data for the legal domain: identification, creation, transformation (to RDF) and linking. The goal of this process is to enhance the presence of legal language resources in the Linguistic Linked Open Data cloud. Since this Chapter is framed within the H2020 LYNX project, aimed at creating a Legal Knowledge Graph, a parallel objective is to employ the resources generated as a linguistic foundation to annotate, classify and translate the legal resources represented in this graph. © 2019 The authors and IOS Press.",Final,,
Speranza G.; Carlino C.; Ahmadi S.,Creating a Multilingual Terminological Resource using Linked Data: The case of Archaeological Domain in the Italian language,1,1,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074832689&partnerID=40&md5=10c0dbfc81df7bc67e5455a6f7bb8809,"The lack of multilingual terminological resources in specialized domains constitutes an obstacle to the access and reuse of information. In the technical domain of cultural heritage and, in particular, archaeology, such an obstacle still exists for Italian language. This paper presents an effort to fill this gap by collecting linguistic data using existing Collaboratively-Constructed Resources and those on the Web of linked data. The collected data are then used to linguistically enrich the ICCD Archaeological Finds Thesaurus– a monolingual Italian thesaurus. Our terminological resource contains 446 terms with translations in four languages and is publicly available in the Resource Description Framework (RDF) in the Ontolex-Lemon model. Copyright © 2019 for this paper by its authors.",Final,,
Dimitrova V.; Fäth C.; Chiarcos C.; Renner-Westermann H.; Abromeit F.,Interoperability of language-related information: Mapping the BLL thesaurus to LEXVO and glottolog,1,1,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059894498&partnerID=40&md5=74ba8aa9e93ca66c4491595ddc11a885,"Since 2013, the thesaurus of the Bibliography of Linguistic Literature (BLL Thesaurus) has been applied in the context of the Linguistik portal, a hub for linguistically relevant information. Several consecutive projects focus on the modeling of the BLL Thesaurus as ontology and its linking to terminological repositories in the Linguistic Linked Open Data (LLOD) cloud. Those mappings facilitate the connection between the Linguistik portal and the cloud. In the paper, we describe the current efforts to establish interoperability between the language-related index terms and repositories providing language identifiers for the web of Linked Data. After an introduction of Lexvo and Glottolog, we outline the scope, the structure, and the peculiarities of the BLL Thesaurus. We discuss the challenges for the design of scientifically plausible language classification and the linking between divergent classifications. We describe the prototype of the linking model and propose pragmatic solutions for structural or conceptual conflicts. Additionally, we depict the benefits from the envisaged interoperability - for the Linguistik portal, and the Linked Open Data Community in general. © LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved.",Final,,
Sakor A.; Mulang I.O.; Singh K.; Shekarpour S.; Vidal M.-E.; Lehmann J.; Auer S.,Old is gold: Linguistic driven approach for entity and relation linking of short text,1,,1,#use,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081090168&partnerID=40&md5=d93ebebd95c7e03c292d4b885c224512,"Short texts challenge NLP tasks such as named entity recognition, disambiguation, linking and relation inference because they do not provide sufficient context or are partially malformed (e.g. wrt. capitalization, long tail entities, implicit relations). In this work, we present the Falcon approach which effectively maps entities and relations within a short text to its mentions of a background knowledge graph. Falcon overcomes the challenges of short text using a light-weight linguistic approach relying on a background knowledge graph. Falcon performs joint entity and relation linking of a short text by leveraging several fundamental principles of English morphology (e.g. compounding, headword identification) and utilizes an extended knowledge graph created by merging entities and relations from various knowledge sources. It uses the context of entities for finding relations and does not require training data. Our empirical study using several standard benchmarks and datasets show that Falcon significantly outperforms state-of-the-art entity and relation linking for short text query inventories. © 2019 Association for Computational Linguistics",Final,,
Passarotti M.C.; Cecchini F.M.; Franzini G.; Litta E.; Mambrini F.; Ruffolo P.,The LILA knowledge base of linguistic resources and NLP tools for latin,1,,1,#ontology #kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069508812&partnerID=40&md5=68fa047075aea852930969826d2ae715,"The LiLa: Linking Latin project was recently awarded funding from the European Research Council to build a Knowledge Base of linguistic resources for Latin. LiLa responds to the growing need in the fields of Computational Linguistics, Humanities Computing and Classics to create an interoperable ecosystem of resources and Natural Language Processing tools for Latin. To this end, LiLa makes use of Linked Open Data practices and standards to connect words to distributed textual and lexical resources via unique identifiers. In so doing, it builds rich knowledge graphs, which can be used for research and teaching purposes alike. This paper details the architecture of the LiLa Knowledge Base and presents the solutions found to address the challenges raised by populating it with a first set of linguistic resources. © Marco C. Passarotti, Flavio M. Cecchini, Greta Franzini, Eleonora Litta, Francesco Mambrini, Paolo Ruffolo.",Final,,
Moussallem D.; Sherif M.A.; Esteves D.; Zampieri M.; Ngomo A.-C.N.,Lidioms: A multilingual linked idioms data set,1,,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059907905&partnerID=40&md5=e142cf84b06aab18e248b52ee5649a37,"In this paper, we describe the LIDIOMS data set, a multilingual RDF representation of idioms currently containing five languages: English, German, Italian, Portuguese, and Russian. The data set is intended to support natural language processing applications by providing links between idioms across languages. The underlying data was crawled and integrated from various sources. To ensure the quality of the crawled data, all idioms were evaluated by at least two native speakers. Herein, we present the model devised for structuring the data. We also provide the details of linking LIDIOMS to well-known multilingual data sets such as BabelNet. The resulting data set complies with best practices according to Linguistic Linked Open Data Community. © LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved.",Final,,
Bosch S.; Eckart T.; Klimek B.; Goldhahn D.; Quasthoff U.,"Preparation and usage of Xhosa lexicographical data for a multilingual, federated environment",1,,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059916234&partnerID=40&md5=cf2477548ac924e66bb8605f1ad5629c,"The South African linguistic landscape is characterised by multilingualism and the influence between their eleven official and some local languages. Unfortunately, for most of the languages the amount and quality of available lexicographical data is suboptimal, even though its availability is essential for all educational institutions and for the development of state-of-the-art language technology. In this paper we present a new source of lexicographical data for Xhosa, a language spoken by more than eight million speakers. For its utilisation in a multilingual and federated environment it is modelled using a dedicated OWL ontology for Bantu languages and possesses all features that are currently considered integral for the promotion of resource reuse as well as long-term usage. In the future, the introduced ontology may be used for other Bantu languages as well and may ease their combination to achieve more extensive, multilingual data stocks. © LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved.",Final,,
Declerck T.; Racioppa S.,Porting multilingual morphological resources to ontolex-lemon,1,,1,#ontology design #kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075340207&doi=10.26615%2f978-954-452-056-4_027&partnerID=40&md5=d9d17387a40bd62f49a73ba72cd38b60,We describe work consisting in porting various morphological resources to the OntoLex-Lemon model. A main objective of this work is to offer a uniform representation of different morphological data sets in order to be able to compare and interlink multilingual resources and to cross-check and interlink or merge the content of morphological resources of one and the same language. The results of our work will be published on the Linguistic Linked Open Data cloud. © 2019 Association for Computational Linguistics (ACL). All rights reserved.,Final,All Open Access; Bronze Open Access,
Chiarcos C.; Ionov M.,Ligt: An LLod-native vocabulary for representing interlinear glossed text as RDF,1,,1,#use,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068056004&doi=10.4230%2fOASIcs.LDK.2019.3&partnerID=40&md5=1194a6ce49aa6b6b03fd05f91410d0ce,"The paper introduces Ligt, a native RDF vocabulary for representing linguistic examples as text with interlinear glosses (IGT) in a linked data formalism. Interlinear glossing is a notation used in various fields of linguistics to provide readers with a way to understand linguistic phenomena and to provide corpus data when documenting endangered languages. This data is usually provided with morpheme-by-morpheme correspondence which is not supported by any established vocabularies for representing linguistic corpora or automated annotations. Interlinear Glossed Text can be stored and exchanged in several formats specifically designed for the purpose, but these differ in their designs and concepts, and they are tied to particular tools, so the reusability of the annotated data is limited. To improve interoperability and reusability, we propose to convert such glosses to a tool-independent representation well-suited for the Web of Data, i.e., a representation in RDF. Beyond establishing structural (format) interoperability by means of a common data representation, our approach also allows using shared vocabularies and terminology repositories available from the (Linguistic) Linked Open Data cloud. We describe the core vocabulary and the converters that use this vocabulary to convert IGT in a format of various widely-used tools into RDF. Ultimately, a Linked Data representation will facilitate the accessibility of language data from less-resourced language varieties within the (Linguistic) Linked Open Data cloud, as well as enable novel ways to access and integrate this information with (L)LOD dictionary data and other types of lexical-semantic resources. In a longer perspective, data currently only available through these formats will become more visible and reusable and contribute to the development of a truly multilingual (semantic) web. © Christian Chiarcos and Maxim Ionov.",Final,,
Sviķe S.; Šķirmante K.,Practice of smart LSP lexicography: The case of a new botanical dictionary with Latvian as a basic language,1,,1,#kg #dataset,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075356206&partnerID=40&md5=7c603666a328df19e7ef900336e1b3fd,"The article provides an insight into the project “A New Botanical Dictionary: Terms in Latvian, Latin, English, Russian, and German” implemented in the second half of 2017 and in 2018 within the Ventspils University of Applied Sciences (VUAS) internal call for proposals “Development of Scientific Activity at the VUAS”. The VUAS Faculty of Translation Studies in collaboration with the Faculty of Information Technologies in their scientific and research work along with other Latvian universities aim to occupy a niche in the branch of applied linguistics, therefore the research is related to this discipline and offers solutions in practical lexicography. The study describes a new botanical dictionary (NBD) - a mobile application prototype - with Latvian as a basic language. An insight into the macrostructure of the dictionary and the structure of entries is given. The research deals with questions concerning IT solutions in general (simple) and semantic search in particular. It also introduces a general search - a morphological approach developed by the authors of the research specifically for the Latvian language; this approach is used to search for Latvian botanical terms in both singular and plural forms. The extracted and linked data methodology developed by the authors is described in detail, as well as the NBD technical solutions and architecture, technologies used, database model, and additional features. © 2019 Lexical Computing CZ s.r.o.. All rights reserved.",Final,,
Ahmadi S.; Hassani H.; McCrae J.P.,Towards electronic lexicography for the Kurdish language,1,,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075376957&partnerID=40&md5=c7ddaad799f8cb6c1d90e9cc4a7b2988,"This paper describes the development of lexicographic resources for Kurdish and provides a lexical model for this language. Kurdish is considered a less-resourced language, and currently, lacks machine-readable lexical resources. The unique potential which Linked Data and the Semantic Web offer to e-lexicography enables interoperability across lexical resources by elevating the traditional linguistic data to machine-processable semantic formats. Therefore, we present our lexicon in Ontolex-Lemon ontology as a standard model for sharing lexical information on the Semantic Web. The research covers the Sorani, Kurmanji, and Hawrami dialects of Kurdish. This research suggests that although Kurdish is a less-resourced language, in terms of documented lexicons, it has a wide range of resources, but because they are not machine-readable they could not contribute to the language processing. The outcome of this project, which is made publicly available, assists scholars in their efforts towards making Kurdish a resource-rich language. © 2019 Lexical Computing CZ s.r.o.. All rights reserved.",Final,,
Orešković M.; Lovrenčić S.; Essert M.,Croatian Network Lexicon within the Syntactic and Semantic Framework and LLOD Cloud,1,,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068895154&doi=10.1093%2fijl%2fecy024&partnerID=40&md5=03c911cc8ea6a4c7ddd946dd3e7c14b7,"This paper presents a new type of network lexicon for the Croatian language based on a syntactic and semantic computational framework. It begins with an overview of the existing Croatian e-dictionaries and online repositories, as well as a brief outline of other relevant network ontological models. The network lexicon, which is based on an innovative approach to word tagging, is described in the remainder of the paper. Instead of presenting a linear (e.g. MULTEX-East) structure, this paper proposes a new hierarchical tree-like T-structure that is very similar to the structure of an ontology. In this approach, each word is processed on multiple levels: From its internal structure (morphs or syllables), via links to external network resources (encyclopaedias), to multiword expressions that can have distinctive roles, such as semantic domains, collocations and even figurative expressions. A network framework facilitates the fetching and filtering of the information related to the searched word in a paradigmatic sense because of the integration of the CroWN, the Croatian version of the English WordNet, and in a syntagmatic sense by building the database of the T-structure patterns from a selected corpus. Finally, the network framework enables the dynamic integration of the lexicon with the Linguistic Linked Open Data cloud; thus, each change in the lexicon will be automatically reflected in the cloud. It is therefore not necessary to perform any periodical synchronisation of the data, a task that is quite common when working with triples stored in a Virtuoso database. Special attention has been paid to the technical components and the data preparation process, which are described in detail to serve as a guide for transforming existing lexicographic data into Linked Open Data triples. © 2019 Oxford University Press. All rights reserved.",Final,,
Passarotti M.; Mambrini F.; Franzini G.; Cecchini F.M.; Litta E.; Moretti G.; Ruffolo P.; Sprugnoli R.,Interlinking through lemmas. The lexical collection of the lila knowledge base of linguistic resources for latin,1,,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094869995&partnerID=40&md5=e0f758c892a82fefabcc553d4d628f83,"This paper presents the structure of the LiLa Knowledge Base, i.e. a collection of multifarious linguistic resources for Latin described with the same vocabulary of knowledge description and interlinked according to the principles of the so-called Linked Data paradigm. Following its highly lexically based nature, the core of the LiLa Knowledge Base consists of a large collection of Latin lemmas, serving as the backbone to achieve interoperability between the resources, by linking all those entries in lexical resources and tokens in corpora that point to the same lemma. After detailing the architecture supporting LiLa, the paper particularly focusses on how we approach the challenges raised by harmonizing different strategies of lemmatization that can be found in linguistic resources for Latin. As an example of the process to connect a linguistic resource to LiLa, the inclusion in the Knowledge Base of a dependency treebank is described and evaluated. © 2019, Edizioni ETS. All rights reserved.",Final,,
Cremaschi M.; Bianchi F.; Maurino A.; Pierotti A.P.,Supporting journalism by combining neural language generation and knowledge graphs,1,,1,#use,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074854423&partnerID=40&md5=deed4622ec53e2a0fcdc4b44f6eb420e,"Natural Language Generation is a field that is becoming relevant in several domains, including journalism. Natural Language Generation techniques can be of great help to journalists, allowing a substantial reduction in the time required to complete repetitive tasks. In this position paper, we enforce the idea that automated tools can reduce the effort required to journalist when writing articles; at the same time we introduce GazelLex (Gazette Lexicalization), a prototype that covers several steps of Natural Language Generation, in order to create soccer articles automatically, using data from Knowledge Graphs, leaving journalists the possibility of refining and editing articles with additional information. We shall present our first results and current limits of the approach, and we shall also describe some lessons learned that might be useful to readers that want to explore this field. Copyright 2019 for this paper by its authors.",Final,,
Eckart T.; Bosch S.; Goldhahn D.; Quasthoff U.; Klimek B.,Translation-based dictionary alignment for under-resourced Bantu languages,1,,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068051098&doi=10.4230%2fOASIcs.LDK.2019.17&partnerID=40&md5=6df49898ec6d6a555b49857c53fd801e,"Despite a large number of active speakers, most Bantu languages can be considered as under- or less-resourced languages. This includes especially the current situation of lexicographical data, which is highly unsatisfactory concerning the size, quality and consistency in format and provided information. Unfortunately, this does not only hold for the amount and quality of data for monolingual dictionaries, but also for their lack of interconnection to form a network of dictionaries. Current endeavours to promote the use of Bantu languages in primary and secondary education in countries like South Africa show the urgent need for high-quality digital dictionaries. This contribution describes a prototypical implementation for aligning Xhosa, Zimbabwean Ndebele and Kalanga language dictionaries based on their English translations using simple string matching techniques and via WordNet URIs. The RDF-based representation of the data using the Bantu Language Model (BLM) and – partial – references to the established WordNet dataset supported this process significantly. © Thomas Eckart, Sonja Bosch, Dirk Goldhahn, Uwe Quasthoff, and Bettina Klimek.",Final,,
Declerck T.; Siegel M.,Porting a crowd-sourced German lexical semantics resource to ontolex-lemon,1,,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075336841&partnerID=40&md5=e137dbb9d781bada28b9018194c5d52b,"In this paper we present our work consisting of mapping the recently created open source German lexical semantics resource “Open-de-WordNet” (OdeNet) into the OntoLex-Lemon format. OdeNet was originally created in order to be integrated in the Open Multilingual WordNet initiative. One motivation for porting OdeNet to OntoLex-Lemon is to publish in the Linguistic Linked Open Data cloud this new WordNet-compliant resource for German. At the same time we can with the help of OntoLex-Lemon link the lemmas of OdeNet to full lexical descriptions and so extend the linguistic coverage of this new WordNet resource, as we did for French, Italian and Spanish WordNets included in the Open Multilingual WordNet collection. As a side effect, the porting of OdeNet to OntoLex-Lemon helped in discovering some issues in the original data. © 2019 Lexical Computing CZ s.r.o.. All rights reserved.",Final,,
Stolk S.,"A thesaurus of old English as linguistic linked data: Using OntoLex, SKOS and lemon-tree to Bring Topical Thesauri to the Semantic Web",1,,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075365453&partnerID=40&md5=317e447506eac5636d3ea2b3ea08c741,"An increasing number of dictionaries are represented on the Web in the form of linguistic linked data, utilizing OntoLex-Lemon for this purpose. Lexicographic resources other than dictionaries, however, have thus far not been the main focus of efforts surrounding this model. In this paper, we discuss porting a topical thesaurus to the Web: A Thesaurus of Old English. By means of this case study, this paper discusses how this thesaurus - and topical thesauri in general - can be represented with OntoLex-Lemon, SKOS and lemon-tree through a fully automated process. Along with discussing the terminology required for expressing A Thesaurus of Old English as linguistic linked data, this paper indicates challenges encountered in the conversion process. These challenges range from material that is not meant to be made available to the general public to distinctions and relations that have been left implicit in the legacy form but are of much value and, indeed, required to be expressed explicitly in its linked data form. The aim of this paper, thus, is to provide recommendations for representing topical thesauri on the Web and to grant insight into aspects that may be encountered in porting similar lexicographic resources in the future. © 2019 Lexical Computing CZ s.r.o.. All rights reserved.",Final,,
Gillis-Webber F.; Tittel S.,The shortcomings of language tags for linked data when modeling lesser-known languages,1,,-1,#excluded,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068056813&doi=10.4230%2fOASIcs.LDK.2019.4&partnerID=40&md5=652704d65b28a7eae4583ddb6be6fb12,"In recent years, the modeling of data from linguistic resources with Resource Description Framework (RDF), following the Linked Data paradigm and using the OntoLex-Lemon vocabulary, has become a prevalent method to create datasets for a multilingual web of data. An important aspect of data modeling is the use of language tags to mark lexicons, lexemes, word senses, etc. of a linguistic dataset. However, attempts to model data from lesser-known languages show significant shortcomings with the authoritative list of language codes by ISO 639: for many lesser-known languages spoken by minorities and also for historical stages of languages, language codes, the basis of language tags, are simply not available. This paper discusses these shortcomings based on the examples of three such languages, i.e., two varieties of click languages of Southern Africa together with Old French, and suggests solutions for the issues identified. © Frances Gillis-Webber and Sabine.",Final,,
Declerck T.; Egorova K.; Schnur E.,An integrated formal representation for terminological and lexical data included in classification schemes,1,,-1,#excluded discussion about ontologies,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059909606&partnerID=40&md5=0b97173848d589671f6bca2eab7e139a,"This paper presents our work dealing with a potential application in e-lexicography: the automatized creation of specialized multilingual dictionaries from structured data, which are available in the form of comparable multilingual classification schemes or taxonomies. As starting examples, we use comparable industry classification schemes, which frequently occur in the context of stock exchanges and business reports. Initially, we planned to follow an approach based on cross-taxonomies and cross-languages string mapping to automatically detect candidate multilingual dictionary entries for this specific domain. However, the need to first transform the comparable classification schemes into a shared formal representation language in order to be able to properly align their components before implementing the algorithms for the multilingual lexicon extraction soon became apparent. We opted for the SKOS-XL vocabulary for modelling the multilingual terminological part of the comparable taxonomies and for OntoLex-Lemon for modelling the multilingual lexical entries which can be extracted from the original data. In this paper, we present the suggested modelling architecture, which demonstrates how terminological elements and lexical items can be formally integrated and explicitly cross-linked in the context of the Linguistic Linked Open Data (LLOD). © LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved.",Final,,
Chiarcos C.; Pagé-Perron É.; Khait I.; Schenk N.; Reckling L.,Towards a linked open data edition of sumerian corpora,1,,1,#use,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056772672&partnerID=40&md5=9c5604b58788ffbe8e0b5eee8b7c38aa,"Linguistic Linked Open Data (LLOD) is a flourishing line of research in the language resource community, so far mostly adopted for selected aspects of linguistics, natural language processing and the semantic web, as well as for practical applications in localization and lexicography. Yet, computational philology seems to be somewhat decoupled from the recent progress in this area: even though LOD as a concept is gaining significant popularity in Digital Humanities, existing LLOD standards and vocabularies are not widely used in this community, and philological resources are underrepresented in the LLOD cloud diagram (http://linguistic-lod.org/llod-cloud). In this paper, we present an application of Linguistic Linked Open Data in Assyriology. We describe the LLOD edition of a linguistically annotated corpus of Sumerian, as well as its linking with lexical resources, repositories of annotation terminology, and the museum collections in which the artifacts bearing these texts are kept. The chosen corpus is the Electronic Text Corpus of Sumerian Royal Inscriptions, a well curated and linguistically annotated archive of Sumerian text, in preparation for the creating and linking of other corpora of cuneiform texts, such as the corpus of Ur III administrative and legal Sumerian texts, as part of the Machine Translation and Automated Analysis of Cuneiform Languages project (https://cdli-gh.github.io/mtaac/). © LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved.",Final,,
Bosque-Gil J.; Lonke D.; Gracia J.; Kernerman I.,Validating the ontolex-lemon lexicography module with K dictionaries' multilingual data,1,,-1,#excluded,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075396460&partnerID=40&md5=6130be10cc1b02b0dfe7b79f47347d08,"The OntoLex-lemon model has gradually acquired the status of de-facto standard for the representation of lexical information according to the principles of Linked Data (LD). Exposing the content of lexicographic resources as LD brings both benefits for their easier sharing, discovery, reusability and enrichment at a Web scale, as well as for their internal linking and better reuse of their components. However, with lemon being originally devised for the lexicalization of ontologies, a 1:1 mapping between its elements and those of a lexicographic resource is not always attainable. In this paper we report our experience of validating the new lexicog module of OntoLex-lemon, which aims at paving the way to bridge those gaps. To that end, we have applied the module to represent lexicographic data coming from the Global multilingual series of K Dictionaries (KD) as a real use case scenario of this module. Attention is drawn to the structures and annotations that lead to modelling challenges, the ways the lexicog module tackles them, and where this modelling phase stands as regards the conversion process and design decisions for KD's Global series. © 2019 Lexical Computing CZ s.r.o.. All rights reserved.",Final,,
Wang C.; Fan Y.; He X.; Zhou A.,Predicting hypernym–hyponym relations for Chinese taxonomy learning,1,,1,#use,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041836685&doi=10.1007%2fs10115-018-1166-1&partnerID=40&md5=372d88ca7d561d032b7c75d1df98b1a5,"Hypernym–hyponym (“is-a”) relations are key components in taxonomies, object hierarchies and knowledge graphs. Robustly harvesting of such relations requires the analysis of the linguistic characteristics of is-a word pairs in the target language. While there is abundant research on is-a relation extraction in English, it still remains a challenge to accurately identify such relations from Chinese knowledge sources due to the flexibility of language expression and the significant differences between the two language families. In this paper, we introduce a weakly supervised framework to extract Chinese is-a relations from user-generated categories. It employs piecewise linear projection models trained on an existing Chinese taxonomy built from Wikipedia and an iterative learning algorithm to update model parameters incrementally. A pattern-based relation selection method is proposed to prevent “semantic drift” in the learning process using bi-criteria optimization. Experimental results on the publicly available test set illustrate that the proposed approach outperforms state-of-the-art methods. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Final,,
Stolk S.,Lemon-tree: Representing topical thesauri on the semantic web,1,,1,#kg #ontology,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068038072&doi=10.4230%2fOASIcs.LDK.2019.16&partnerID=40&md5=476879d1b0d006a47ecf98629ca40617,"An increasing number of dictionaries are represented on the Web in the form of linguistic linked data using the lemon vocabulary. Such a representation facilitates interoperability across linguistic resources, has the potential to increase their visibility, and promotes their reuse. Lexicographic resources other than dictionaries have thus far not been the main focus of efforts surrounding lemon and its modules. In this paper, fundamental needs are analysed for representing topical thesauri specifically and a solution is provided for two important areas hitherto problematic: (1) levels that can be distinguished in their topical system and (2) a looser form of categorization than lexicalization. The novel lemon-tree model contains terminology to overcome these issues and acts as bridge between existing Web standards in order to bring topical thesauri, too, to the Semantic Web. © Sander Stolk.",Final,,
Klimek B.; Schädlich R.; Kröger D.; Knese E.; Elßmann B.,LiDO RDF: From a relational database to a linked data graph of linguistic terms and bibliographic data,1,,1,#kg ,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059893820&partnerID=40&md5=bedc08ce16e4d5a66f66e82e46cdd539,"Forty years ago the linguist Dr. Christian Lehmann developed a framework for documenting linguistic terms, concepts and bibliographic data that resulted in the LiDo Terminological and Bibliographical Database (LiDo TBD). Since 2006 students and linguistic researchers benefit from the data by looking it up on the Web. Even though, the LiDo TBD is implemented as a relational database, its underlying framework aims at yielding a terminological network containing data nodes that are connected via specific relation edges in order to create an interrelated data graph. Now, with the emergence of Semantic Web technologies we were able to implement this pioneering work by converting the LiDo TBD relational database into a Linked Data graph. In this paper we present and describe the creation of the LiDo RDF dataset and introduce the LiDo RDF project. The goals of this project are to enable the direct use and reuse of the data both for the scientific research community and machine processing alike as well as to enable a valuable enrichment of already existing linguistic terminological and bibliographic data by including LiDo RDF in the LLOD cloud. © LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved.",Final,,
Huang C.-R.; Hsieh S.-K.; Prévot L.; Hsiao P.-Y.; Chang H.Y.,Linking basic lexicon to shared ontology for endangered languages: A linked data approach toward formosan languages,1,,-1,#excluded,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064702571&doi=10.1353%2fjcl.2018.0009&partnerID=40&md5=3d66b43be4467b398e9b147c0966175f,"This paper proposes an innovative approach to link basic lexicon (e.g. Swadesh list) to upper ontology as the foundation of OntoLex interface to address the challenge of building language resources for endangered languages in the linked data paradigm. A linked data approach to language resources requires existing, and preferably sizable, language resources. For endangered and other less-resourced languages, however, the scarcity of existing resources limits the possibilities and potential benefits of linking. The challenges are then, how can construction of language resources for endangered language continue to thrive in the linked data paradigm, and how can the linked data approach benefit language resources for endangered languages. Our proposal requires the bare minimum of available data and we show with examples from Formosan languages (Austronesian or aboriginal languages of Taiwan (Blust 2013, 20))i that 1) this approach is applicable to endangered languages, and that 2) in spite of the restrictions imposed by scarcity of resources, the linked linguistic data consisting of basic lexicon + upper ontology generate important new information. Comparing Swadesh lists from different languages allowed us to build a small shared ontology that reflects direct human experience, and can serve as the cross-lingual conceptual core. In addition, these micro-ontologized lexicons can be used as seeds for developing a fully-grown and more comprehensive documentation of linguistically motivated ontology for each language. Copyright © 2018 by the Journal of Chinese Linguistics. All rights reserved.",Final,All Open Access; Green Open Access,
Chiarcos C.; Khait I.; Pagé-Perron É.; Schenk N.; Jayanth; Fäth C.; Steuer J.; Mcgrath W.; Wang J.,Annotating a low-resource language with LLOD technology: Sumerian morphology and syntax,1,,-1,#excluded #old a more recent work is already considered,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056770596&doi=10.3390%2finfo9110290&partnerID=40&md5=130578f66b8e56ab08efe6cc1b2e3005,"This paper describes work on the morphological and syntactic annotation of Sumerian cuneiformas amodel for lowresource languages in general. Cuneiformtexts are invaluable sources for the study of history, languages, economy, and cultures of AncientMesopotamia and its surrounding regions. Assyriology, the discipline dedicated to their study, has vast research potential, but lacks the modern means for computational processing and analysis. Our project, Machine Translation and Automated Analysis of Cuneiform Languages, aims to fill this gap by bringing together corpus data, lexical data, linguistic annotations and object metadata. The project's main goal is to build a pipeline for machine translation and annotation of Sumerian Ur III administrative texts. The rich and structured data is then to bemade accessible in the formof (Linguistic) Linked Open Data (LLOD), which should open themto a larger research community. Our contribution is two-fold: in terms of language technology, our work represents the first attempt to develop an integrative infrastructure for the annotation of morphology and syntax on the basis of RDF technologies and LLOD resources. With respect to Assyriology, we work towards producing the first syntactically annotated corpus of Sumerian. © 2018 by the authors.",Final,All Open Access; Gold Open Access; Green Open Access,
Khan A.F.,Towards the representation of etymological data on the semantic web,1,,1,#ontology #etimology  #kg,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059041892&doi=10.3390%2finfo9120304&partnerID=40&md5=b9cd784d03714065e3bf861ca6c85ccb,"In this article, we look at the potential for a wide-coverage modelling of etymological information as linked data using the Resource Data Framework (RDF) data model. We begin with a discussion of some of the most typical features of etymological data and the challenges that these might pose to an RDF-based modelling. We then propose a new vocabulary for representing etymological data, the Ontolex-lemon Etymological Extension (lemonETY), based on the ontolex-lemon model. Each of the main elements of our new model is motivated with reference to the preceding discussion. © 2018 by the authors.",Final,All Open Access; Gold Open Access,
Scholz J.; Hrastnig E.; Wandl-Vogt E.,A spatio-temporal linked data representation for modeling spatio-temporal dialect data,1,,-1,#excluded #notavailable,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031278354&doi=10.1007%2f978-3-319-63946-8_44&partnerID=40&md5=67f08b1802bbfabac30c7639b4b464b4,"Collections of linguistic and dialect data often lack a semantic description and the ability to establish relations to external datasets, from e.g. demography, socio-economics, or geography. Based on existing projects—the Database of Bavarian Dialects in Austria and exploreAT!—this paper elaborates on a spatio-temporal Linked Data model for representing linguistic/dialect data. Here we focus on utilizing existing data and publishing them using a virtual RDF graph. Additionally, we exploit external data sources like DBPedia and geonames.org, to specify the meaning of dialect records and make use of stable geographical placenames. In the paper we highlight a spatio-temporal modeling and representation of linguistic records relying on the notion of a discrete lifespan of an object. Based on a real-world example—using the lemma “Karotte” (engl. carrot) we show how the usage of a specific dialect word (“Karottn”) changes from 1916 until 2016—by exploiting the expressive power of GeoSPARQL. © Springer International Publishing AG 2018.",Final,,
McCrae J.P.,Mapping WordNet instances to Wikipedia,1,,1,#kg,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043690969&partnerID=40&md5=9981621542ca07a208ae63cbc325a0d1,"Lexical resource differ from encyclopaedic resources and represent two distinct types of resource covering general language and named entities respectively. However, many lexical resources, including Princeton WordNet, contain many proper nouns, referring to named entities in the world yet it is not possible or desirable for a lexical resource to cover all named entities that may reasonably occur in a text. In this paper, we propose that instead of including synsets for instance concepts PWN should instead provide links to Wikipedia articles describing the concept. In order to enable this we have created a gold-quality mapping between all of the 7,742 instances in PWN and Wikipedia (where such a mapping is possible). As such, this resource aims to provide a gold standard for link discovery, while also allowing PWN to distinguish itself from other resources such as DBpedia or BabelNet. Moreover, this linking connects PWN to the Linguistic Linked Open Data cloud, thus creating a richer, more usable resource for natural language processing. © 2018 Global WordNet Association. All rights reserved.",Final,,
Perera R.; Nand P.,An ensemble architecture for linked data lexicalization,1,,-1,#excluded #noLLOD,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055443976&doi=10.1007%2f978-3-319-77113-7_34&partnerID=40&md5=326adfaeb7d1d3d741e9e860049613ce,"Linked Data has revamped the representation of knowledge by introducing the triple data structure which can encode knowledge with the associated semantics including the context by interlinking with external resources across documents. Although Linked Data is an attractive and effective mechanism to represent knowledge as created and consumed by humans in the form of a natural language, it still has a dimension of separation from natural language. Hence, in recent times, there has been an increase interest in transforming Linked Data into natural language in order to harness the benefits of Linked Data in applications interacting with natural language. This paper presents a framework that lexicalizes the Linked Data triples into natural language using an ensemble architecture. The proposed architecture is comprised of four different pattern based modules which lexicalize triples by analysing the triple features. The four pattern mining modules are based on occupational metonyms, Context Free Grammar (CFG), relation extraction using Open Information Extraction (OpenIE), and triple properties. The framework was evaluated using a two-fold evaluation process consisting of linguistic accuracy analysis and human evaluation for a test sample. The linguistic accuracy evaluation showed that the framework can produce 283 accurate lexicalization patterns for a set of 25 ontology classes resulting in a 70.75% accuracy, which is an approximately 91% increase compared to the existing state-of-the-art model. © Springer Nature Switzerland AG 2018.",Final,,
Kesäniemi J.; Vartiainen T.; Säily T.; Nevalainen T.,Open Science for English historical corpus linguistics: Introducing the language change database,1,,-1,#nopeerreviewed #excluded preface,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045345125&partnerID=40&md5=9e25e1611a4212f5dfb8679b09cfef7f,"This paper discusses the development of an open-Access resource that can be used as a baseline for new corpus-linguistic research into the history of English: The Language Change Database (LCD). The LCD draws together information extracted from hundreds of corpusbased articles that investigate the ways in which English has changed in the course of history. The database includes annotated summaries of the articles, as well as numerical data extracted from the articles and transformed into machine-readable form, thus providing scholars of English with the opportunity to study fundamental questions about the nature, rate and direction of language change. It will also make the work done in the field more cumulative by ensuring that the research community will have continuous access to existing results and research data. We will also introduce a tool that takes advantage of this new source of structured research data. The LCD Aggregated Data Analysis workbench (LADA) makes use of annotated versions of the numerical data available from the LCD and provides a workflow for performing meta-Analytical experimentations with an aggregated set of data tables from multiple publications. Combined with the LCD as the source of collaborative, trusted and curated linked research data, the LADA meta-Analysis tool demonstrates how open data can be used in innovative ways to support new research through data-driven aggregation of empirical findings in the context of historical linguistics. © 2018 CEUR-WS. All rights reserved.",Final,,
Simov K.; Osenova P.,Special thematic section on semantic models for natural language processing,1,,1,#use,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044005006&doi=10.2478%2fcait-2018-0008&partnerID=40&md5=96a18a0b072e4fb358c5aaff3fb263de,"With the availability of large language data online, cross-linked lexical resources (such as BabelNet, Predicate Matrix and UBY) and semantically annotated corpora (SemCor, OntoNotes, etc.), more and more applications in Natural Language Processing (NLP) have started to exploit various semantic models. The semantic models have been created on the base of LSA, clustering, word embeddings, deep learning, neural networks, etc., and abstract logical forms, such as Minimal Recursion Semantics (MRS) or Abstract Meaning Representation (AMR), etc. Additionally, the Linguistic Linked Open Data Cloud has been initiated (LLOD Cloud) which interlinks linguistic data for improving the tasks of NLP. This cloud has been expanding enormously for the last four-five years. It includes corpora, lexicons, thesauri, knowledge bases of various kinds, organized around appropriate ontologies, such as LEMON. The semantic models behind the data organization as well as the representation of the semantic resources themselves are a challenge to the NLP community. The NLP applications that extensively rely on the above discussed models include Machine Translation, Information Extraction, Question Answering, Text Simplification, etc. © 2001-2018 Institute of Information and Communication Technologies at Bulgarian Academy of Sciences.",Final,All Open Access; Gold Open Access,
Me´száros T.; Kiss M.,The DHmine dictionary work-flow: Creating a knowledge-based author's dictionary,1,,-1,#excluded #outofscope #noLLOD,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050453108&partnerID=40&md5=9c736fc9bdf2d1c9b5e698f73b2e7e79,"Digitalized author's dictionaries could play an important role in humanities research. Not only could they provide better ways to study an individual author's vocabulary, but they could also act as a knowledge source for other computer-based methods. We present the process of making an author's dictionary of headwords, writing variations, word forms and corpus citations extended with part-of-speech, linguistic, literary and semantic information. We also describe how this extended dictionary incorporates knowledge from linked open data sources and from critical annotations and builds an RDF knowledge base attached to the dictionary. The result is a vast knowledge source about an author's oeuvre that can be studied and used to enhance corpus analysis. We demonstrate our method on processing a large text corpora of 1.5 million words from the 18th century and on creating the digital author's dictionary of Kelemen Mikes. © Lexicography in Global Contexts.",Final,,
Supriyono P.; Scheider S.,Translating verbally communicated local geographic knowledge using semantic technologies: A balinese example,1,,-1,#excluded #notFreelyAvailable,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031301869&doi=10.1007%2f978-3-319-63946-8_36&partnerID=40&md5=52d134b09920a079367628ae50aa167e,"Using cognitive linguistic strategies, people can verbally encode and convey their spatial realities with little effort (i.e. “my house is right across the street from the grocery store”). However, to date there are a limited number of ways to transform such spatial information into forms that are useful for computational analysis in a geographic information system (GIS), and for sharing across research communities. This paper uses a case study in the Balinese language to investigate the spatial and linguistic information necessary to compute such transformations. That is, to transform verbally communicated spatial scenes into GIS-suitable data. We propose an ontology which captures reference frames used in certain Balinese locative expressions together with the parameters (ground, direction and template) required for transformation. The approach allows for the sharing of translation methods and the reuse of contextual information on the Web. Based on this model, we identify open research questions on the way to supporting approximate transformations of locative expressions. © Springer International Publishing AG 2018.",Final,,
Gangemi A.; Alam M.; Presutti V.,Linked metaphors,1,,1,#excluded #outofscope,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055349844&partnerID=40&md5=dd8e69433a9ec72725d4e8b1f70af660,"The poster summarizes Amnestic Forgery, an ontology for metaphor semantics, based on MetaNet and Framester factual-linguistic linked data. An example of metaphor generation based on linked metaphors is shown. © 2018 CEUR-WS. All rights reserved.",Final,,
Jiang Z.; Gu Q.; Yin Y.; Chen D.,Enriching word embeddings with domain knowledge for readability assessment,1,,-1,#excluded #outofscope #no lingusitic #noLLOD,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119407962&partnerID=40&md5=5e5d5bf1d59a8e84a572d8b7dd592b78,"In this paper, we present a method which learns the word embedding for readability assessment. For the existing word embedding models, they typically focus on the syntactic or semantic relations of words, while ignoring the reading difficulty, thus they may not be suitable for readability assessment. Hence, we provide the knowledge-enriched word embedding (KEWE), which encodes the knowledge on reading difficulty into the representation of words. Specifically, we extract the knowledge on word-level difficulty from three perspectives to construct a knowledge graph, and develop two word embedding models to incorporate the difficulty context derived from the knowledge graph to define the loss functions. Experiments are designed to apply KEWE for readability assessment on both English and Chinese datasets, and the results demonstrate both effectiveness and potential of KEWE. © 2018 COLING 2018 - 27th International Conference on Computational Linguistics, Proceedings. All rights reserved.",Final,,
Gracia J.; Villegas M.; Gómez-Pérez A.; Bel N.,The apertium bilingual dictionaries on the web of data,1,,1,#kg,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048132779&doi=10.3233%2fSW-170258&partnerID=40&md5=41cd0efe9b873ab82788a607f6819531,"Bilingual electronic dictionaries contain collections of lexical entries in two languages, with explicitly declared translation relations between such entries. Nevertheless, they are typically developed in isolation, in their own formats and accessible through proprietary APIs. In this paper we propose the use of Semantic Web techniques to make translations available on the Web to be consumed by other semantic enabled resources in a direct manner, based on standard languages and query means. In particular, we describe the conversion of the Apertium family of bilingual dictionaries and lexicons into RDF (Resource Description Framework) and how their data have been made accessible on the Web as linked data. As a result, all the converted dictionaries (many of them covering under-resourced languages) are connected among them and can be easily traversed from one to another to obtain, for instance, translations between language pairs not originally connected in any of the original dictionaries. © 2018 - IOS Press and the authors. All rights reserved.",Final,All Open Access; Green Open Access,
Stolz A.; Hepp M.; Boggs R.A.,Linked open data for linguists: Publishing the hartmann von aue-portal in RDF,1,,1,#kg,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032680307&doi=10.1007%2f978-3-319-69459-7_20&partnerID=40&md5=cf3fed95bfa3f1e8423819bbc19e7333,"The Hartmann von Aue-portal is a decade-long initiative to employ Web technology in order to support the study of the early German. It provides a comprehensive knowledge base on lexicographic and other aspects of the works of Hartmann von Aue, one of the key epic poets of Middle High German literature; namely lemmata, word forms, tagmemes, adverbs, and the like, including original contexts for entries. The portal is available for human users in the form of a Web application. Linked Open Data (LOD) is a recent approach in the evolution of Web technology that supports the publication of information on the Web in a way suitable for the intelligent consumption and processing of contents by computers instead of humans using Web browsers. In this paper, we study the use of modern LOD approaches for linguistics, describe the conversion of the complete Hartmann von Aue-portal into LOD, and show the usage for data-driven analyses via SPARQL queries and literate programming with Python. © 2017, Springer International Publishing AG.",Final,,
Loughnane R.; McCurdy K.; Kolb P.; Selent S.,Linked data for language-learning applications,1,,-1,#related,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076448651&partnerID=40&md5=6d479791bc2a43179a7e9b2c1f4b6143,"The use of linked data within languagelearning applications is an open research question. A research prototype is presented that applies linked-data principles to store linguistic annotation generated from language-learning content using a variety of NLP tools. The result is a database that links learning content, linguistic annotation and open-source resources, on top of which a diverse range of tools for language-learning applications can be built. © EMNLP 2017 - 12th Workshop on Innovative Use of NLP for Building Educational Applications, BEA 2017 - Proceedings of the Workshop. All rights reserved.",Final,,
Kirillovich A.; Nevzorova O.; Gimadiev E.; Loukachevitch N.,RuThes cloud: Towards a multilevel linguistic linked open data resource for Russian,1,,-1,#excluded #notavailable,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034266316&doi=10.1007%2f978-3-319-69548-8_4&partnerID=40&md5=429af2d4ae4b394cf5134c923a203adb,"In this paper we present a new multi-level Linguistic Linked Open Data resource for Russian. It covers four linguistic levels: semantic, lexical, morphological and syntactic. The resource has been constructed on base of the well-known RuThes thesaurus and the original hitherto unpublished Extended Zaliznyak grammatical dictionary. The resource is represented in terms of SKOS, Lemon, and LexInfo ontologies and a new custom ontology. Building the resource, we automatically completed the following tasks: merging source resources upon common lexical entries, decomposing complex lexical entries, and publishing constructed resource as LLOD-compatible dataset. We demonstrate the use case in which the developed resource is exploited in IR task. We hope that our work can serve as a crystallization point of the LLOD cloud in Russian. © 2017, Springer International Publishing AG.",Final,,
Roa-Valverde A.J.; Sanchez-Alonso S.; Sicilia M.-A.; Fensel D.,An approach to measuring and annotating the confidence of Wiktionary translations,1,,-1,#excluded it is just a proof of concept,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011711366&doi=10.1007%2fs10579-017-9384-9&partnerID=40&md5=66c55efe68e8c2789eea4d671944de2e,"Wiktionary is an online collaborative project based on the same principle than Wikipedia, where users can create, edit and delete entries containing lexical information. While the open nature of Wiktionary is the reason for its fast growth, it has also brought a problem: how reliable is the lexical information contained in every article? If we are planing to use Wiktionary translations as source content to accomplish a certain use case, we need to be able to answer this question and extract measures of their confidence. In this paper we present our work on assessing the quality of Wiktionary translations by introducing confidence metrics. Additionally, we describe our effort to share Wiktionary translations and the associated confidence values as linked data. © 2017, Springer Science+Business Media Dordrecht.",Final,,
Bosque-Gil J.; Gracia J.; Montiel-Ponsoda E.,Towards a module for lexicography in ontolex,1,,-1,#excluded,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029164729&partnerID=40&md5=59590259367673c6dda5b47b3936819d,"Dictionaries are increasingly being transformed into linguistic linked lata (LLD) relying on the lemon and OntoLex models, but this conversion is not always straightforward. For both linked data (LD) based applications to exploit all content provided in dictionaries and lexicographers adopting LD technologies, the original data and structure should be retrievable from the LLD version to prevent any loss of information in the transformation. In this position statement we motivate the need for a new module in OntoLex targeted at the representation of dictionaries and which will address structures and annotations commonly found in lexicography. Some of the issues we identified in our initial experiences are presented as input for discussion, along with our initial approaches to solve them. Such a module is intended to be compatible with other modules in OntoLex and should guarantee information preservation, making LD a viable mechanism for lexicographers in the development of lexica.",Final,,
Tchechmedjiev A.; Mandon T.; Lafourcade M.; Laurent A.; Todorov K.,Ontolex JeuxDeMots and its alignment to the linguistic linked open data cloud,1,,1,#kg,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032199318&doi=10.1007%2f978-3-319-68288-4_40&partnerID=40&md5=0db3c34e3d720f2de18b82e094d3c894,"JeuxDeMots (JdM) is a rich collaborative lexical network in French, built on a crowdsourcing principle as a game with a purpose, represented in an ad-hoc tabular format. In the interest of reuse and interoperability, we propose a conversion algorithm for JdM following the Ontolex model, along with a word sense alignment algorithm, called JdMBabelizer, that anchors JdM sense-refinements to synsets in the lemon edition of BabelNet and thus to the Linguistic Linked Open Data cloud. Our alignment algorithm exploits the richness of JdM in terms of weighted semantic-lexical relations—particularly the inhibition relation between senses—that are specific to JdM. We produce a reference alignment dataset for JdM and BabelNet that we use to evaluate the quality of our algorithm and that we make available to the community. The obtained results are comparable to those of state of the art approaches. © Springer International Publishing AG 2017.",Final,All Open Access; Green Open Access,
McCrae J.P.; Wood I.; Hicks A.,The colloquial WordNet: Extending Princeton WordNet with neologisms,0/1,,1,#kg,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021200771&doi=10.1007%2f978-3-319-59888-8_17&partnerID=40&md5=342fddfc3c9aa9e763e0926c66e2b925,"Princeton WordNet is one of the most important resources for natural language processing, but has not been updated for over ten years and is not suitable for analyzing the fast moving language as used on social media. We propose an extension to WordNet, with new terms that have been found from Twitter and Reddit, and cover language usage that is emergent or vulgar. In addition to our methodology for extraction, we analyze new terms to provide information about how new words are entering the English language. Finally, we discuss publishing this resource both as linguistic linked open data and as part of the Global WordNet Association’s Interlingual Index. © Springer International Publishing AG 2017.",Final,All Open Access; Green Open Access,
Li N.; Sun J.,Improving Chinese term association from the linguistic perspective,0/1,,-1,#excluded #noLLOD #noLinkedDataResource,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015800333&doi=10.5771%2f0943-7444-2017-1-13&partnerID=40&md5=67bc39c511ee3cdf7051e756735b5646,"The study aims to solve how to construct the semantic relations of specific domain terms by applying linguistic rules. The semantic structure analysis at the morpheme level was used for semantic measure, and a morpheme-based term association model was proposed by improving and combining the literal-based similarity algorithm and co-occurrence relatedness methods. This study provides a novel insight into the method of semantic analysis and calculation by morpheme parsing, and the proposed solution is feasible for the automatic association of compound terms. The results show that this approach could be used to construct appropriate term association and form a reasonable structural knowledge graph. However, due to linguistic differences, the viability and effectiveness of the use of our method in non-Chinese linguistic environments should be verified.",Final,,
Rouces J.; De Melo G.; Hose K.,FrameBase: Enabling integration of heterogeneous knowledge,0/1,,1,#use,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027393990&doi=10.3233%2fSW-170279&partnerID=40&md5=9ac1b3786a699d788b9981f53c067da2,"Large-scale knowledge graphs such as those in the Linked Open Data cloud are typically stored as subject-predicate-object triples. However, many facts about the world involve more than two entities. While n-ary relations can be converted to triples in a number of ways, unfortunately, the structurally different choices made in different knowledge sources significantly impede our ability to connect them. They also increase semantic heterogeneity, making it impossible to query the data concisely and without prior knowledge of each individual source. This article presents FrameBase, a wide-coverage knowledge base schema that uses linguistic frames to represent and query n-ary relations from other knowledge bases, providing multiple levels of granularity connected via logical entailment. Overall, this provides a means for semantic integration from heterogeneous sources under a single schema and opens up possibilities to draw on natural language processing techniques for querying and data mining. © 2017 - IOS Press and the authors. All rights reserved.",Final,All Open Access; Green Open Access,
Chiarcos C.; Ionov M.; Rind-Pawlowski M.; Fäth C.; Schreur J.W.; Nevskaya I.,LLODifying linguistic glosses,0/1,,1,#use,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021193597&doi=10.1007%2f978-3-319-59888-8_7&partnerID=40&md5=ef597d9ae10042b6d31316b2cdcc00b1,"Interlinear glossed text (IGT) is a notation used in various fields of linguistics to provide readers with a way to understand the linguistic phenomena. We describe the representation of IGT data in RDF, the conversion from two popular tools, and their automated linking with resources from the Linguistic Linked Open Data (LLOD) cloud. We argue that such an LLOD edition of IGT data facilitates their reusability, their infrastructural support and their integration with external data sources. Our converters are available under an open source license, two data sets will be published along with the final version of this paper. To our best knowledge, this is the first attempt to publish IGT data sets as Linguistic Linked Open Data we are aware of. © Springer International Publishing AG 2017.",Final,,
Falk I.; Stein A.,"LVF-lemon - Towards a linked data representation of ""Les Verbes français""",0/1,,-1,#outofscope #excluded,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037070562&partnerID=40&md5=365a74903d639d09137482d95e151c55,In this study we elaborate a road map for the conversion of a traditional lexical syntactico-semantic resource for French into a linguistic linked open data (LLOD) model. Our approach uses current best-practices and the analyses of earlier similar undertakings (lemonUBY and PDEV-lemon) to tease out the most appropriate representation for our resource.,Final,,
List J.-M.; Cysouw M.; Forkel R.,Concepticon: A resource for the linking of concept lists,0/1,,1,#use,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037152460&partnerID=40&md5=3122a0ba89f81396469e2cb366878186,"We present an attempt to link the large amount of different concept lists which are used in the linguistic literature, ranging from Swadesh lists in historical linguistics to naming tests in clinical studies and psycholinguistics. This resource, our Concepticon, links 30 222 concept labels from 160 conceptlists to 2495 concept sets. Each concept set is given a unique identifier, a unique label, and a human-readable definition. Concept sets are further structured by defining different relations between the concepts. The resource can be used for various purposes. Serving as a rich reference for new and existing databases in diachronic and synchronic linguistics, it allows researchers a quick access to studies on semantic change, cross-linguistic polysemies, and semantic associations.",Final,,
Klimek B.; Arndt N.; Krause S.; Arndt T.,Creating linked data morphological language resources with MMoOn the Hebrew Morpheme Inventory,0/1,,1,#ontology #kg,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021200234&partnerID=40&md5=cfec73254b9b1bb4e4b3e76c31ca99fb,"The development of standard models for describing general lexical resources has led to the emergence of numerous lexical datasets of various languages in the Semantic Web. However, there are no models that describe the domain of morphology in a similar manner. As a result, there are hardly any language resources of morphemic data available in RDF to date. This paper presents the creation of the Hebrew Morpheme Inventory from a manually compiled tabular dataset comprising around 52.000 entries. It is an ongoing effort of representing the lexemes, word-forms and morphologigal patterns together with their underlying relations based on the newly created Multilingual Morpheme Ontology (MMoOn). It will be shown how segmented Hebrew language data can be granularly described in a Linked Data format, thus, serving as an exemplary case for creating morpheme inventories of any inflectional language with MMoOn. The resulting dataset is described a) according to the structure of the underlying data format, b) with respect to the Hebrew language characteristic of building word-forms directly from roots, c) by exemplifying how inflectional information is realized and d) with regard to its enrichment with external links to sense resources.",Final,,
Krause S.; Hennig L.; Moro A.; Weissenborn D.; Xu F.; Uszkoreit H.; Navigli R.,Sar-graphs: A language resource connecting linguistic knowledge with semantic relations from knowledge graphs,,,1,#kg,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979468241&doi=10.1016%2fj.websem.2016.03.004&partnerID=40&md5=d542dd33fc6a5fbc7011ea0a08f87328,"Recent years have seen a significant growth and increased usage of large-scale knowledge resources in both academic research and industry. We can distinguish two main types of knowledge resources: those that store factual information about entities in the form of semantic relations (e.g., Freebase), namely so-called knowledge graphs, and those that represent general linguistic knowledge (e.g., WordNet or UWN). In this article, we present a third type of knowledge resource which completes the picture by connecting the two first types. Instances of this resource are graphs of semantically-associated relations (sar-graphs), whose purpose is to link semantic relations from factual knowledge graphs with their linguistic representations in human language. We present a general method for constructing sar-graphs using a language- and relation-independent, distantly supervised approach which, apart from generic language processing tools, relies solely on the availability of a lexical semantic resource, providing sense information for words, as well as a knowledge base containing seed relation instances. Using these seeds, our method extracts, validates and merges relation-specific linguistic patterns from text to create sar-graphs. To cope with the noisily labeled data arising in a distantly supervised setting, we propose several automatic pattern confidence estimation strategies, and also show how manual supervision can be used to improve the quality of sar-graph instances. We demonstrate the applicability of our method by constructing sar-graphs for 25 semantic relations, of which we make a subset publicly available at http://sargraph.dfki.de. We believe sar-graphs will prove to be useful linguistic resources for a wide variety of natural language processing tasks, and in particular for information extraction and knowledge base population. We illustrate their usefulness with experiments in relation extraction and in computer assisted language learning. © 2016 Elsevier B.V.",Final,,
Lenz M.; Bergmann R.,Case-Based Adaptation of Argument Graphs with WordNet and Large Language Models,,,-1,#excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172236578&doi=10.1007%2f978-3-031-40177-0_17&partnerID=40&md5=16c053188e78a303828ea1b95c941983,"Finding information online is hard, even more so once you get into the domain of argumentation. There have been developments around the specialized argumentation machines that incorporate structural features of arguments, but all current approaches share one pitfall: They operate on a corpora of limited sizes. Consequently, it may happen that a user searches for a rather general term like cost increases, but the machine is only able to serve arguments concerned with rent increases. We aim to bridge this gap by introducing approaches to generalize/specialize a found argument using a combination of WordNet and Large Language Models. The techniques are evaluated on a new benchmark dataset with diverse queries using our fully featured implementation. Both the dataset and the code are publicly available on GitHub. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Final,,
Wang D.; Fan H.; Liu J.,Towards Bootstrapping Biomedical Named Entity Recognition using Reinforcement Learning,,,-1,#excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100352914&doi=10.1109%2fBIBM49941.2020.9313571&partnerID=40&md5=110fb7c7180f1b8670bb70e9679f03f4,"Named entity recognition is one of the most fundamental problems in knowledge graph. In biomedical field, labeling high-quality biomedical entities requires plenty of linguistic knowledge due to abbreviation and specificity. Using dictionary is the simplest way for labeling, but it is difficult to obtain a versatile dictionary and usually a dictionary for one corpus is not suitable for another corpus due to bad transferability. Current mainstream recognition methods require lots of manpower, which is time-consuming and laborious. To handle this challenge, we present a novel approach to automatically recognize new biomedical entities. First, we use a small number of manually labeled biomedical entities as seeds to label some biomedical texts and learn their features autonomously. Then by using a tagger based on supervised learning and an instance selector based on reinforcement learning, we iteratively generate new biomedical entities. Experiment results demonstrate that our method can deal with biomedical named entity recognition and obtain significant performances in both English and Chinese biomedical datasets. © 2020 IEEE.",Final,,
Bennacer N.; Vioulès M.J.; López M.A.; Quercini G.,A multilingual approach to discover cross-language links in wikipedia,,,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949963735&doi=10.1007%2f978-3-319-26190-4_36&partnerID=40&md5=c82c2a83d3eec7745f518d9a128b6b1e,"Wikipedia is a well-known public and collaborative encyclopaedia consisting of millions of articles. Initially in English, the popular website has grown to include versions in over 288 languages. These versions and their articles are interconnected via cross-language links, which not only facilitate navigation and understanding of concepts in multiple languages, but have been used in natural language processing applications, developments in linked open data, and expansion of minor Wikipedia language versions. These applications are the motivation for an automatic, robust, and accurate technique to identify cross-language links. In this paper, we present a multilingual approach called EurekaCL to automatically identify missing cross-language links in Wikipedia. More precisely, given a Wikipedia article (the source) EurekaCL uses the multilingual and semantic features of BabelNet 2.0 in order to efficiently identify a set of candidate articles in a target language that are likely to cover the same topic as the source. The Wikipedia graph structure is then exploited both to prune and to rank the candidates. Our evaluation carried out on 42,000 pairs of articles in eight language versions of Wikipedia shows that our candidate selection and pruning procedures allow an effective selection of candidates which significantly helps the determination of the correct article in the target language version. © Springer International Publishing Switzerland 2015.",Final,All Open Access; Green Open Access,
Parvizi A.; Kohl M.; Gonzàlez M.; Saurí R.,Towards a linguistic ontology with an emphasis on reasoning and knowledge reuse,,,-1,#ouofscope #excluded,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029149766&partnerID=40&md5=12d7ec8159d4d10df6decaf424febf12,"The Dictionaries division at Oxford University Press (OUP) is aiming to model, integrate, and publish lexical content for 100 languages focussing on digitally under-represented languages. While there are multiple ontologies designed for linguistic resources, none had adequate features for meeting our requirements, chief of which was the capability to losslessly capture diverse features of many different languages in a dictionary format, while supplying a framework for inferring relations like translation, derivation, etc., between the data. Building on valuable features of existing models, and working with OUP monolingual and bilingual dictionary datasets, we have designed and implemented a new linguistic ontology. The ontology has been reviewed by a number of computational linguists, and we are working to move more dictionary data into it. We have also developed APIs to surface the linked data to dictionary websites.",Final,,
Gangemi A.; Alam M.; Asprino L.; Presutti V.; Recupero D.R.,Framester: A wide coverage linguistic linked data hub,,,1,#kg,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997124448&doi=10.1007%2f978-3-319-49004-5_16&partnerID=40&md5=cdc7212b9de89606b0d5a31cdceae74c,"Semantic web applications leveraging NLP can benefit from easy access to expressive lexical resources such as FrameNet. However, the usefulness of FrameNet is affected by its limited coverage and nonstandard semantics. The access to existing linguistic resources is also limited because of poor connectivity among them. We present some strategies based on Linguistic Linked Data to broaden FrameNet coverage and formal linkage of lexical and factual resources. We created a novel resource, Framester, which acts as a hub between FrameNet, Word- Net, VerbNet, BabelNet, DBpedia, Yago, DOLCE-Zero, as well as other resources. Framester is not only a strongly connected knowledge graph, but also applies a rigorous formal treatment for Fillmore’s frame semantics, enabling full-fledged OWL querying and reasoning on a large framebased knowledge graph. We also describe Word Frame Disambiguation, an application that reuses Framester data as a base in order to perform frame detection from text, with results comparable in precision to the state of the art, but with a much higher coverage. © Springer International Publishing AG 2016.",Final,,
Frontini F.; Del Gratta R.; Monachini M.,GeodomainWordNet: Linking the geonames ontology to WordNet,,,1,#kg,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981165734&doi=10.1007%2f978-3-319-43808-5_18&partnerID=40&md5=3f1d2bb1b27820ba3aeed8801419595d,"This paper illustrates the transformation of GeoNames’ ontology concepts, with their English labels and glosses, into a Geo-Domain WordNet-like resource in English, its translation into Italian, and its linking to the existing generic WordNets of both languages. The paper describes the criteria used for the linking of domain synsets to each other and to the generic ones and presents the published resource in RDF according to the w3c and lemon schema. © Springer International Publishing Switzerland 2016.",Final,,
Chiarcos C.; Fäth C.; Renner-Westermann H.; Abromeit F.; Dimitrova V.,"Lin|gu|is|tik: Building the linguist's pathway to bibliographies, libraries, language resources and linked open data",,,1,#kg,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021238984&partnerID=40&md5=4414354a6f7f836f71f360aaee4b97f6,"This paper introduces a novel research tool for the field of linguistics: The Lin\gu\is\tik web portal provides a virtual library which offers scientific information on every linguistic subject. It comprises selected internet sources and databases as well as catalogues for linguistic literature, and addresses an interdisciplinary audience. The virtual library is the most recent outcome of the Special Subject Collection Linguistics of the German Research Foundation (DFG), and also integrates the knowledge accumulated in the Bibliography of Linguistic Literature. In addition to the portal, we describe long-term goals and prospects with a special focus on ongoing efforts regarding an extension towards integrating language resources and Linguistic Linked Open Data.",Final,,
Corcoglioniti F.; Rospocher M.; Aprosio A.P.; Tonelli S.,PreMOn: A lemon extension for exposing predicate models as linked data,,,1,#kg #ontology,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996521101&partnerID=40&md5=35c56745a964db987ac33ebd1911f586,"We introduce PreMOn (predicate model for ontologies), a linguistic resource for exposing predicate models (PropBank, NomBank, VerbNet, and FrameNet) and mappings between them (e.g, SemLink) as Linked Open Data. It consists of two components: (i) the PreMOn Ontology, an extension of the lemon model by the W3C Ontology-Lexica Community Group, that enables to homogeneously represent data from the various predicate models; and, (ii) the PreMOn Dataset, a collection of RDF datasets integrating various versions of the aforementioned predicate models and mapping resources. PreMOn is freely available and accessible online in different ways, including through a dedicated SPARQL endpoint.",Final,,
Carvalho S.; Costa R.; Roche C.,LESS can indeed be more: Linguistic and conceptual challenges in the age of interoperability,,,-1,,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984808375&partnerID=40&md5=784a0061124f859594770fdb081bdc79,"The advent of the Semantic Web and, more recently, of the Linked Data initiative, has paved the way for new perspectives and opportunities in Terminology, namely regarding the operationalization of terminological products. Within the biomedical domain, changes have been substantial in the past decades and at their heart stand the current challenges regarding the production, use, storage and dissemination of medical data, information, and knowledge. In a context where biomedical terminological resources are becoming increasingly concept-oriented, terminology work should reflect a double dimension (both linguistic and conceptual) that may, in turn, support the aspired operationalization and interoperability in this field. Therefore, the purpose of this paper is to present a case study, based around the concept of <Laparoendoscopic single-site surgery>, in which a methodology anchored in Terminology's double dimension aims to contribute to the enrichment of the Systematized Nomenclature of Medicine-Clinical Terms (SNOMED CT).",Final,,
Burns G.A.; Hermjakob U.; Ambite J.L.,Abstract meaning representations as linked data,,,-1,#excluded,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992597297&doi=10.1007%2f978-3-319-46547-0_2&partnerID=40&md5=11aa13bd011ac43dcb155862f19d1a41,"The complex relationship between natural language and formal semantic representations can be investigated by the development of large, semantically-annotated corpora. The “Abstract Meaning Representation” (AMR) formulation describes the semantics of a whole sentence as a rooted, labeled graph, where nodes represent concepts/entities (such as PropBank frames and named entities) and edges represent relations between concepts (such as verb roles). AMRs have been used to annotate corpora of classic books, newstext and biomedical literature. Research on semantic parsers that generate AMRs from text is progressing rapidly. In this paper, we describe an AMR corpus as Linked Data (AMR-LD) and the techniques used to generate it (including an opensource implementation). We discuss the benefits of AMR-LD, including convenient analysis using SPARQL queries and ontology inferences enabled by embedding into the web of Linked Data, as well as the impact of semantic web representations directly derived from natural language. © Springer International Publishing AG 2016.",Final,All Open Access; Bronze Open Access,
Wang C.; He X.,Chinese hypernym-hyponym extraction from user generated categories,,,-1,#excluded,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040907214&partnerID=40&md5=4dffd7dea849ef48421b9663021909b9,"Hypernym-hyponym (""is-a"") relations are key components in taxonomies, object hierarchies and knowledge graphs. While there is abundant research on is-a relation extraction in English, it still remains a challenge to identify such relations from Chinese knowledge sources accurately due to the flexibility of language expression. In this paper, we introduce a weakly supervised framework to extract Chinese is-a relations from user generated categories. It employs piece-wise linear projection models trained on a Chinese taxonomy and an iterative learning algorithm to update models incrementally. A pattern-based relation selection method is proposed to prevent ""semantic drift"" in the learning process using bi-criteria optimization. Experimental results illustrate that the proposed approach outperforms state-of-the-art methods. © 1963-2018 ACL.",Final,,
Roy S.D.; Zeng W.,Cognitive canonicalization of natural language queries using semantic strata,,,-1,#excluded #noLLOD,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891861533&doi=10.1145%2f2539053&partnerID=40&md5=67d12047b8a4c17c9513322579b2c965,"Natural language search relies strongly on perceiving semantics in a query sentence. Semantics is captured by the relationship among the query words, represented as a network (graph). Such a network of words can be fed into larger ontologies, like DBpedia or Google Knowledge Graph, where they appear as subgraphs-fashioning the name subnetworks (subnets). Thus, subnet is a canonical form for interfacing a natural language query to a graph database and is an integral step for graph-based searching. In this article, we present a novel standalone NLP technique that leverages the cognitive psychology notion of semantic strata for semantic subnetwork extraction from natural language queries. The cognitive model describes some of the fundamental structures employed by the human cognition to construct semantic information in the brain, called semantic strata. We propose a computational model based on conditional random fields to capture the cognitive abstraction provided by semantic strata, facilitating cognitive canonicalization of the query. Our results, conducted on approximately 5000 queries, suggest that the cognitive canonicals based on semantic strata are capable of significantly improving parsing and role labeling performance beyond pure lexical approaches, such as parts-of-speech based techniques. We also find that cognitive canonicalized subnets are more semantically coherent compared to syntax trees when explored in graph ontologies like DBpedia and improve ranking of retrieved documents. © 2013 ACM 1550-4875/2013/12-ART17 15.00.",Final,,
Bartolini R.; Del Gratta R.; Frontini F.,Towards the establishment of a linguistic linked data network for Italian,,,1,#ontology authoring #kg,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037098297&partnerID=40&md5=89c8ca58eb33e3b3f409e2e0ec84c502,"This paper describes the conversion of ItalWordNet and of a domain WordNet into RDF and their linking to the (L)LOD cloud and to other existing resources. A brief presentation of the resources is given, and the conversion and resulting datasets are described. © LDL 2013.All right reserved.",Final,,
Elbedweihy K.; Wrigley S.N.; Ciravegna F.; Zhang Z.,Using BabelNet in bridging the gap between natural language queries and linked data concepts,,,1,#WSD #use,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924389956&partnerID=40&md5=8100a50afb90f17b4fc3b428021f8274,"Many semantic search tool evaluations have reported a user preference for free natural language as a query input approach as opposed to controlled or view-based inputs. Although the exibility offered by this approach is a significant advantage, it can also be a major difficulty. Allowing users complete freedom in the choice of terms increases the difficulty for the search tools to match terms with the underlying data. This causes either a mismatch which affects precision, or a missing match which affects recall. In this paper, we present an empirical investigation on the use of named entity recognition, word sense disambiguation, and ontology-based heuristics in an approach attempting to bridge this gap between user terms and ontology concepts, properties and entities. We use the dataset provided by the Question Answering over Linked Data (QALD-2) workshop in our analysis and tests.",Final,,
Murnane E.L.; Haslhofer B.; Lagoze C.,RESLVE: Leveraging user interest to improve entity disambiguation on short text,,,-1,#excluded ,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893095042&partnerID=40&md5=76145714843a3d376c18dcd090808dd1,"We address the Named Entity Disambiguation (NED) prob- lem for short, user-generated texts on the social Web. In such settings, the lack of linguistic features and sparse lexical context result in a high degree of ambiguity and sharp performance drops of nearly 50% in the accuracy of conven- tional NED systems. We handle these challenges by developing a general model of user-interest with respect to a personal knowledge context and instantiate it using Wikipedia. We conduct systematic evaluations using individuals' posts from Twitter, YouTube, and Flickr and demonstrate that our novel technique is able to achieve performance gains be- yond state-of-the-art NED methods.",Final,,
Narducci F.; Palmonari M.; Semeraro G.,Cross-language semantic matching for discovering links to e-gov services in the LOD cloud,,,-1,#excluded ,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922538421&partnerID=40&md5=623c690799697625acabade880c692e4,"The large diffusion of e-gov initiatives is increasing the attention of public administrations towards the Open Data initiative. The adoption of open data in the e-gov domain produces different advantages in terms of more transparent government, development of better public services, economic growth and social value. However, the process of data opening should adopt standards and open formats. Only in this way it is possible to share experiences with other service providers, to exploit best practices from other cities or countries, and to be easily connected to the Linked Open Data (LOD) cloud. In this paper we present CroSeR (Cross-language Service Retriever), a tool able to match and retrieve cross-language e-gov services stored in the LOD cloud. The main goal of this work is to help public administrations to connect their e-gov services to services, provided by other administrations, already connected to the LOD cloud. We adopted a Wikipedia-based semantic representation in order to overcome the problems related to match really short textual descriptions associated to the services. A preliminary evaluation on an open catalog of e-gov services showed that the adopted techniques are promising and are more effective than techniques based only on keyword representation.",Final,,
Hayashi Y.,Migrating Psycholinguistic Semantic Feature Norms into Linked Data in Linguistics,,,-1,#excluded ,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122856650&partnerID=40&md5=f533c148768baf5b9424cec68a0c8750,"Semantic feature norms, originally utilized in the field of psycholinguistics as a tool for studying human semantic representation and computation, have recently attracted the attention of some NLP/IR researchers who wish to improve their task performances. However, currently available semantic feature norms are, by nature, not well-structured, making them difficult to integrate into existing resources of various kinds. In this paper, by examining an actual set of semantic feature norms, we investigate which types of semantic features should be migrated into Linked Data in Linguistics (LDL) and how the migration could be done. © LDL 2013.All right reserved.",Final,,
Sànchez-Rada J.F.; Iglesias C.A.,Onyx: Describing emotions on the web of data,,,1,#use #sentiment anlysis,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921891280&partnerID=40&md5=27c09a1447f4dd117e1549445a220b66,"Textual emotion analysis is a new field whose aim is to detect emotions in user generated content. It complements Sentiment Analysis in the characterization of users subjective opinions and feelings. Nevertheless, there is a lack of available lexical and semantic emotion resources that could foster the development of emotion analysis services. Some of the barriers for developing such resources are the diversity of emotion theories and the absence of a vocabulary to express emotion characteristics. This article presents a semantic vocabulary, called Onyx, intended to provide support to represent emotion characteristics in lexical resources and emotion analysis services. Onyx follows the Linked Data principles as it is aligned with the Provenance Ontology. It also takes a linguistic Linked Data approach: it is aligned with the Provenance Ontology, it represents lexical resources as linked data, and has been integrated with Lemon, an increasingly popular RDF model for representing lexical entries. Furthermore, it does not prescribe any emotion model and can be linked to heterogeneous emotion models expressed as Linked Data. Onyx representations can also be published using W3C EmotionML markup, based on the proposed mapping.",Final,,
Uszkoreit H.; Xu F.,From strings to things SAR-graphs: A new type of resource for connecting knowledge and language,,,-1,#excluded #updatedby,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924363298&partnerID=40&md5=4679a904fad3033c17eed0a18dc12bd2,"Recent research and development have created the necessary ingredients for a major push in web-scale language understanding: large repositories of structured knowledge (DBpedia, the Google knowledge graph, Freebase, YAGO) progress in language processing (parsing, information extraction, computational semantics), linguistic knowledge resources (Treebanks, WordNet, BabelNet, UWN) and new powerful techniques for machine learning. A major goal is the automatic aggregation of knowledge from textual data. A central component of this endeavor is relation extraction (RE). In this paper, we will outline a new approach to connecting repositories of world knowledge with linguistic knowledge (syntactic and lexical semantics) via web-scale relation extraction technologies.",Final,,
Gangemi A.; Draicchio F.; Presutti V.; Nuzzolese A.G.; Reforgiato D.,A machine rèader for the semantic web,,,-1,#excluded #does not materialize,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924721722&partnerID=40&md5=a03357216569cc4a23ba700a7ca6533c,"FRED is a machine reading tool for converting text into internally well-connected and quality linked-data-ready ontologies in web- service-acceptable time. It implements a novel approach for ontology design from natural language sentences, combining Discourse Representation Theory (DRT), linguistic frame semantics, and Ontology Design Patterns (ODP). The current version of the tool includes Earmark-based markup, and enrichment with word sense disambiguation (WSD) and named entity resolution (NER) off-the-shelf components.",Final,,
Aggarwal N.; Polajnar T.; Buitelaar P.,Cross-lingual natural language querying over the web of data,,,1,#use,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884972019&doi=10.1007%2f978-3-642-38824-8_13&partnerID=40&md5=d9a6a71b02be64505ee73179c1c85e0a,"The rapid growth of the Semantic Web offers a wealth of semantic knowledge in the form of Linked Data and ontologies, which can be considered as large knowledge graphs of marked up Web data. However, much of this knowledge is only available in English, affecting effective information access in the multilingual Web. A particular challenge arises from the vocabulary gap resulting from the difference in the query and the data languages. In this paper, we present an approach to perform cross-lingual natural language queries on Linked Data. Our method includes three components: entity identification, linguistic analysis, and semantic relatedness. We use Cross-Lingual Explicit Semantic Analysis to overcome the language gap between the queries and data. The experimental results are evaluated against 50 German natural language queries. We show that an approach using a cross-lingual similarity and relatedness measure outperforms other systems that use automatic translation. We also discuss the queries that can be handled by our approach. © 2013 Springer-Verlag Berlin Heidelberg.",Final,,
Yamamoto Y.; Kawamoto S.,Building linked open data of the Life Science Dictionary,,,1,#kg #dataset,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922691188&partnerID=40&md5=0a44644e56cf3d3115649b07cc5ac342,"There is a growing need for efficient and integrated access to databases provided by diverse institutions. Using a linked data design pattern allows the diverse data on the Internet to be linked effectively and accessed efficiently by computers. In addition, providing a dictionary to translate words into another language in Resource Description Framework (RDF) is useful to cross a language barrier such as English and Japanese when we want to access datasets in multiple languages. Here, we built a Linked Open Dataset of the Life Science Dictionary (LSD) with links to DBpedia. LSD consists of various lexical resources including English-Japanese / Japanese-English dictionaries and a thesaurus using the MeSH vocabulary. The latest version of LSD contains 110 thousand English and 120 thousand Japanese terms. Since we believe that LSD is a useful language resource in the life science domain to process Japanese and English text data seamlessly, linking LSD to DBpedia enables us to find related knowledge more easily and therefore contributes to the life science research community.",Final,,
Jain P.; Hitzler P.; Verma K.; Yeh P.Z.; Sheth A.,Moving beyond sameAs with PLATO: Partonomy detection for linked data,,,-1,#excluded #outofscope,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864056254&doi=10.1145%2f2309996.2310004&partnerID=40&md5=173ee008d0f077450e82adff5308b871,"The Linked Open Data (LOD) Cloud has gained significant traction over the past few years. With over 275 interlinked datasets across diverse domains such as life science, geography, politics, and more, the LOD Cloud has the potential to support a variety of applications ranging from open domain question answering to drug discovery. Despite its significant size (approx. 30 billion triples), the data is relatively sparely interlinked (approx. 400 million links). A semantically richer LOD Cloud is needed to fully realize its potential. Data in the LOD Cloud are currently interlinked mainly via the owl: sameAs property, which is inadequate for many applications. Additional properties capturing relations based on causality or partonomy are needed to enable the answering of complex questions and to support applications. In this paper, we present a solution to enrich the LOD Cloud by automatically detecting partonomic relationships, which are well-established, fundamental properties grounded in linguistics and philosophy. We empirically evaluate our solution across several domains, and show that our approach performs well on detecting partonomic properties between LOD Cloud data. Copyright 2012 ACM.",Final,,
Di Buccio E.; Di Nunzio G.M.; Silvello G.,A system for exposing linguistic linked open data,,,1,#kg,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867676826&doi=10.1007%2f978-3-642-33290-6_19&partnerID=40&md5=543919d93ecbc214e49abd47f8d9ce8e,"In this paper we introduce the Atlante Sintattico d'Italia, Syntactic Atlas of Italy (ASIt) enterprise which is a linguistic project aiming to account for minimally different variants within a sample of closely related languages. One of the main goals of ASIt is to share and make linguistic data re-usable. In order to create a universally available resource and be compliant with other relevant linguistic projects, we define a Resource Description Framework (RDF) model for the ASIt linguistic data thus providing an instrument to expose these data as Linked Open Data (LOD). By exploiting RDF native capabilities we overcome the ASIt methodological and technical peculiarities and enable different linguistic projects to read, manipulate and re-use linguistic data. © 2012 Springer-Verlag.",Final,,
Augenstein I.; Padó S.; Rudolph S.,LODifier: Generating linked data from unstructured text,,,1,#use ,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861720649&doi=10.1007%2f978-3-642-30284-8_21&partnerID=40&md5=97a20dcedfeacd87da0a9ac6928831df,"The automated extraction of information from text and its transformation into a formal description is an important goal in both Semantic Web research and computational linguistics. The extracted information can be used for a variety of tasks such as ontology generation, question answering and information retrieval. LODifier is an approach that combines deep semantic analysis with named entity recognition, word sense disambiguation and controlled Semantic Web vocabularies in order to extract named entities and relations between them from text and to convert them into an RDF representation which is linked to DBpedia and WordNet. We present the architecture of our tool and discuss design decisions made. An evaluation of the tool on a story link detection task gives clear evidence of its practical potential. © 2012 Springer-Verlag.",Final,All Open Access; Bronze Open Access,
Zhou W.; Wang H.; Chao J.; Zhang W.; Yu Y.,LODDO: Using linked open data description overlap to measure semantic relatedness between named entities,,,-1,#excluded #noLLOD,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862180240&doi=10.1007%2f978-3-642-29923-0_18&partnerID=40&md5=15e8ef5c2bc5557755411b680757b500,"Measuring semantic relatedness plays an important role in information retrieval and Natural Language Processing. However, little attention has been paid to measuring semantic relatedness between named entities, which is also very significant. As the existing knowledge based approaches have the entity coverage issue and the statistical based approaches have unreliable result to low frequent entities, we propose a more comprehensive approach by leveraging Linked Open Data (LOD) to solve these problems. LOD consists of lots of data sources from different domains and provides rich a priori knowledge about the entities in the world. By exploiting the semantic associations in LOD, we propose a novel algorithm, called LODDO, to measure the semantic relatedness between named entities. The experimental results show the high performance and robustness of our approach. © 2012 Springer-Verlag.",Final,,
Tiddi I.; Mustapha N.B.; Vanrompay Y.; Aufaure M.-A.,Ontology learning from open linked data and web snippets,,,-1,#excluded,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873841493&doi=10.1007%2f978-3-642-33618-8_59&partnerID=40&md5=0ef395ad10fdadc4ed878c932a45af46,"The Web of Open Linked Data (OLD) is a recommended best practice for exposing, sharing, and connecting pieces of data, information, and knowledge on the Semantic Web using URIs and RDF. Such data can be used as a training source for ontology learning from web textual contents in order to bridge the gap between structured data and the Web. In this paper, we propose a new method of ontology learning that consists in learning linguistic patterns related to OLD entities attributes from web snippets. Our insight is to use the Linked Data as a skeleton for ontology construction and for pattern learning from texts. The contribution resides on learning patterns for relations existing in the Web of Linked Data from Web content. These patterns are used to populate the ontology core schema with new entities and attributes values. The experiments of the proposal have shown promising results in precision. © 2012 Springer-Verlag.",Final,All Open Access; Green Open Access,
Pinheiro V.; Furtado V.; Pequeno T.; Ferreira C.,Towards a common sense base in Portuguese for the linked open data cloud,,,1,#use #kg ,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858427776&doi=10.1007%2f978-3-642-28885-2_15&partnerID=40&md5=44c852eeccc94e8bf61c459775b10a50,"The Linked Open Data (LOD) cloud is a promising reality since the major content producers are offering their data on an open and linked network, through RDF (Resource Description Framework), with the aim of providing Semantic Web applications with a single global database for retrieval of related content and to perform inferences over the network. However, bases with Portuguese-language content are still incipient. In this paper we present the process of inclusion of the InferenceNet - the first resource with common sense and inferentialist knowledge in Portuguese language - on the LOD. Our main goal is to leverage the use and development of Semantic Web applications by content producers in Portuguese language. We develop and evaluated a platform, called SemWidgets, for the creation and execution of widgets able to access and reason over InferenceNet and the open linked data, like DBPedia, Yago, and Article Search API of the New York Times. © 2012 Springer-Verlag.",Final,,
Zouaq A.; Gasevic D.; Hatala M.,Linguistic patterns for information extraction in OntoCmaps,,,-1,#excluded,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891808337&partnerID=40&md5=7034faeb0d316549708e5ab24da61899,"Linguistic patterns have proven their importance for the knowledge engineering field especially with the ever-increasing amount of available data. This is especially true for the Semantic Web, which relies on a formalization of knowledge into triples and linked data. This paper presents a number of syntactic patterns, based on dependency grammars, which output triples useful for the ontology learning task. Our experimental results show that these patterns are a good starting base for text mining initiatives in general and ontology learning in particular.",Final,,
Kuwabara K.; Kinomura S.,Mediating accesses to multiple information sources in a multi-lingual application,,,-1,#excluded,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870872899&doi=10.1007%2f978-3-642-34630-9_34&partnerID=40&md5=4cc9e955a8d6ecce59cd0ccc5c887cdf,"This paper describes an approach to mediating accesses to multiple information sources in a multi-lingual application. There are many information sources available on the Internet in different languages, and machine translation services are also available to allow multi-lingual access to information sources. Domain-dependent translation dictionaries are often used to make translation more appropriate. In the proposed approach, the domain-dependent translation dictionaries are represented as linked data. Using the data available from the translation dictionaries, accesses to the information sources that are represented as linked data can be customized. By applying the linked data concept, a multi-lingual application can be constructed in a flexible way. © 2012 Springer-Verlag.",Final,,
Aggarwal N.; Buitelaar P.,A system description of natural language query over DBpedia,,,1,#use #accessibility,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893280764&partnerID=40&md5=a5abfafedd1db93d78ad06f4e1057074,"This paper describes our system, which is developed as a first step towards implementing a methodology for natural language querying over semantic structured information (semantic web). This work focuses on interpretation of natural language queries (NL-Query) to facilitate querying over Linked Data. This interpretation includes query annotation with Linked Data concepts (classes and instances), a deep linguistic analysis and semantic similarity/relatedness to generate potential SPARQL queries for a given NL-Query. We evaluate our approach on QALD-2 test dataset and achieve a F1 score of 0.46, an average precision of 0.44 and an average recall of 0.48.",Final,,
Freitas A.; Oliveira J.G.; O'Riain S.; Curry E.; Pereira Da Silva J.C.,Treo: Best-effort natural language queries over linked data,,,-1,#excluded,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959657728&doi=10.1007%2f978-3-642-22327-3_40&partnerID=40&md5=bc92930f4071aaf6d20cda41523149eb,"Linked Data promises an unprecedented availability of data on the Web. However, this vision comes together with the associated challenges of querying highly heterogeneous and distributed data. In order to query Linked Data on the Web today, end-users need to be aware of which datasets potentially contain the data and the data model behind these datasets. This query paradigm, deeply attached to the traditional perspective of structured queries over databases, does not suit the heterogeneity and scale of the Web, where it is impractical for data consumers to have an a priori understanding of the structure and location of available datasets. This work describes Treo, a best-effort natural language query mechanism for Linked Data, which focuses on the problem of bridging the semantic gap between end-user natural language queries and Linked Datasets. © 2011 Springer-Verlag.",Final,All Open Access; Green Open Access,
Montiel-Ponsoda E.; Gracia J.; Aguado-De-Cea G.; Gómez-Pérez A.,Representing translations on the semantic Web,,,-1,#kg #ontology,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891078136&partnerID=40&md5=d5572583822d5e3291b8f8a2c8816273,"The increase of ontologies and data sets published in the Web in languages other than English raises some issues related to the representation of linguistic (multilingual) information in ontologies. Such linguistic descriptions can contribute to the establishment of links between ontologies and data sets described in multiple natural languages in the Linked Open Data cloud. For these reasons, several models have been proposed recently to enable richer linguistic descriptions in ontologies. Among them, we find lemon, an RDF ontology-lexicon model that defines specific modules for different types of linguistic descriptions. In this contribution we propose a new module to represent translation relations between lexicons in different natural languages associated to the same ontology or belonging to different ontologies. This module can enable the representation of different types of translation relations, as well as translation metadata such as provenance or the reliability score of translations.",Final,,
Hayashi Y.,Direct and indirect linking of lexical objects for evolving lexical linked data,,,1,#use #task #synset,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891124416&partnerID=40&md5=4dfb614a158c5969b4e336e79321ede8,"Servicization of language resources in a Web-based environment has opened up the potential for dynamically combined virtual lexical resources. Evolving lexical linked data could be realized, provided being recovered/discovered links among lexical resources are properly organized and maintained. This position paper examines a scenario, in which lexical semantic resources are cross-linguistically enriched, and sketches how this scenario could come about while discussing necessary ingredients. The discussions naturally include how the existing lexicon modeling framework could be applied and should be extended.",Final,,
Araúz P.L.; Redondo P.J.M.,Ecolexicon: Contextualizing an environmental ontology,,,1,#ontology #kg,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887508763&partnerID=40&md5=81f959af16c8c4a9740dec860d3351e8,"EcoLexicon is a Terminological Knowledge Base (TKB) on environment enhanced by both linguistic and knowledge representation techniques. Our TKB is primarily hosted in a relational database (RDB) but at the same time integrated in an ontological model. This is a powerful representational model, as it adds the semantic expressiveness lacking in RDBs. In turn, the design of ontologies can also benefit from the theoretical background of linguistics, especially from cognitive approaches. Consequently, the upper-level classes in our ontology correspond to basic semantic roles (AGENT, PATIENT, RESULT, etc.) as described in Frame Semantics (Fillmore, 1992). On the other hand, ontologies provide a suitable schema for sharing and reusing semantic resources. In this sense, Linked Data is a good initiative to cope with the current information overload in the web and make the most of other similar approaches. This could also enrich our system with new information, complementing our TKB from a different perspective or even with other contents, such as real-world geographical instances. Nevertheless, information overload not only occurs when interconnecting different systems. Before considering the interoperability of other environmental knowledge-based projects, we must first deal with overinformation in our own TKB, mostly due to multidimensionality and contextual variation.",Final,,
Wang Z.; Li L.; Zeng D.,Knowledge-Enhanced Natural Language Inference Based on Knowledge Graphs,,,1,#use #naturalLanguageInference,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118819345&partnerID=40&md5=3cb022a173acf84d242a962c1bc4b235,"Natural Language Inference (NLI) is a vital task in natural language processing. It aims to identify the logical relationship between two sentences. Most of the existing approaches make such inference based on semantic knowledge obtained through training corpus. The adoption of background knowledge is rarely seen or limited to a few specific types. In this paper, we propose a novel Knowledge Graph-enhanced NLI (KGNLI) model to leverage the usage of background knowledge stored in knowledge graphs in the field of NLI. KGNLI model consists of three components: a semantic-relation representation module, a knowledge-relation representation module, and a label prediction module. Different from previous methods, various kinds of background knowledge can be flexibly combined in the proposed KGNLI model. Experiments on four benchmarks, SNLI, MultiNLI, SciTail, and BNLI, validate the effectiveness of our model. © 2020 COLING 2020 - 28th International Conference on Computational Linguistics, Proceedings of the Conference. All rights reserved.",Final,,
Casadei S.,Semiotic Knowledge Models for Personal Knowledge Repositories,,,1,#use,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179546774&doi=10.5220%2f0012209100003598&partnerID=40&md5=233ec7719a4b45185dcb5753aed35c4b,"Knowledge graphs have been used successfully to represent and acquire general knowledge and have also been proposed for personal knowledge representations. While general knowledge data can be modelled statistically as being a noisy projection of universal (and crisp) entities, categories, and relationships, personal knowledge data requires a more refined model: each user’s peculiarities and fluctuations in associating words with meanings and meanings with words should be tracked and analysed instead of being treated as noise and averaged out. This position paper describes a semiotic knowledge model whose primitives are the signification events which occur when symbols such as words and linguistic expressions are associated with an instantaneous meaning. Semiotic structures constructed from these primitives with users’ active participation, enable them to create, update, modify, organize, re-organize and curate detailed and comprehensive representations of their own personal knowledge by means of their own personal terminologies, taxonomies, and organizational schemes. Copyright © 2023 by SCITEPRESS – Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0).",Final,All Open Access; Hybrid Gold Open Access,
Montiel-Ponsoda E.; Bosque-Gil J.; Gracia J.; Aguado-De-Cea G.; Vila-Suero D.,Towards the integration of multilingual terminologies: An example of a linked data prototype,,,1,#kg,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955270336&partnerID=40&md5=0c93f08a0480b4fa64875dc53df9a78c,"Many language resources are nowadays available in machine readable formats, but still contained in isolated silos. Current Semantic Web-based techniques enable the transformation and linking of those resources to become a navigable graph of linked language resources, which can be directly consumed by third-party applications. The prototype we have developed builds on a web user interface and SPARQL endpoint initially developed to query a single terminological database (Terminesp), now extended to navigate a set of multilingual terminologies. The vocabulary used to represent these terminologies into the linked data format is lemon-ontolex, a de facto standard for representing lexical information relative to ontologies and for linking lexicons and machine-readable dictionaries to the Semantic Web.",Final,,
Shekarpour S.; Hoffner K.; Lehmann J.; Auer S.,Keyword query expansion on linked data using linguistic and semantic features,,,-1,#excluded #noLLOD,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893945883&doi=10.1109%2fICSC.2013.41&partnerID=40&md5=94c1ad1d9cc5ca064669c3adc183737e,"Effective search in structured information based on textual user input is of high importance in thousands of applications. Query expansion methods augment the original query of a user with alternative query elements with similar meaning to increase the chance of retrieving appropriate resources. In this work, we introduce a number of new query expansion features based on semantic and linguistic inferencing over Linked Open Data. We evaluate the effectiveness of each feature individually as well as their combinations employing several machine learning approaches. The evaluation is carried out on a training dataset extracted from the QALD question answering benchmark. Furthermore, we propose an optimized linear combination of linguistic and lightweight semantic features in order to predict the usefulness of each expansion candidate. Our experimental study shows a considerable improvement in precision and recall over baseline approaches. © 2013 IEEE.",Final,All Open Access; Green Open Access,
Aranovich R.,Modeling Grammars with Knowledge Representation Methods: Subcategorization as a Test Case,,,-1,#excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175976004&doi=10.1007%2f978-3-031-43458-7_21&partnerID=40&md5=1e0d292b9e033ed32009481d911107dc,"An OWL ontology is used to model a grammar that accounts for subcategorization, showing that ontologies are able to generate (mildly) context-sensitive languages. Semantic Web knowledge representation methods offer a useful way to model the implicit knowledge that defines human linguistic abilities. When a grammar is modeled as a set of ontological constraints (i.e. classes with restrictions on their properties), ungrammatical sentences are defined as facts that lead to inconsistencies which can be discovered by a reasoner. Property chains are used to “pass on” the category of a syntactic complement as the value of a head’s subcategorization feature, modeling the concept of structure sharing that is central to constraint-based theories of syntax like HPSG. By treating utterances as instances and syntactic constraints as axioms, this approach offers points of contact with efforts to model grammars as Linguistic Linked Open Data in the Semantic Web. © The Author(s), under exclusive license to Springer Nature Switzerland AG. 2023.",Final,,
Gatiatullin A.; Kubedinova L.; Prokopyev N.; Suleymanov D.,Linguistic Knowledge Graphs of the 'Turkic Morpheme' Portal,,,-1,#not available #excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177550341&doi=10.1109%2fUBMK59864.2023.10286723&partnerID=40&md5=5e7cf92cafa6ddd41f0a97562b21eb76,"This paper presents a description of linguistic knowledge graphs, which form the basis of knowledge base of the portal 'Turkic morpheme'. Peculiarity of these knowledge graphs is that ontological models that form the basis are developed using pragmatically oriented approach to linguistic models' development and are focused on the most complete description of structural and functional features of the Turkic languages. These knowledge graphs are a useful resource, both for creating information and reference systems for Turkic languages, and application programs for computer processing. © 2023 IEEE.",Final,,
Declerck T.; Bajčetić L.; Sérasset G.,Adding Information to Multiword Terms in Wiktionary,,,-1,#excluded,2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171336522&partnerID=40&md5=65b77c44f06749bd27262b5f9bd5eec9,"We describe ongoing work dealing with the potential “auto-enrichment” of “Multiword terms” (MWTs) that are included in the English edition of Wiktionary. The idea is to use and combine information contained in the lexical components of the MWTs and to propagate this extracted and filtered information into the lexical description of the MWTs, as those are typically equipped with less lexical information as it is the case for their lexical components. We started our work with the generation of pronunciation information for such MWTs, on the base of the pronunciation information available for their components. We present in this paper first achievements but also issues we encountered. Addressing those issues lead us to consider additional resources for supporting our approach, like DBnary and WikiPron. This step was ultimately leading to suggestions of adaptations for those additional resources, which, in the case of DBnary, are already implemented. We are currently extending our approach to a morphosyntactic and semantic enrichment of the English MWTs in Wiktionary. © 2023 Lexical Computing CZ s.r.o.. All rights reserved.",Final,,
Declerck T.,Towards a new Ontology for Sign Languages,,,-1,#excluded,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140453253&partnerID=40&md5=66408d93e4a95af9feb947fc4ac977c4,"We present the current status of a new ontology for representing constitutive elements of Sign Languages (SL). This development emerged from investigations on how to represent multimodal lexical data in the OntoLex-Lemon framework, with the goal to publish such data in the Linguistic Linked Open Data (LLOD) cloud. While studying the literature and various sites dealing with sign languages, we saw the need to harmonise all the data categories (or features) defined and used in those sources, and to organise them in an ontology to which lexical descriptions in OntoLex-Lemon could be linked. We make the code of the first version of this ontology available, so that it can be further developed collaboratively by both the Linked Data and the SL communities. © European Language Resources Association (ELRA), licensed under CC-BY-NC-4.0.",Final,,
Fang T.; Zhang H.; Wang W.; Song Y.; He B.,DISCOS: Bridging the gap between discourse knowledge and commonsense knowledge,,,-1,#included,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106111969&doi=10.1145%2f3442381.3450117&partnerID=40&md5=7cd003556dcc8efcff823d16b9c256dd,"Commonsense knowledge is crucial for artificial intelligence systems to understand natural language. Previous commonsense knowledge acquisition approaches typically rely on human annotations (for example, ATOMIC) or text generation models (for example, COMET.) Human annotation could provide high-quality commonsense knowledge, yet its high cost often results in relatively small scale and low coverage. On the other hand, generation models have the potential to automatically generate more knowledge. Nonetheless, machine learning models often fit the training data well and thus struggle to generate high-quality novel knowledge. To address the limitations of previous approaches, in this paper, we propose an alternative commonsense knowledge acquisition framework DISCOS (from DIScourse to COmmonSense), which automatically populates expensive complex commonsense knowledge to more affordable linguistic knowledge resources. Experiments demonstrate that we can successfully convert discourse knowledge about eventualities from ASER, a large-scale discourse knowledge graph, into if-then commonsense knowledge defined in ATOMIC without any additional annotation effort. Further study suggests that DISCOS significantly outperforms previous supervised approaches in terms of novelty and diversity with comparable quality. In total, we can acquire 3.4M ATOMIC-like inferential commonsense knowledge by populating ATOMIC on the core part of ASER. Codes and data are available at https://github.com/HKUST-KnowComp/DISCOS-commonsense.  Â© 2021 ACM.",Final,All Open Access; Green Open Access,
Kancheva Z.; Radev I.,Linguistic vs. Encyclopedic knowledge. classification of mwes on thbase of domain information,,,-1,#excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113186014&partnerID=40&md5=4e2910b0455b4635b6086dd44465ddb1,"This paper reports on the first steps in the creation of linked data through the mapping of BTB-WordNet and the Bulgarian Wikipedia. The task of expanding the BTB-WordNet with encyclopedic knowledge is done by mapping its synsets to Wikipedia pages with many MWEs found in the articles and subjected to further analysis. We look for a way to filter the Wikipedia MWEs in the effort of selecting the ones most beneficial to the enrichment of BTB-WN. © 2020, Institute for Bulgarian Language. All rights reserved.",Final,,
Lecailliez L.,Preliminary thoughts on issues of modeling Japanese dictionaries using the OntoLex model,,,-1,#excluded,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062198224&partnerID=40&md5=347401fe1a9978811e4ec8034ce61b56,"Recent works aiming at making Linked Data dictionaries make use of the Lemon or OntoLex models. Application to existing dictionaries revealed the need for extensions to the model to properly deal with lexicographic data without loss of information. These works however focus on languages found in Europe, and thus let the issue of Est-Asian lexicography for future exploration. This paper provides a small typology of existing dictionaries in Japan and exposes issues in existing related works that could form the ground of new modules for OntoLex. © 2017 Tribun EU s. r. o. All rights reserved.",Final,,
Presutti V.; Draicchio F.; Gangemi A.,Knowledge extraction based on discourse representation theory and linguistic frames,,,-1,#excluded,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867674858&doi=10.1007%2f978-3-642-33876-2_12&partnerID=40&md5=049f5a5dbcd8257df05de6ea22b268f2,"We have implemented a novel approach for robust ontology design from natural language texts by combining Discourse Representation Theory (DRT), linguistic frame semantics, and ontology design patterns. We show that DRT-based frame detection is feasible by conducting a comparative evaluation of our approach and existing tools. Furthermore, we define a mapping between DRT and RDF/OWL for the production of quality linked data and ontologies, and present FRED, an online tool for converting text into internally well-connected and linked-data-ready ontologies in web-service-acceptable time. © 2012 Springer-Verlag.",Final,,
Freitas A.; Oliveira J.G.; O'Riain S.; Curry E.; Pereira Da Silva J.C.,Querying linked data using semantic relatedness: A vocabulary independent approach,,,-1,#excluded,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959637229&doi=10.1007%2f978-3-642-22327-3_5&partnerID=40&md5=9247d80361293060b08ea25cca680a0e,"Linked Data brings the promise of incorporating a new dimension to the Web where the availability of Web-scale data can determine a paradigmatic transformation of the Web and its applications. However, together with its opportunities, Linked Data brings inherent challenges in the way users and applications consume the available data. Users consuming Linked Data on the Web, or on corporate intranets, should be able to search and query data spread over potentially a large number of heterogeneous, complex and distributed datasets. Ideally, a query mechanism for Linked Data should abstract users from the representation of data. This work focuses on the investigation of a vocabulary independent natural language query mechanism for Linked Data, using an approach based on the combination of entity search, a Wikipedia-based semantic relatedness measure and spreading activation. The combination of these three elements in a query mechanism for Linked Data is a new contribution in the space. Wikipedia-based relatedness measures address existing limitations of existing works which are based on similarity measures/term expansion based on WordNet. Experimental results using the query mechanism to answer 50 natural language queries over DBPedia achieved a mean reciprocal rank of 61.4%, an average precision of 48.7% and average recall of 57.2%, answering 70% of the queries. © 2011 Springer-Verlag.",Final,All Open Access; Green Open Access,
Ni Y.; Zhang L.; Qiu Z.; Wang C.,Enhancing the open-domain classification of named entity using linked open data,,,-1,#excluded,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650922445&doi=10.1007%2f978-3-642-17746-0_36&partnerID=40&md5=cc6e831b28ac06d2dec2a31ff8da76c6,"Many applications make use of named entity classification. Machine learning is the preferred technique adopted for many named entity classification methods where the choice of features is critical to final performance. Existing approaches explore only the features derived from the characteristic of the named entity itself or its linguistic context. With the development of the Semantic Web, a large number of data sources are published and connected across the Web as Linked Open Data (LOD). LOD provides rich a priori knowledge about entity type information, knowledge that can be a valuable asset when used in connection with named entity classification. In this paper, we explore the use of LOD to enhance named entity classification. Our method extracts information from LOD and builds a type knowledge base which is used to score a (named entity string, type) pair. This score is then injected as one or more features into the existing classifier in order to improve its performance. We conducted a thorough experimental study and report the results, which confirm the effectiveness of our proposed method. © 2010 Springer-Verlag.",Final,All Open Access; Bronze Open Access,
Gillis-Webber F.; Tittel S.,A framework for shared agreement of language tags beyond ISO 639,,-1,-1,,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096512485&partnerID=40&md5=e44ab9b4c0659408b02cf0f00d949ed5,"The identification and annotation of languages in an unambiguous and standardized way is essential for the description of linguistic data. It is the prerequisite for machine-based interpretation, aggregation, and re-use of the data with respect to different languages. This makes it a key aspect especially for Linked Data and the multilingual Semantic Web. The standard for language tags is defined by IETF's BCP 47 and ISO 639 provides the language codes that are the tags' main constituents. However, for the identification of lesser-known languages, endangered languages, regional varieties or historical stages of a language, the ISO 639 codes are insufficient. Also, the optional language sub-tags compliant with BCP 47 do not offer a possibility fine-grained enough to represent linguistic variation. We propose a versatile pattern that extends the BCP 47 sub-tag privateuse and is, thus, able to overcome the limits of BCP 47 and ISO 639. Sufficient coverage of the pattern is demonstrated with the use case of linguistic Linked Data of the endangered Gascon language. We show how to use a URI shortcode for the extended sub-tag, making the length compliant with BCP 47. We achieve this with a web application and API developed to encode and decode the language tag. © European Language Resources Association (ELRA), licensed under CC-BY-NC",Final,,
Rettinger A.; Schumilin A.; Thoma S.; Ell B.,Learning a cross-lingual semantic representation of relations expressed in text,,1,-1,#excluded,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937406143&doi=10.1007%2f978-3-319-18818-8_21&partnerID=40&md5=19640ca2dd0bc2e31d2fae09b2b784c1,"Learning cross-lingual semantic representations of relations from textual data is useful for tasks like cross-lingual information retrieval and question answering. So far, research has been mainly focused on cross-lingual entity linking, which is confined to linking between phrases in a text document and their corresponding entities in a knowledge base but cannot link to relations. In this paper, we present an approach for inducing clusters of semantically related relations expressed in text, where relation clusters (i) can be extracted from text of different languages, (ii) are embedded in a semantic representation of the context, and (iii) can be linked across languages to properties in a knowledge base. This is achieved by combining multi-lingual semantic role labeling (SRL) with cross-lingual entity linking followed by spectral clustering of the annotated SRL graphs. With our initial implementation we learned a cross-lingual lexicon of relation expressions from English and Spanish Wikipedia articles. To demonstrate its usefulness we apply it to crosslingual question answering over linked data. © Springer International Publishing Switzerland 2015.",Final,All Open Access; Bronze Open Access,
Draicchio F.; Gangemi A.; Presutti V.; Nuzzolese A.G.,FRED: From natural language text to RDF and OWL in one click,,1,1,#use,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893803453&doi=10.1007%2f978-3-642-41242-4_36&partnerID=40&md5=3e70d41fd698f9811e5b8d19df26b6a8,"FRED is an online tool for converting text into internally well-connected and quality linked-data-ready ontologies in web-service-acceptable time. It implements a novel approach for ontology design from natural language sentences. In this paper we present a demonstration of such tool combining Discourse Representation Theory (DRT), linguistic frame semantics, and Ontology Design Patterns (ODP). The tool is based on Boxer which implements a DRT-compliant deep parser. The logical output of Boxer enriched with semantic data from Verbnet or FrameNet frames is transformed into RDF/OWL by means of a mapping model and a set of heuristics following ODP best-practice [5] of OWL ontologies and RDF data design. © Springer-Verlag 2013.",Final,All Open Access; Bronze Open Access,
Lesnikova T.; David J.; Euzenat J.,Interlinking English and Chinese RDF data using BabelNet,,1,1,#use,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959182734&doi=10.1145%2f2682571.2797089&partnerID=40&md5=14dd737f9c6e954599d219a883a53ab9,"Linked data technologies make it possible to publish and link structured data on the Web. Although RDF is not about text, many RDF data providers publish their data in their own language. Cross-lingual interlinking aims at discovering links between identical resources across knowledge bases in different languages. In this paper, we present a method for interlinking RDF resources described in English and Chinese using the BabelNet multilingual lexicon. Resources are represented as vectors of identifiers and then similarity between these resources is computed. The method achieves an F-measure of 88%. The results are also compared to a translation-based method. © 2015 ACM.",Final,All Open Access; Green Open Access,
Ciroku F.; De Giorgis S.; Gangemi A.; Martinez-Pandiani D.S.; Presutti V.,Automated multimodal sensemaking: Ontology-based integration of linguistic frames and visual data,,1,1,#use ,2024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174832331&doi=10.1016%2fj.chb.2023.107997&partnerID=40&md5=aa66d8255e2636105d6d877f8ee1fcf1,"Frame evocation from visual data is an essential process for multimodal sensemaking, due to the multimodal abstraction provided by frame semantics. However, there is a scarcity of data-driven approaches and tools to automate it. We propose a novel approach for explainable automated multimodal sensemaking by linking linguistic frames to their physical visual occurrences, using ontology-based knowledge engineering techniques. We pair the evocation of linguistic frames from text to visual data as “framal visual manifestations”. We present a deep ontological analysis of the implicit data model of the Visual Genome image dataset, and its formalization in the novel Visual Sense Ontology (VSO). To enhance the multimodal data from this dataset, we introduce a framal knowledge expansion pipeline that extracts and connects linguistic frames – including values and emotions – to images, using multiple linguistic resources for disambiguation. It then introduces the Visual Sense Knowledge Graph (VSKG), a novel resource. VSKG is a queryable knowledge graph that enhances the accessibility and comprehensibility of Visual Genome's multimodal data, based on SPARQL queries. VSKG includes frame visual evocation data, enabling more advanced forms of explicit reasoning; analysis and sensemaking. Our work represents a significant advancement in the automation of frame evocation and multimodal sense-making, performed in a fully interpretable and transparent way, with potential applications in various fields, including the fields of knowledge representation, computer vision, and natural language processing. © 2023",Final,,
Ionov M.,APiCS-ligt: Towards semantic enrichment of interlinear glossed text,,,1,#kg,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115061858&doi=10.4230%2fOASIcs.LDK.2021.27&partnerID=40&md5=8b0740db12112bbd97074f3fc27d3c10,"This paper presents APiCS-Ligt, an LLOD version of a collection of interlinear glossed linguistic examples from APiCS, the Atlas of Pidgin and Creole Language Structures. Interlinear glossed text (IGT) plays an important role in typological and theoretical linguistic research, especially with understudied and endangered languages: It provides a way to understand linguistic phenomena without necessarily knowing the source language which is crucial for these languages since native speakers are not always easily accessible. Previously, we presented Ligt, RDF vocabulary created for representing interlinear glosses in text segments. In this paper, we present our conversion of the APiCS IGT dataset into this model and describe our efforts in linking linguistic annotations to an external ontology to add semantic representation. © Maxim Ionov; licensed under Creative Commons License CC-BY 4.0",Final,,
Chiarcos C.; Ionov M.,Linking discourse marker inventories,,,1,#use,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115090315&doi=10.4230%2fOASIcs.LDK.2021.40&partnerID=40&md5=8ce3d81f124faee294626d7eb577c118,"The paper describes the first comprehensive edition of machine-readable discourse marker lexicons. Discourse markers such as and, because, but, though or thereafter are essential communicative signals in human conversation, as they indicate how an utterance relates to its communicative context. As much of this information is implicit or expressed differently in different languages, discourse parsing, context-adequate natural language generation and machine translation are considered particularly challenging aspects of Natural Language Processing. Providing this data in machine-readable, standard-compliant form will thus facilitate such technical tasks, and moreover, allow to explore techniques for translation inference to be applied to this particular group of lexical resources that was previously largely neglected in the context of Linguistic Linked (Open) Data. © Christian Chiarcos and Maxim Ionov; licensed under Creative Commons License CC-BY 4.0",Final,,
Postiglione M.,Towards an Italian Healthcare Knowledge Graph,,1,1,#use,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118970715&doi=10.1007%2f978-3-030-89657-7_29&partnerID=40&md5=943210e16e59138126728bf8aa3fd786,"Electronic Health Records (EHRs), Big Data, Knowledge Graphs (KGs) and machine learning can potentially be a great step towards the technological shift from the one-size-fit-all medicine, where treatments are based on an equal protocol for all the patients, to the precision medicine, which takes count of all their individual information: lifestyle, preferences, health history, genomics, and so on. However, the lack of data which characterizes low-resource languages is a huge limitation for the application of the above-mentioned technologies. In this work, we will try to fill this gap by means of transformer language models and few-shot approaches and we will apply similarity-based deep learning techniques on the constructed KG for downstream applications. The proposed architecture is general and thus applicable to any low-resource language. © 2021, Springer Nature Switzerland AG.",Final,,
Nordhoff S.,Modelling and annotating interlinear glossed text from 280 different endangered languages as Linked Data with LIGT,,1,-1,#excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115109079&partnerID=40&md5=fc42847c2f8cfff3b81a5d4294b7955d,"This paper reports on the harvesting, analysis, and annotation of 20k documents from 4 different endangered language archives in 280 different low-resource languages. The documents are heterogeneous as to their provenance (holding archive, language, geographical area, creator) and internal structure (annotation types, metalanguages), but they have the ELAN-XML format in common. Typical annotations include sentence-level translations, morpheme-segmentation, morpheme-level translations, and parts-of-speech. The ELAN format gives a lot of freedom to document creators, and hence the data set is very heterogeneous. We use regularities in the ELAN format to arrive at a common internal representation of sentences, words, and morphemes, with translations into one or more additional languages. Building upon the paradigm of Linguistic Linked Open Data (LLOD, Chiarcos et al. (2012b)), the document elements receive unique identifiers and are linked to other resources such as Glottolog for languages, Wikidata for semantic concepts, and the Leipzig Glossing Rules list for category abbreviations. We provide an RDF export in the LIGT format (Chiarcos and Ionov (2019)), enabling uniform and interoperable access with some semantic enrichments to a formerly disparate resource type difficult to access. Two use cases (semantic search and colexification) are presented to show the viability of the approach. © 2020 14th Linguistic Annotation Workshop, LAW 2020 - Proceedings. All Rights Reserved.",Final,,
Gabryszak A.; Krause S.; Hennig L.; Xu F.; Uszkoreit H.,Relation- and phrase-level linking of FrameNet with sar-graphs,,,1,#use,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037086784&partnerID=40&md5=0b41c874f72fe4702c69393a28f34610,"Recent research shows the importance of linking linguistic knowledge resources for the creation of large-scale linguistic data. We describe our approach for combining two English resources, FrameNet and sar-graphs, and illustrate the benefits of the linked data in a relation extraction setting. While FrameNet consists of schematic representations of situations, linked to lexemes and their valency patterns, sar-graphs are knowledge resources that connect semantic relations from factual knowledge graphs to the linguistic phrases used to express instances of these relations. We analyze the conceptual similarities and differences of both resources and propose to link sar-graphs and FrameNet on the levels of relations/frames as well as phrases. The former alignment involves a manual ontology mapping step, which allows us to extend sar-graphs with new phrase patterns from FrameNet. The phrase-level linking, on the other hand, is fully automatic. We investigate the quality of the automatically constructed links and identify two main classes of errors.",Final,,
Menke P.; McCrae J.; Cimiano P.,Releasing multimodal data as Linguistic Linked Open Data: An experience report,,,1,#kg #ontology #multimodality,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055604675&partnerID=40&md5=167a7935a0cc0443d7462b6485521403,"In this paper we describe an implemented framework for releasing multimodal corpora as Linked Data. In particular, we describe our experiences in releasing a multimodal corpus based on an online chat game as Linked Data. Building on an internal multimodal data model we call FiESTA, we have implemented a library that enhances existing libraries and classes by functionality allowing to convert the data to RDF. Our framework is implemented on the Rails web application framework. We argue that this work can be highly useful for further contributions to the Linked Data community, especially from the fields of spoken dialogue and multimodal communication. © LDL 2013.All right reserved.",Final,,
Moran S.; Brümmer M.,Lemon-aid: using Lemon to aid quantitative historical linguistic analysis,,,1,#kg #ontology #approach ,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902596084&partnerID=40&md5=46be621b09758cdde3339f8a62f0a54b,"In this short paper, we describe how we converted dictionary and wordlist data made available by the QuantHistLing project into the Lexicon Model for Ontologies. By doing so, we leverage Linked Data to combine disparate lexical resources – more than fifty lexicons and dictionaries – by converting the lexical data into an RDF model that is specified by Lemon. The resulting new Linked Data resource, what we call the QHL dataset, provides researchers with a translation graph, which allows users to query across the underlying lexicons and dictionaries to extract semantically-aligned wordlists. © LDL 2013.All right reserved.",Final,,
Miao Q.; Lu H.; Zhang S.; Meng Y.,Cross-lingual link discovery between Chinese and english wiki knowledge bases,,,-1,#excluded,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922783862&partnerID=40&md5=f6f8378f947a26f10f4ad4f6f598c9ba,"Wikipedia is an online multilingual encyclopedia that contains a very large number of articles covering most written languages. However, one critical issue for Wikipedia is that the pages in different languages are rarely linked except for the cross-lingual link between pages about the same subject. This could pose serious difficulties to humans and machines who try to seek information from different lingual sources. In order to address above issue, we propose a hybrid approach that exploits anchor strength, topic relevance and entity knowledge graph to automatically discovery cross-lingual links. In addition, we develop CELD, a system for automatically linking key terms in Chinese documents with English Concepts. As demonstrated in the experiment evaluation, the proposed model outperforms several baselines on the NTCIR data set, which has been designed especially for the cross-lingual link discovery evaluation. © 2013 Qingliang Miao, Huayu Lu, Shu Zhang, and Yao Meng.",Final,,
Hosseini H.; Mansouri M.; Bagheri E.,A systemic functional linguistics approach to implicit entity recognition in tweets,,0/1,0/1,#use #ner,2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130179375&doi=10.1016%2fj.ipm.2022.102957&partnerID=40&md5=5c39a8d58f6495316fd81056929b8b92,"The identification of knowledge graph entity mentions in textual content has already attracted much attention. The major assumption of existing work is that entities are explicitly mentioned in text and would only need to be disambiguated and linked. However, this assumption does not necessarily hold for social content where a significant portion of information is implied. The focus of our work in this paper is to identify whether textual social content include implicit mentions of knowledge graph entities or not, hence forming a two-class classification problem. To this end, we adopt the systemic functional linguistic framework that allows for capturing meaning expressed through language. Based on this theoretical framework we systematically introduce two classes of features, namely syntagmatic and paradigmatic features, for implicit entity recognition. In our experiments, we show the utility of these features for the task, report on ablation studies, measure the impact of each feature subset on each other and also provide a detailed error analysis of our technique. © 2022 Elsevier Ltd",Final,,
Basaldella M.; Liu F.; Shareghi E.; Collier N.,COMETA: A corpus for medical entity linking in the social media,,0/1,-1,#excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104050301&partnerID=40&md5=171cac0eda6616793ff18f4f90b8ac14,"Whilst there has been growing progress in Entity Linking (EL) for general language, existing datasets fail to address the complex nature of health terminology in layman's language. Meanwhile, there is a growing need for applications that can understand the public's voice in the health domain. To address this we introduce a new corpus called COMETA, consisting of 20k English biomedical entity mentions from Reddit expert-annotated with links to SNOMED CT, a widely-used medical knowledge graph. Our corpus satisfies a combination of desirable properties, from scale and coverage to diversity and quality, that to the best of our knowledge has not been met by any of the existing resources in the field. Through benchmark experiments on 20 EL baselines from string- to neural-based models we shed light on the ability of these systems to perform complex inference on entities and concepts under 2 challenging evaluation scenarios. Our experimental results on COMETA illustrate that no golden bullet exists and even the best mainstream techniques still have a significant performance gap to fill, while the best solution relies on combining different views of data. © 2020 Association for Computational Linguistics",Final,,
Wu S.; Li Y.; Zhang D.; Zhou Y.; Wu Z.,Diverse and informative dialogue generation with context-specific commonsense knowledge awareness,,0/1,0/1,#use #commonsense #chinese #textgeneration,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098764003&partnerID=40&md5=06cd1ae45bf6e9d828a4ddc212e64d99,"Generative dialogue systems tend to produce generic responses, which often leads to boring conversations. For alleviating this issue, Recent studies proposed to retrieve and introduce knowledge facts from knowledge graphs. While this paradigm works to a certain extent, it usually retrieves knowledge facts only based on the entity word itself, without considering the specific dialogue context. Thus, the introduction of the context-irrelevant knowledge facts can impact the quality of generations. To this end, this paper proposes a novel commonsense knowledge-aware dialogue generation model, ConKADI. We design a Felicitous Fact mechanism to help the model focus on the knowledge facts that are highly relevant to the context; furthermore, two techniques, Context-Knowledge Fusion and Flexible Mode Fusion are proposed to facilitate the integration of the knowledge in the ConKADI. We collect and build a large-scale Chinese dataset aligned with the commonsense knowledge for dialogue generation. Extensive evaluations over both an open-released English dataset and our Chinese dataset demonstrate that our approach ConKADI outperforms the state-of-the-art approach CCM, in most experiments. © 2020 Association for Computational Linguistics",Final,,
Garcia-Silva A.; Denaux R.; Gomez-Perez J.M.,"Learning embeddings from scientific corpora using lexical, grammatical and semantic information",,0/1,-1,#excluded,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077823532&partnerID=40&md5=532805ed6a552718a6e8de3f1b4c3abc,"Natural language processing can assist scientists to leverage the increasing amount of information contained in scientific bibliography. The current trend, based on deep learning and embeddings, uses representations at the (sub)word level that require large amounts of training data and neural architectures with millions of parameters to learn successful language models, like BERT. However, these representations may not be well suited for the scientific domain, where it is common to find complex terms, e.g. multi-word, with a domain-specific meaning in a very specific context. In this paper we propose an approach based on a linguistic analysis of the corpus using a knowledge graph to learn representations that can unambiguously capture such terms and their meaning. We learn embeddings from different linguistic annotations on the text and evaluate them through a classification task over the SciGraph taxonomy, showing that our representations outperform (sub)word-level approaches. Copyright © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",Final,,
Nooralahzadeh F.; Lopez C.; Cabrio E.; Gandon F.; Segond F.,Adapting semantic spreading activation to entity linking in text,,0/1,-1,#excluded #noLLOD,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977477929&doi=10.1007%2f978-3-319-41754-7_7&partnerID=40&md5=5027d825770508f71c1bb987cfa91b5a,"The extraction and the disambiguation of knowledge guided by textual resources on the web is a crucial process to advance the Web of Linked Data. The goal of our work is to semantically enrich raw data by linking the mentions of named entities in the text to the corresponding known entities in knowledge bases. In our approach multiple aspects are considered: the prior knowledge of an entity in Wikipedia (i.e. the keyphraseness and commonness features that can be precomputed by crawling the Wikipedia dump), a set of features extracted from the input text and from the knowledge base, along with the correlation/relevancy among the resources in Linked Data. More precisely, this work explores the collective ranking approach formalized as a weighted graph model, in which the mentions in the input text and the candidate entities from knowledge bases are linked using the local compatibility and the global relatedness measures. Experiments on the datasets of the Open Knowledge Extraction (OKE) challenge with different configurations of our approach in each phase of the linking pipeline reveal its optimum mode. We investigate the notion of semantic relatedness between two entities represented as sets of neighbours in Linked Open Data that relies on an associative retrieval algorithm, with consideration of common neighbourhood. This measure improves the performance of prior link-based models and outperforms the explicit inter-link relevancy measure among entities (mostly Wikipedia-centric). Thus, our approach is resilient to non-existent or sparse links among related entities. © Springer International Publishing Switzerland 2016.",Final,,
Zhang Y.; Mangeot M.; Bellynck V.; Boitet C.,Jibiki-LINKS: a Tool between Traditional Dictionaries and Lexical Networks for Modelling Lexical Resources,,0/1,0/1,#excluded,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122040960&partnerID=40&md5=b3db9332575ae130fd7b3f1a1af2855c,"Between simple electronic dictionaries such as the TLFi (computerized French Language Treasure)1 and lexical networks like WordNet2 (Diller et al., 1990; Vossen, 1998), the lexical databases are growing at high speed. Our work is about the addition of rich links to lexical databases, in the context of the parallel development of lexical networks. Current research on management tools for lexical databases is strongly influenced by the field of massive data (""big data"") and by the Web of data (""linked data""). In lexical networks, one can build and use arbitrary links, but possible queries cannot model all the usual interactions with lexicographers-developers and users, that are needed, and derive from the paper world. Our work aims to find a solution that allows for the main advantages of lexical networks, while providing the equivalent of paper dictionaries by doing the lexicographic work in lexical DBs. © 2014 Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, CogALex 2014 at the 25th International Conference on Computational Linguistics, COLING 2014. All rights reserved.",Final,,
Ardissono L.; Lucenteforte M.; Mauro N.; Savoca A.; Voghera A.; Riccia L.L.,OnToMap - Semantic community maps for knowledge sharing,,0/1 ,-1,#excluded #noLLOD,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026431330&doi=10.1145%2f3078714.3078747&partnerID=40&md5=7121aff747f9d4fb4f94b020548b8f5f,"We present the information retrieval model adopted in the On- ToMap Participatory GIS. the model addresses the limitations of keyword-based and category-based search by semantically interpreting the information needs specified in free-text search queries. the model is based on an ontological representation of linguistic and encyclopaedic knowledge, which makes it possible to exploit terms and synonyms occurring in the definitions of concepts to flexibly match the user's and system's terminologies. this feature enables users to query the application using their own vocabulary. © 2017 Copyright held by the owner/author(s).",Final,,
Kundu S.; Khot T.; Sabharwal A.; Clark P.,Exploiting explicit paths for multi-hop reading comprehension,,-1,-1,#excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084048724&partnerID=40&md5=fa88b2415cccb661045197a953ab39e4,"We propose a novel, path-based reasoning approach for the multi-hop reading comprehension task where a system needs to combine facts from multiple passages to answer a question. Although inspired by multi-hop reasoning over knowledge graphs, our proposed approach operates directly over unstructured text. It generates potential paths through passages and scores them without any direct path supervision. The proposed model, named PathNet, attempts to extract implicit relations from text through entity pair representations, and compose them to encode each path. To capture additional context, PathNet also composes the passage representations along each path to compute a passage-based representation. Unlike previous approaches, our model is then able to explain its reasoning via these explicit paths through the passages. We show that our approach outperforms prior models on the multi-hop Wikihop dataset, and also can be generalized to apply to the OpenBookQA dataset, matching state-of-the-art performance. © 2019 Association for Computational Linguistics.",Final,,
Liu C.; Cohn T.; Frermann L.,Commonsense Knowledge in Word Associations and ConceptNet,,,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128898257&partnerID=40&md5=d8f3fcbf1d0993a84a8fef0833f0525b,"Humans use countless basic, shared facts about the world to efficiently navigate in their environment. This commonsense knowledge is rarely communicated explicitly, however, understanding how commonsense knowledge is represented in different paradigms is important for both deeper understanding of human cognition and for augmenting automatic reasoning systems. This paper presents an in-depth comparison of two large-scale resources of general knowledge: ConceptNet, an engineered relational database, and SWOW a knowledge graph derived from crowd-sourced word associations. We examine the structure, overlap and differences between the two graphs, as well as the extent to which they encode situational commonsense knowledge. We finally show empirically that both resources improve downstream task performance on commonsense reasoning benchmarks over text-only baselines, suggesting that large-scale word association data, which have been obtained for several languages through crowd-sourcing, can be a valuable complement to curated knowledge graphs. © 2021 Association for Computational Linguistics.",Final,,
Liang Z.; Yang J.; Liu H.; Huang K.,A Semantic Filter Based on Relations for Knowledge Graph Completion,,,-1,#excluded,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127045685&partnerID=40&md5=52d36181394c61b12bcc1d67dedb2105,"Knowledge graph embedding, representing entities and relations in the knowledge graphs with high-dimensional vectors, has made significant progress in link prediction. More researchers have explored the representational capabilities of models in recent years. That is, they investigate better representational models to fit symmetry/antisymmetry and combination relationships. The current embedding models are more inclined to utilize the identical vector for the same entity in various triples to measure the matching performance. The observation that measuring the rationality of specific triples means comparing the matching degree of the specific attributes associated with the relations is well-known. Inspired by this fact, this paper designs Semantic Filter Based on Relations(SFBR) to extract the required attributes of the entities. Then the rationality of triples is compared under these extracted attributes through the traditional embedding models. The semantic filter module can be added to most geometric and tensor decomposition models with minimal additional memory. Experiments on the benchmark datasets show that the semantic filter based on relations can suppress the impact of other attribute dimensions and improve link prediction performance. The tensor decomposition models with SFBR have achieved state-of-the-art. © 2021 Association for Computational Linguistics",Final,,
Iglesias C.Á.; Sánchez-Rada J.F.,Sentiment Analysis meets Linguistic Linked Data: An overview of the State of the Art,,,1,#use,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126004020&partnerID=40&md5=17ad64d61f470f48ea762b6d6ccc710b,"Sentiment Analysis has received plenty of attention from both industry and academia because its application can reveal new insights from social interactions. The wide range of final users of these services includes public services, businesses, and individuals. Linked data technologies provide an effective and seamless way for integrating services and interlinking language resources. This paper provides an introduction to the main approaches, applications, and datasets. © 2021 Copyright for this paper by its authors",Final,,
Calixto I.; Raganato A.; Pasini T.,Wikipedia Entities as Rendezvous across Languages: Grounding Multilingual Language Models by Predicting Wikipedia Hyperlinks,,,1,#use,2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128159332&partnerID=40&md5=ceeed2bdf67e50e491da5199f5ac7045,"Masked language models have quickly become the de facto standard when processing text. Recently, several approaches have been proposed to further enrich word representations with external knowledge sources such as knowledge graphs. However, these models are devised and evaluated in a monolingual setting only. In this work, we propose a language-independent entity prediction task as an intermediate training procedure to ground word representations on entity semantics and bridge the gap across different languages by means of a shared vocabulary of entities. We show that our approach effectively injects new lexical-semantic knowledge into neural models, improving their performance on different semantic tasks in the zero-shot crosslingual setting. As an additional advantage, our intermediate training does not require any supplementary input, allowing our models to be applied to new datasets right away. In our experiments, we use Wikipedia articles in up to 100 languages and already observe consistent gains compared to strong baselines when predicting entities using only the English Wikipedia. Further adding extra languages lead to improvements in most tasks up to a certain point, but overall we found it non-trivial to scale improvements in model transferability by training on ever increasing amounts of Wikipedia languages. © 2021 Association for Computational Linguistics.",Final,,
Sakor A.; Singh K.; Patel A.; Vidal M.-E.,Falcon 2.0: An Entity and Relation Linking Tool over Wikidata,,,1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095866274&doi=10.1145%2f3340531.3412777&partnerID=40&md5=89836c6a6ea4d848ca13698761f2d222,"The Natural Language Processing (NLP) community has significantly contributed to the solutions for entity and relation recognition from a natural language text, and possibly linking them to proper matches in Knowledge Graphs (KGs). Considering Wikidata as the background KG, there are still limited tools to link knowledge within the text to Wikidata. In this paper, we present Falcon 2.0, the first joint entity and relation linking tool over Wikidata. It receives a short natural language text in the English language and outputs a ranked list of entities and relations annotated with the proper candidates in Wikidata. The candidates are represented by their Internationalized Resource Identifier (IRI) in Wikidata. Falcon 2.0 resorts to the English language model for the recognition task (e.g., N-Gram tiling and N-Gram splitting), and then an optimization approach for the linking task. We have empirically studied the performance of Falcon 2.0 on Wikidata and concluded that it outperforms all the existing baselines. Falcon 2.0 is open source and can be reused by the community; all the required instructions of Falcon 2.0 are well-documented at our GitHub repository (https://github.com/SDM-TIB/falcon2.0). We also demonstrate an online API, which can be run without any technical expertise. Falcon 2.0 and its background knowledge bases are available as resources at https://labs.tib.eu/falcon/falcon2/. © 2020 Owner/Author.",Final,All Open Access; Bronze Open Access; Green Open Access,
Ivanov V.; Solnyshkina M.,A method for assessment of text complexity based on knowledge graphs,,,1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105003287&partnerID=40&md5=c02abc5d81e98943e3c6b7caaaca1928,"The study explores the problem of assessing text complexity. In this paper we focus on measuring conceptual complexity and propose using knowledge graphs to this end. On the first stage of the research, RuThes-Lite thesaurus, a linguistic knowledge base with a total size of over 100,000 text entries (words and collocations), was used to elicit concepts in the texts of schoolbooks and represent text fragments as graphs. In the second series of experiments, we assessed complexity of English texts using knowledge graphs WordNet and Wikidata. Finally, we identified graph-based semantic characteristics of texts impacting complexity. The most significant research findings include identification of statistically significant correlations of the selected features, such as node degree, number of connected nodes, average shortest path, with text complexity. © 2020 Copyright for this paper by its authors.",Final,,
Ghosh S.; Kundu A.; Pramanick A.; Bhattacharya I.,Discovering knowledge graph schema from short natural language text via dialog,,-1,-1,#outofscope #excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118464513&partnerID=40&md5=4f8afa6b778a6d12087b213c9aa77ae4,"We study the problem of schema discovery for knowledge graphs. We propose a solution where an agent engages in multi-turn dialog with an expert for this purpose. Each minidialog focuses on a short natural language statement, and looks to elicit the expert's desired schema-based interpretation of that statement, taking into account possible augmentations to the schema. The overall schema evolves by performing dialog over a collection of such statements. We take into account the probability that the expert does not respond to a query, and model this probability as a function of the complexity of the query. For such mini-dialogs with response uncertainty, we propose a dialog strategy that looks to elicit the schema over as short a dialog as possible. By combining the notion of uncertainty sampling from active learning with generalized binary search, the strategy asks the query with the highest expected reduction of entropy. We show that this significantly reduces dialog complexity while engaging the expert in meaningful dialog. © 2020 Association for Computational Linguistics.",Final,,
da Silva A.L.; Rigo S.J.; de Moraes J.B.,An algorithm to generate short sentences in natural language from linked open data based on linguistic templates,,,-1,#not available,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100554753&partnerID=40&md5=00e490a74e75e2f5a51c9f83f251b4ab,"The generation of natural language phrases from Linked Open Data can benefit from a significant amount of information available on the internet, as well as from the existence of properties within them, which appears, mostly, in the RDF format. These properties can represent semantic relationships between concepts that might help in creating sentences in natural language. Nevertheless, research in this field tends not to use the information in RDF. We support that this is a factor that might foster the generation of more natural phrases. In this scenario, this research explores these RDF properties for the generation of natural language phrases. The short sentences generated by the algorithm implementation were evaluated regarding their fluency by linguists and native English speakers. The results show that the sentences generated are promising regarding sentence fluency. Copyright © 2020 Inderscience Enterprises Ltd.",Final,,
Zhao C.; Walker M.; Chaturvedi S.,Bridging the structural gap between encoding and decoding for data-to-text generation,,,1,#use,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095506853&partnerID=40&md5=3fd340163697efc401cb15375abcf4fd,"Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose DUALENC, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text. © 2020 Association for Computational Linguistics",Final,,
Gao P.; Zhang X.; Qi G.,Discovering hypernymy relationships in Chinese traffic legal texts,,,-1,#excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081170455&doi=10.1007%2f978-981-15-3412-6_11&partnerID=40&md5=32cae49367164fb7c9d537de018f23d6,"Currently, Knowledge Graph is playing a crucial rule in some knowledge-based applications, such as semantic search and data integration. Due to the particularity of the vocabulary and language pattern in the Chinese legal domain, the exploration of hierarchical legal knowledge structures is still challenging. In this paper, we first explore a combination of pattern-based and linguistic-rule-based approach in helping experts to identify hypernymy relationships in large-scale traffic legal corpus. Using these relationships as ground truths, we then propose a supervised hypernymy classification of candidate term pairs using an attention-based bidirectional LSTM model, in which a global context of each candidate is defined as the feature for classification. We compare the performance of our approach with state-of-art baselines on real-world data. The evaluation results show that our approach is quite effective in finding Chinese hypernym-hyponym in the traffic legal domain. © Springer Nature Singapore Pte Ltd 2020.",Final,,
Wang Y.; Zhang H.; Shi G.; Liu Z.; Zhou Q.,A Model of Text-Enhanced Knowledge Graph Representation Learning with Mutual Attention,,,-1,#excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082595651&doi=10.1109%2fACCESS.2020.2981212&partnerID=40&md5=175585f822164711b1ac0ec0c87d7c87,"Recently, it has gained lots of interests to jointly learn the embeddings of knowledge graph (KG) and text information. However, previous work fails to incorporate the complex structural signals (from structure representation) and semantic signals (from text representation). This paper proposes a novel text-enhanced knowledge graph representation model, which can utilize textual information to enhance the knowledge representations. Especially, a mutual attention mechanism between KG and text is proposed to learn more accurate textual representations for further improving knowledge graph representation, within a unified parameter sharing semantic space. Different from conventional joint models, no complicated linguistic analysis or strict alignments between KG and text are required to train our model. Besides, the proposed model could fully incorporate the multi-direction signals. Experimental results show that the proposed model achieves the state-of-the-art performance on both link prediction and triple classification tasks, and significantly outperforms previous text-enhanced knowledge representation models. © 2013 IEEE.",Final,All Open Access; Gold Open Access,
Yan D.; Bi Y.; Huang X.,Knowledge Graph Representation of Syntactic and Semantic Information,,1?,-1,#outofscope #excluded,2020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078472600&doi=10.1007%2f978-3-030-38189-9_57&partnerID=40&md5=7759781177a3021657ff7f65115cfb8c,"Representation of linguistic knowledge is one of the keys to helping machines understand natural languages. This paper follows the idea from linguistic data to linguistic knowledge and to knowledge representation. At the syntactic level, the syntactic structure and its variants in the corpus are summarized, and the syntactic functions undertaken by the arguments are analyzed. At the semantic level, the semantic roles and semantic types of arguments are analyzed. The purpose is to reveal the interaction between syntax and semantics. Finally, this paper explores a fusion representation method of linguistic data and linguistic knowledge, and carries out a case study. © Springer Nature Switzerland AG 2020.",Final,,
Moussallem D.; Ngomo A.-C.N.; Buitelaar P.; Arcan M.,Utilizing knowledge graphs for neural machine translation augmentation,,,1,#use,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077242522&doi=10.1145%2f3360901.3364423&partnerID=40&md5=00ed4866d01ddb7772e12ec8e6bf6b07,"While neural networks have led to substantial progress in machine translation, their success depends heavily on large amounts of training data. However, parallel training corpora are not always readily available. Moreover, out-of-vocabulary words - -mostly entities and terminological expressions - -pose a difficult challenge to Neural Machine Translation systems. Recent efforts have tried to alleviate the data sparsity problem by augmenting the training data using different strategies, such as external knowledge injection. In this paper, we hypothesize that knowledge graphs enhance the semantic feature extraction of neural models, thus optimizing the translation of entities and terminological expressions in texts and consequently leading to better translation quality. We investigate two different strategies for incorporating knowledge graphs into neural models without modifying the neural network architectures. Additionally, we examine the effectiveness of our augmented models on domain-specific texts and ontologies. Our knowledge-graph-augmented neural translation model, dubbed KG-NMT, achieves significant and consistent improvements of +3 BLEU, METEOR and chrF3 on average on the newstest datasets between 2015 and 2018 for the WMT English-German translation task. © 2019 ACM.",Final,,
Chen D.; Li Y.; Yang M.; Zheng H.-T.; Shen Y.,Knowledge-aware textual entailment with graph attention network,,,-1,#excluded,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075468679&doi=10.1145%2f3357384.3358071&partnerID=40&md5=122e9e69faf8c86a7cfce803b0cf3b2e,"Textual entailment is a central problem of language variability, which has been attracting a lot of interest and it poses significant issues in front of systems aimed at natural language understanding. Recently, various frameworks have been proposed for textual entailment recognition, ranging from traditional computational linguistics techniques to deep learning model based methods. However, recent deep neural networks that achieve the state of the art on textual entailment task only consider the context information of the given sentences rather than the real-world background information and knowledge beyond the context. In the paper, we propose a Knowledge-Context Interactive Textual Entailment Network (KCI-TEN) that learns graph level sentence representations by harnessing external knowledge graph with graph attention network. We further propose a text-graph interaction mechanism for neural based entailment matching learning, which endows the redundancy and noise with less importance and put emphasis on the informative representations. Experiments on the SciTail dataset demonstrate that KCI-TEN outperforms the state-of-the-art methods. © 2019 Association for Computing Machinery.",Final,,
Weng J.; Luo J.; Ding G.; Qiu J.; Gao Y.,Group-intelligence construction of linguistic-humanity KG in Chinese verses,,,-1,#excluded,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083261030&doi=10.1109%2fSKG49510.2019.00028&partnerID=40&md5=ec439ea0e32cbbf50e04163fda1863d6,"Unlike the instrumental need of language, linguistic humanity of language means that languae, like Chinese verses, always show certain emotion, reflect attitude toward thing and convey creator's characters. To model and visualize such complicated and cross Humanism, Knowledge Graph(KG) is modified to adapt the full-relation Chinese verses and leading-in friendly visualization. Different from conventional Ml or rule-based construction, we adopt the group-intelligence construction with entity adding task, linking adding task and knowledge triple validation task to constructe the KG by alocating task destributed model. While using group-intelligence construction may lead to multi-answer and irrelevant-answer case, socring mechanism with user-consideration and group-consideration is definded. This research shows construction based on the group intelligence can optimize the KG and represent different congnize of Chinese Verses. © 2019 IEEE.",Final,,
Tuan Y.-L.; Chen Y.-N.; Lee H.-Y.,Dykgchat: Benchmarking dialogue generation grounding on dynamic knowledge graphs,,,-1,#excluded,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084288031&partnerID=40&md5=e4262146741d134ceff7c12ef1ab3d43,"Data-driven, knowledge-grounded neural conversation models are capable of generating more informative responses. However, these models have not yet demonstrated that they can zero-shot adapt to updated, unseen knowledge graphs. This paper proposes a new task about how to apply dynamic knowledge graphs in neural conversation model and presents a novel TV series conversation corpus (DyKgChat) for the task. Our new task and corpus aids in understanding the influence of dynamic knowledge graphs on responses generation. Also, we propose a preliminary model that selects an output from two networks at each time step: a sequence-to-sequence model (Seq2Seq) and a multi-hop reasoning model, in order to support dynamic knowledge graphs. To benchmark this new task and evaluate the capability of adaptation, we introduce several evaluation metrics and the experiments show that our proposed approach outperforms previous knowledge-grounded conversation models. The proposed corpus and model can motivate the future research directions1. © 2019 Association for Computational Linguistics",Final,,
Tittel S.; Gillis-Webber F.,Identification of languages in linked data: A diachronic-diatopic case study of French,,,-1,#excluded,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075366383&partnerID=40&md5=aea148aede353c01085e73c850755362,"When modelling linguistic resources as Linked Data, the identification of languages using language tags and language codes is a mandatory task. IETF's BCP 47 defines the standard for tags, and ISO 639 provides the codes. However, these codes are insufficient for the identification of diatopic variation within a language and, also, for different historical language stages. This weakness hampers the accurate identification of data, which in turn leads to ambiguity when extending, aggregating and re-using this data-a key notion of Linked Open Data and the Semantic Web. We show the limitations of language identification with a case study of French linguistic data from both a diachronic and a diatopic perspective. Our exemplary data derives from dictionaries of Old French, Middle French, and of Modern French dialects, and from a Modern French linguistic atlas. For each exemplar, we propose a solution using the privateuse sub-tag of BCP 47's language tag, staying within the boundaries of existing standards. Using a predefined pattern for the privateuse sub-tag, the solutions enable a dialect, a patois, in combination with a time period, to be defined and identified. This can lead to shared agreement of language tags that will increase interoperability within the context of Linked Data. © 2019 Lexical Computing CZ s.r.o.. All rights reserved.",Final,,
Klimek B.; McCrae J.P.; Bosque-Gil J.; Ionov M.; Tauber J.K.; Chiarcos C.,Challenges for the representation of morphology in ontology lexicons,,,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075400264&partnerID=40&md5=9b44dcee20294d5e1cc0fd3368d8fa3e,"Recent years have experienced a growing trend in the publication of language resources as Linguistic Linked Data (LLD) to enhance their discovery, reuse and the interoperability of tools that consume language data. To this aim, the OntoLex-lemon model has emerged as a de facto standard to represent lexical data on the Web. However, traditional dictionaries contain a considerable amount of morphological information which is not straightforwardly representable as LLD within the current model. In order to fill this gap a new Morphology Module of OntoLex-lemon is currently being developed. This paper presents the results of this model as on-going work as well as the underlying challenges that emerged during the module development. Based on the MMoOn Core ontology, it aims to account for a wide range of morphological information, ranging from endings to derive whole paradigms to the decomposition and generation of lexical entries which is in compliance to other OntoLex-lemon modules and facilitates the encoding of complex morphological data in ontology lexicons. © 2019 Lexical Computing CZ s.r.o.. All rights reserved.",Final,,
Mambrini F.; Passarotti M.,Harmonizing different lemmatization strategies for building a knowledge base of linguistic resources for Latin,,,1,#kg,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084299792&partnerID=40&md5=74ddb24df2cd510d88af9e9b77c6dbeb,"The interoperability between lemmatized corpora of Latin and other resources that use the lemma as indexing key is hampered by the multiple lemmatization strategies that different projects adopt. In this paper we discuss how we tackle the challenges raised by harmonizing different lemmatization criteria in a project that aims to connect linguistic resources for Latin using the Linked Data paradigm. The paper introduces the architecture supporting an open-ended, lemma-based Knowledge Base, built to make textual and lexical resources for Latin interoperable. Particularly, the paper describes the inclusion into the Knowledge Base of its lexical basis, of a word formation lexicon and of a lemmatized and syntactically annotated corpus. © 2019 Association for Computational Linguistics",Final,,
Marginean A.,Question answering over biomedical linked data with Grammatical Framework,,,1,#use #accessibility ,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010934356&doi=10.3233%2fSW-160223&partnerID=40&md5=801d07da07668112d1560beb59e2cbea,"The blending of linked data with ontologies leverages the access to data. GFMed introduces grammars for a controlled natural language targeted towards biomedical linked data and the corresponding controlled SPARQL language. The grammars are described in Grammatical Framework and introduce linguistic and SPARQL phrases mostly about drugs, diseases and relationships between them. The semantic and linguistic chunks correspond to Description Logic constructors. Problems and solutions for querying biomedical linked data with Romanian, besides English, are also considered in the context of GF. © 2017-IOS.",Final,,
Fang Z.; Wang H.; Gracia J.; Bosque-Gil J.; Ruan T.,Zhishi.Lemon: On publishing zhishi.me as linguistic linked open data,,,1,#dataset #kg ? ontology,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992618544&doi=10.1007%2f978-3-319-46547-0_6&partnerID=40&md5=1248163c87a71e2be8157c7cabb40489,"Recently, a growing number of linguistic resources in different languages have been published and interlinked as part of the Linguistic Linked Open Data (LLOD) cloud. However, in comparison to English and other prominent languages, the presence of Chinese in such a cloud is still limited, despite the fact that Chinese is the most spoken language worldwide. Publishing more Chinese language resources in the LLOD cloud can benefit both academia and industry to better understand the language itself and to further build multilingual applications that will improve the flow of data and services across countries. In this paper we describe Zhishi.lemon, a newly developed dataset based on the lemon model that constitutes the lexical realization of Zhishi.me, one of the largest Chinese datasets in the Linked Open Data (LOD) cloud. Zhishi.lemon combines the lemon core with the lemon translation module in order to build a linked data lexicon in Chinese with translations into Spanish and English. Links to BabelNet (a vast multilingual encyclopedic resource) have been provided as well.We also present a showcase of this module along with the technical details of transforming Zhishi.me to Zhishi.lemon. The dataset is accessible on the Web for both humans (via a Web interface) and software agents (with a SPARQL endpoint). © Springer International Publishing AG 2016.",Final,All Open Access; Bronze Open Access,
Ji K.; Wang S.; Carlson L.,Multilingual dictionary linking and aggregation: Quality from consistency,,,1,#use,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992573934&partnerID=40&md5=51d48c2d56423bce5da6b05b8084ca46,"The growth of Web-accessible dictionaries and term data has led to a proliferation of platforms distributing the same lexical resources in different combinations and packagings. Finding the right word or translation is like finding a needle in a haystack. The quantity of the data is undercut by the doubtful quality of the resources. Our aim is to cut down the quantity and raise the quality by matching and aggregating entries within and across dictionaries. In this exploratory paper, our goal is to see how far we can get by using information extracted from multiple dictionaries themselves. Our hypothesis is that the more limited quantity of data in dictionaries is compensated by their richer structure and more concentrated information content. We hope to take advantage of the structure of dictionaries by basing quality criteria and measures on linguistic and terminological considerations. The plan of campaign is to derive quality criteria to recognise well-constructed dictionary entries from a model dictionary, and then attempt to convert the criteria into language-independent frequency-based measures. As a model dictionary we use the Princeton WordNet. The measures derived from it are tested against data extracted from BabelNet. © 2016, CEUR-WS. All rights reserved.",Final,,
,,,,,,,,,,,
,,,,28,,,,,,,
,,,,240,,,,,,,
,,,,92,,,,,,,
,,,,96,,,,,,,
,,,,,,,,,,,
,,,,456,188,,,,,,